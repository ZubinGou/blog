<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>《统计自然语言处理》第13章 - 文本分类与情感分类 - Zubin`s Site</title><meta name="Description" content="关于 LoveIt 主题"><meta property="og:title" content="《统计自然语言处理》第13章 - 文本分类与情感分类" />
<meta property="og:description" content="ch13 文本分类与情感分类 13.1 文本分类概述 [Sebastiani, 2002]数学模型描述文本分类： 获得函数（分类器）：$\Phi: {D} \times {C} \rightarrow{{T}, \quad {F}}$ 文档：$D={d_1, d_2, &hellip;,d_{|D|}}$ 类" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://binko.me/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/" />
<meta property="og:image" content="https://binko.me/logo.png"/>
<meta property="article:published_time" content="2021-01-20T17:56:11+08:00" />
<meta property="article:modified_time" content="2021-01-20T17:56:11+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://binko.me/logo.png"/>

<meta name="twitter:title" content="《统计自然语言处理》第13章 - 文本分类与情感分类"/>
<meta name="twitter:description" content="ch13 文本分类与情感分类 13.1 文本分类概述 [Sebastiani, 2002]数学模型描述文本分类： 获得函数（分类器）：$\Phi: {D} \times {C} \rightarrow{{T}, \quad {F}}$ 文档：$D={d_1, d_2, &hellip;,d_{|D|}}$ 类"/>
<meta name="application-name" content="LoveIt">
<meta name="apple-mobile-web-app-title" content="LoveIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://binko.me/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/" /><link rel="prev" href="https://binko.me/snlp-ch9-%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/" /><link rel="next" href="https://binko.me/nndl-book-ch1-%E7%BB%AA%E8%AE%BA/" /><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "《统计自然语言处理》第13章 - 文本分类与情感分类",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/binko.me\/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/binko.me\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","keywords": "统计自然语言处理, NLP, statistics, notes","wordcount":  5679 ,
        "url": "https:\/\/binko.me\/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB\/","datePublished": "2021-01-20T17:56:11+08:00","dateModified": "2021-01-20T17:56:11+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "ZubinGou","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/binko.me\/images\/avatar.png",
                    "width":  304 ,
                    "height":  304 
                }},"author": {
                "@type": "Person",
                "name": "ZubinGou"
            },"description": ""
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper">

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [['$','$'], ['\\(','\\)']],
          displayMath: [['$$','$$']],
          processEscapes: true,
          processEnvironments: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          TeX: {
              equationNumbers: { autoNumber: "AMS" },
              extensions: ["AMSmath.js", "AMSsymbols.js"]
          }
      }
  });
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>


<header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Zubin`s Site">Zubin`s <span class="header-title-post"><i class='fas fa-paw'></i></span></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item language" title="选择语言">简体中文<i class="fas fa-chevron-right fa-fw"></i>
                        <select class="language-select" id="language-select-desktop" onchange="location = this.value;"><option value="/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/" selected>简体中文</option></select>
                    </a><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Zubin`s Site">Zubin`s <span class="header-title-post"><i class='fas fa-paw'></i></span></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw"></i>
            </a><a href="javascript:void(0);" class="menu-item" title="选择语言">简体中文<i class="fas fa-chevron-right fa-fw"></i>
                    <select class="language-select" onchange="location = this.value;"><option value="/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/" selected>简体中文</option></select>
                </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">《统计自然语言处理》第13章 - 文本分类与情感分类</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://binko.me" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw"></i>ZubinGou</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/nlp/"><i class="far fa-folder fa-fw"></i>NLP</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2021-01-20">2021-01-20</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 5679 字&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 12 分钟&nbsp;<span id="/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/" class="leancloud_visitors" data-flag-title="《统计自然语言处理》第13章 - 文本分类与情感分类">
                        <i class="far fa-eye fa-fw"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#131-文本分类概述">13.1 文本分类概述</a></li>
    <li><a href="#132-文本表示">13.2 文本表示</a></li>
    <li><a href="#133-文本特征选择">13.3 文本特征选择</a>
      <ul>
        <li><a href="#1331-文档频率df">13.3.1 文档频率DF</a></li>
        <li><a href="#1332-信息增益ig法">13.3.2 信息增益（IG）法</a></li>
        <li><a href="#1333-chi2统计量开方检验">13.3.3 $\chi^2$统计量/开方检验</a></li>
        <li><a href="#1334-互信息mi法">13.3.4 互信息（MI）法</a></li>
        <li><a href="#其他方法">其他方法</a></li>
      </ul>
    </li>
    <li><a href="#134-特征权重计算方法">13.4 特征权重计算方法</a></li>
    <li><a href="#135-分类器设计">13.5 分类器设计</a>
      <ul>
        <li><a href="#1351-朴素贝叶斯分类器">13.5.1 朴素贝叶斯分类器</a></li>
        <li><a href="#1352-svm分类器">13.5.2 SVM分类器</a></li>
        <li><a href="#1353-k-最邻近法knn">13.5.3 k-最邻近法（kNN）</a></li>
        <li><a href="#1354-神经网络nnet分类器">13.5.4 神经网络（NNet）分类器</a></li>
        <li><a href="#1355-线性最小平方拟合法linear-least-squares-fit-llsf">13.5.5 线性最小平方拟合法（linear least-squares fit, LLSF）</a></li>
        <li><a href="#1356-决策树分类器">13.5.6 决策树分类器</a></li>
        <li><a href="#1357-模糊分类器">13.5.7 模糊分类器</a></li>
        <li><a href="#1358-rocchio分类器">13.5.8 Rocchio分类器</a></li>
        <li><a href="#1359-基于投票的分类方法">13.5.9 基于投票的分类方法</a></li>
      </ul>
    </li>
    <li><a href="#136-文本分类性能测评">13.6 文本分类性能测评</a></li>
    <li><a href="#137-情感分类">13.7 情感分类</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h1 id="ch13-文本分类与情感分类">ch13 文本分类与情感分类</h1>
<h2 id="131-文本分类概述">13.1 文本分类概述</h2>
<ul>
<li>
<p>[Sebastiani, 2002]数学模型描述文本分类：</p>
<ul>
<li>获得函数（分类器）：$\Phi: {D} \times {C} \rightarrow{{T}, \quad {F}}$</li>
<li>文档：$D={d_1, d_2, &hellip;,d_{|D|}}$</li>
<li>类别：${C}=\left{{c}_{1}, {c}_{2}, \ldots, {c}_{|C|}\right}$</li>
</ul>
</li>
<li>
<p>关键问题</p>
<ul>
<li>文本表示</li>
<li>分类器设计</li>
</ul>
</li>
<li>
<p>文本分类系统
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="../../_resources/9582ff377cf44784b6ea0bd41ae78f8d.png"
        data-srcset="../../_resources/9582ff377cf44784b6ea0bd41ae78f8d.png, ../../_resources/9582ff377cf44784b6ea0bd41ae78f8d.png 1.5x, ../../_resources/9582ff377cf44784b6ea0bd41ae78f8d.png 2x"
        data-sizes="auto"
        alt="../../_resources/9582ff377cf44784b6ea0bd41ae78f8d.png"
        title="4cd185d07b05e0d8520cfee2419a9b68.png" />
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="../../_resources/34779827acf5402ba07a979a2aee2e33.png"
        data-srcset="../../_resources/34779827acf5402ba07a979a2aee2e33.png, ../../_resources/34779827acf5402ba07a979a2aee2e33.png 1.5x, ../../_resources/34779827acf5402ba07a979a2aee2e33.png 2x"
        data-sizes="auto"
        alt="../../_resources/34779827acf5402ba07a979a2aee2e33.png"
        title="57a31df7f3712c77169af2b6c03a9711.png" /></p>
<ol>
<li>
<p>文本预处理：分词，取出停用词，过滤低频词，编码归一化等</p>
</li>
<li>
<p>文本向量化：如使用向量空间模型VSM或者概率统计模型对文本进行表示，使计算机能够理解计算，用的方法基于集合论模型、基于代数轮模型、基于频率统计模型等</p>
</li>
<li>
<p>文本特征提取和选择：特征提取对应着特征项的选择和特征权重的计算。是文本分类的核心内容，常用的特征提取方法：
1)用映射或者变换的方法对原始特征降维（word2vec）；</p>
<p>2)从原始的特征中挑选出一些最具代表性的特征；</p>
<p>3)根据专家的知识挑选出最具影响力的特征；</p>
<p>4)基于数学的方法选取出最具分类信息的特征。</p>
</li>
<li>
<p>分类器选择：回归模型，二元独立概率模型，语言模型建模IR模型</p>
</li>
</ol>
</li>
<li>
<p>文本分类系统分类</p>
<ul>
<li>基于知识工程（knowledge learning，KE）
<ul>
<li>专家人工规则</li>
</ul>
</li>
<li>基于机器学习（machine learning，ML）
<ul>
<li>用训练样本进行特征选择、分类器参数训练</li>
<li>根据选择的特征对分类输入样本进行形式化</li>
<li>输入到分类器进行类别判定</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="132-文本表示">13.2 文本表示</h2>
<ul>
<li>向量空间模型（vector space modle，VSM）
<ul>
<li>文档（document）：通常是文章中具有一定规模的片段，如句子、句群、段落、段落组直至整篇文章。</li>
<li>项／特征项（term/feature term）：特征项是VSM中最小的不可分的语言单元，可以是字、词、词组或短语等。一个文档的内容被看成是它含有的特征项所组成的集合，表示为：Document＝D（t1，t2，…，tn），其中tk是特征项，1≤k≤n。</li>
<li>项的权重（term weight）：对文档n个特征项依据一定原则赋予权重$w_k$，D＝D（t1，w1;t2，w2;…;tn，wn），简记为D＝D（w1，w2，…，wn）</li>
<li>VSM定义：给定一个文档D（t1，w1;t2，w2;…;tn，wn），D符合以下两条约定：
<ol>
<li>各个特征项tk（1≤k≤n）互异（即没有重复）；</li>
<li>各个特征项tk无先后顺序关系（即不考虑文档的内部结构）</li>
</ol>
</li>
<li>特征项$t_k$看作n维坐标系，权重$w_k$作为坐标值，文本表示维n维向量
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="../../_resources/82468445660f4c8d8cf5603fae23deec.png"
        data-srcset="../../_resources/82468445660f4c8d8cf5603fae23deec.png, ../../_resources/82468445660f4c8d8cf5603fae23deec.png 1.5x, ../../_resources/82468445660f4c8d8cf5603fae23deec.png 2x"
        data-sizes="auto"
        alt="../../_resources/82468445660f4c8d8cf5603fae23deec.png"
        title="d038a8bfa260c4b1cd7da04def10bc06.png" /></li>
</ul>
</li>
<li>向量的相似性度量（similarity）：任意两个文档D1和D2之间的相似系数Sim（D1，D2）指两个文档内容的相关程度（degree of relevance）
<ul>
<li>向量内积：$\operatorname{Sim}\left(D_{1}, D_{2}\right)=\sum_{k=1}^{n} w_{1 k} \times w_{2 k}$</li>
<li>考虑归一化，向量余弦：$\operatorname{Sim}\left(D_{1}, D_{2}\right)=\cos \theta=\frac{\sum_{k=1}^{n} w_{1 k} \times w_{2 k}}{\sum_{k=1}^{n} w_{1 k}^{2} \sum_{k=1}^{n} w_{2 k}^{2}}$</li>
</ul>
</li>
<li>除了VSM以外表示方法：
<ul>
<li>词组表示法：
<ul>
<li>提高不显著</li>
<li>提高了特征向量语义含量，但降低了特征向量统计质量，使特征向量更加稀疏</li>
</ul>
</li>
<li>概念表示法
<ul>
<li>用概念（concept）作为特征向量的特征表示</li>
<li>用概念代替单个词可以在一定程度上解决自然语言的歧义性和多样性给特征向量带来的噪声问题，有利于提高文本分类的效果</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="133-文本特征选择">13.3 文本特征选择</h2>
<ul>
<li>文本特征可以是：字、词、短语、概念等等</li>
<li>常用方法：
<ul>
<li>文档频率（document frequency, DF）特征提取法</li>
<li>信息增益（information gain, IG）法</li>
<li>χ2统计量（CHI）法</li>
<li>互信息（mutual information, MI）方法</li>
</ul>
</li>
</ul>
<h3 id="1331-文档频率df">13.3.1 文档频率DF</h3>
<ul>
<li>文档频率（DF）= 包含某特征项的文档数量 / 总文档数量</li>
<li>舍弃DF过小（没有代表性）、过大（没有区分度）的特征</li>
<li>优点：降低向量计算复杂度，可能提高分类准确率，因为去掉了一部分噪声特征，简单易行</li>
<li>缺陷：理论根据不足。根据信息论，某些低频率特征往往包含较多信息</li>
</ul>
<h3 id="1332-信息增益ig法">13.3.2 信息增益（IG）法</h3>
<ul>
<li>信息增益法：根据某特征项$t_i$使得期望信息或者信息熵的有效减少量（信息增益）来判断其重要程度以取舍</li>
<li>信息增益 = 不考虑任何特征时文档的熵 - 考虑该特征后文档的熵
$\begin{aligned} \operatorname{Gain}\left(t_{i}\right)=&amp; \text { Entropy }(S)-\text { Expected Entropy }\left(S_{t_{i}}\right) \\=&amp;\left{-\sum_{j=1}^{M} P\left(C_{j}\right) \times \log P\left(C_{j}\right)\right}-\left{P\left(t_{i}\right) \times\left[-\sum_{j=1}^{M} P\left(C_{j} \mid t_{i}\right) \times \log P\left(C_{j} \mid t_{i}\right)\right]\right.\ &amp;\left.+P\left(\bar{t}_{i}\right) \times\left[-\sum_{i=1}^{M} P\left(C_{j} \mid \bar{t}_{i}\right) \times \log P\left(C_{j} \mid \bar{t}_{i}\right)\right]\right} \end{aligned}$</li>
<li>信息增益法是理论上最好的特征选取方法，但实际上许多高信息增益的特征出现频率较低，选取特征数目少时往往存在数据稀疏问题，分类效果差</li>
</ul>
<h3 id="1333-chi2统计量开方检验">13.3.3 $\chi^2$统计量/开方检验</h3>
<ul>
<li>
<p>$\chi^2$统计量（CHI）衡量的是特征项ti和类别Cj之间的相关联程度，并假设ti和Cj之间符合具有一阶自由度的$\chi^2$分布</p>
</li>
<li>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="../../_resources/5c695beda9de4e259c1f532b0abef568.png"
        data-srcset="../../_resources/5c695beda9de4e259c1f532b0abef568.png, ../../_resources/5c695beda9de4e259c1f532b0abef568.png 1.5x, ../../_resources/5c695beda9de4e259c1f532b0abef568.png 2x"
        data-sizes="auto"
        alt="../../_resources/5c695beda9de4e259c1f532b0abef568.png"
        title="9f02bc61446d7b37570ceca72d35a6e2.png" />
$\chi^{2}\left(t_{i}, C_{j}\right)=\frac{N \times(A \times D-C \times B)^{2}}{(A+C) \times(B+D) \times(A+B) \times(C+D)}$</p>
</li>
<li>
<p>两种实现方法</p>
<ol>
<li>最大值法：分别计算$t_i$对于每个类别的CHI值，然后在整个训练语料上：
$\chi_{\mathrm{MAX}}^{2}\left(t_{i}\right)=\max _{j=1}^{M} x\left{\chi^{2}\left(t_{i}, C_{j}\right)\right}$</li>
<li>平均值法：计算各特征对于各类别的平均值
$\chi_{\mathrm{AVG}}^{2}\left(t_{i}\right)=\sum_{j=1}^{M} P\left(C_{j}\right) \chi^{2}\left(t_{i}, C_{j}\right)$</li>
</ol>
<ul>
<li>保留统计量高于给定阈值的特征</li>
</ul>
</li>
<li>
<p>开方检验的缺点：忽略了词频，夸大了低频词的作用（低频词缺陷）。</p>
</li>
</ul>
<h3 id="1334-互信息mi法">13.3.4 互信息（MI）法</h3>
<ul>
<li>基本思想：互信息越大，特征ti和类别Cj共现的程度越大。
$\begin{aligned} I\left(t_{i}, C_{j}\right) &amp;=\log \frac{P\left(t_{i}, C_{j}\right)}{P\left(t_{i}\right) P\left(C_{j}\right)} \\ &amp;=\log \frac{P\left(t_{i} \mid C_{j}\right)}{P\left(t_{i}\right)} \\ &amp; \approx \log \frac{A \times N}{(A+C) \times(A+B)} \end{aligned}$</li>
<li>若特征ti和类别Cj无关，则P（ti，Cj）＝P（ti）×P（Cj），那么，
I（ti，Cj）＝0</li>
<li>两种处理方法
<ol>
<li>最大值法：$I_{\mathrm{MAX}}\left(t_{i}\right)=\max _{j=1}^{M} \mathrm{x}\left[P\left(C_{j}\right) \times I\left(t_{i}, C_{j}\right)\right]$</li>
<li>平均值法：$I_{\mathrm{AVG}}\left(t_{i}\right)=\sum_{j=1}^{M} P\left(C_{j}\right) I\left(t_{i}, C_{j}\right)$</li>
</ol>
</li>
</ul>
<h3 id="其他方法">其他方法</h3>
<ul>
<li>DTP（distance to transition point）方法［Moyotl-Hernández and Jiménez-Salazar, 2005］</li>
<li>期望交叉熵法</li>
<li>文本证据权法</li>
<li>优势率方法［Mademnic and Grobelnik, 1999］</li>
<li>“类别区分词”的特征提取方法［周茜等，2004］</li>
<li>基于粗糙集（rough set）的特征提取方法 TFACQ［Hu et al., 2003］</li>
<li>强类信息词（strong information class word, SCIW）方法［Li and Zong, 2005a］</li>
</ul>
<h2 id="134-特征权重计算方法">13.4 特征权重计算方法</h2>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="../../_resources/c955db382e48427fa740024766ae6dec.png"
        data-srcset="../../_resources/c955db382e48427fa740024766ae6dec.png, ../../_resources/c955db382e48427fa740024766ae6dec.png 1.5x, ../../_resources/c955db382e48427fa740024766ae6dec.png 2x"
        data-sizes="auto"
        alt="../../_resources/c955db382e48427fa740024766ae6dec.png"
        title="ca339b10ccf104b3cdd78aad29be5aac.png" />
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="../../_resources/925dc149b0c5448fbca7bfb1e80e6e27.png"
        data-srcset="../../_resources/925dc149b0c5448fbca7bfb1e80e6e27.png, ../../_resources/925dc149b0c5448fbca7bfb1e80e6e27.png 1.5x, ../../_resources/925dc149b0c5448fbca7bfb1e80e6e27.png 2x"
        data-sizes="auto"
        alt="../../_resources/925dc149b0c5448fbca7bfb1e80e6e27.png"
        title="9619e4941924763c9588f4f0c1765527.png" /></p>
<h2 id="135-分类器设计">13.5 分类器设计</h2>
<ul>
<li>常用分类算法
<ul>
<li>朴素的贝叶斯分类法（naΪve Bayesian classifier）</li>
<li>基于支持向量机（support vector machines,SVM）的分类器</li>
<li>k-最近邻法（k-nearest neighbor, kNN）</li>
<li>神经网络法（neural network, NNet）</li>
<li>决策树（decision tree）分类法</li>
<li>模糊分类法（fuzzy classifier）</li>
<li>Rocchio分类方法</li>
<li>Boosting算法</li>
</ul>
</li>
</ul>
<h3 id="1351-朴素贝叶斯分类器">13.5.1 朴素贝叶斯分类器</h3>
<p>朴素贝叶斯( naive Bayes)是一种最简单常用的概率生成式模型（Generative Model），生成式模型是指有多少类，我们就学习多少个模型，分别计算新测试样本 $x$跟三个类别的联合概率$P(x, y)$，再根据贝叶斯公式计算选取使得$P(y \mid x)$最大的作为分类。而判别式模型（Discrimitive Model）训练数据得到分类函数和分界面（如SVM），不能反应训练数据本身的特性。</p>
<p>基本思想：利用特征项和类别的联合概率来估计给定文档的类别概率。假设文本是基于词的一元模型。</p>
<p>假设现有的类别$C=(c_1,c_2&hellip;c_m)$，则文档最可能属于$\hat{c}=\underset{c \in C}{\operatorname{argmax}} P(c \mid d)$类，使用贝叶斯公式转换为如下形式：
$$
\hat{c}=\underset{c \in C}{\operatorname{argmax}} P(c \mid d)=\underset{c \in C}{\operatorname{argmax}} \frac{P(d \mid c) P(c)}{P(d)}
$$
分母相同可以忽略，得到：
$$
\hat{c}=\underset{c \in C}{\operatorname{argmax}} P(c \mid d)=\underset{c \in C}{\operatorname{argmax}} P(d \mid c) P(c)
$$
这个公式由两部分组成，前面那部分$P(d|c)$ 称为似然函数，后面那部分$P(c)$ 称为先验概率。使用词袋模型来表示文档$d$，文档$d$的每个特征表示为：$d={f_1,f_2,f_3……f_n}$，那么这里的特征$f_i$ 其实就是单词$w_i$ 出现的频率（次数），公式转化为：
$$
\hat{c}=\underset{c \in C}{\operatorname{argmax}} \overbrace{P\left(f_{1}, f_{2}, \ldots, f_{n} \mid c\right)}^{\text {likelihood }} \overbrace{P(c)}^{\text {prior }}
$$
朴素贝叶斯的“朴素”表现在假设各个特征之间相互独立（条件独立性假设），则$P\left(f_{1}, f_{2} \ldots \ldots_{n} \mid c\right)=P\left(f_{1} \mid c\right){\times} P\left(f_{2} \mid c\right){\times} \ldots \ldots{\times} P\left(f_{n} \mid c\right)$，故而公式变为
$$
c_{N B}=\underset{c \in C}{\operatorname{argmax}} P(c) \prod_{f \in F} P(f \mid c)
$$
因为每个概率的值很小，多个相乘则可能出现下溢（underflower）， 引入对数函数$log$，在$log\ space$中进行计算：
$$
c_{N B}=\underset{c \in C}{\operatorname{argmax}} \log P(c)+\sum_{i \in \text {positions}} \log P\left(w_{i} \mid c\right)
$$</p>
<ol>
<li>文档采用DF向量表示法：
$P\left(\right.$ Doc $\left.\mid C_{i}\right)=\prod_{t_{j} \in V} P\left(\operatorname{Doc}\left(t_{j}\right) \mid C_{i}\right)$
$P($ Doc $)=\sum_{i}\left[P\left(C_{i}\right) \prod_{t_{i} \in V} P\left(\operatorname{Doc}\left(t_{i}\right) \mid C_{i}\right)\right]$
$P\left(C_{i} \mid\right.$ Doc $)=\frac{P\left(C_{i}\right) \prod_{t_{j} \in V} P\left(\operatorname{Doc}\left(t_{j}\right) \mid C_{i}\right)}{\sum_{i}\left[P\left(C_{i}\right) \prod_{t_{j} \in V} P\left(\operatorname{Doc}\left(t_{j}\right) \mid C_{i}\right)\right]}$
<ul>
<li>拉普拉斯估计：$P\left(\operatorname{Doc}\left(t_{j}\right) \mid C_{i}\right)=\frac{1+N\left(\operatorname{Doc}\left(t_{j}\right) \mid C_{i}\right)}{2+\left|D_{c_{i}}\right|}$</li>
<li>分子加1和分母加2背后的基本原理是这样的：在执行实际的试验之前，我们假设已经有两次试验，一次成功和一次失败</li>
</ul>
</li>
<li>文档采用TF向量表示法：
$P\left(C_{i} \mid\right.$ Doc $)=\frac{P\left(C_{i}\right) \prod_{t_{i} \in V} P\left(t_{j} \mid C_{i}\right)^{\mathrm{TF}\left(t_{i}, \text { Doc }\right)}}{\sum_{j}\left[P\left(C_{j}\right) \prod_{t_{i} \in V} P\left(t_{i} \mid C_{j}\right)^{\mathrm{TF}\left(t_{i}, \mathrm{D}_{0}\right)}\right]}$
<ul>
<li>拉普拉斯估计：$P\left(t_{i} \mid C_{i}\right)=\frac{1+\operatorname{TF}\left(t_{i}, C_{i}\right)}{|V|+\sum_{j} \operatorname{TF}\left(t_{j}, C_{i}\right)}$</li>
<li>加一平滑，对每个类别下所有划分的计数加1</li>
</ul>
</li>
</ol>
<h3 id="1352-svm分类器">13.5.2 SVM分类器</h3>
<ul>
<li>对于多类模式识别问题通常需要建立多个两类分类器</li>
</ul>
<h3 id="1353-k-最邻近法knn">13.5.3 k-最邻近法（kNN）</h3>
<ul>
<li>在训练集中找邻近的k个文档，对其中每类的每个文档进行权重（余弦相似度）求和，作为该类和测试文档的相似度，决策规则：
$y\left(x, C_{j}\right)=\sum_{d_{i} \in k \mathrm{NN}} \operatorname{sim}\left(x, d_{i}\right) y\left(d_{i}, C_{j}\right)-b_{j}$
<ul>
<li>$y(d_i，C_j)$为1表示di属于分类Cj，0表不属于。</li>
<li>$b_j$为二元决策的阈值</li>
</ul>
</li>
</ul>
<h3 id="1354-神经网络nnet分类器">13.5.4 神经网络（NNet）分类器</h3>
<ul>
<li>输入单词或者更复杂特征向量，机器学习输入到分类的非线性映射</li>
</ul>
<h3 id="1355-线性最小平方拟合法linear-least-squares-fit-llsf">13.5.5 线性最小平方拟合法（linear least-squares fit, LLSF）</h3>
<ul>
<li>从训练集和分类文档中学习得到多元回归模型（multivariate regression model）</li>
<li>$\boldsymbol{F}_{\mathrm{LS}}=\arg \min _{F}|\boldsymbol{F} \times \boldsymbol{A}-\boldsymbol{B}|^{2}$</li>
<li>矩阵A和矩阵B描述的是训练数据（对应栏分别是输入和输出向量）；FLS为结果矩阵，定义了从任意文档到加权分类向量的映射。对这些分类的权重映射值排序，同时结合阈值算法，就可以来判别输入文档所属的类别。阈值是从训练中学习获取的</li>
</ul>
<h3 id="1356-决策树分类器">13.5.6 决策树分类器</h3>
<ul>
<li>树的根结点是整个数据集合空间，每个分结点是对一个单一变量的测试，该测试将数据集合空间分割成两个或更多个类别，即决策树可以是二叉树也可以是多叉树。每个叶结点是属于单一类别的记录。</li>
<li>训练集生成决策树，测试集修剪决策树</li>
<li>一般可通过递归分割的过程构建决策树，其生成过程通常是自上而下的，目的为最佳分割</li>
<li>从根结点到叶结点都有一条路径，这条路径就是一条决策“规则”</li>
<li>信息增益是决策树训练中常用的衡量给定属性区分训练样本能力的定量标准</li>
</ul>
<h3 id="1357-模糊分类器">13.5.7 模糊分类器</h3>
<ul>
<li>任何一个文本或文本类都可以通过其特征关键词描述，因此，可以用一个定义在特征关键词类上的模糊集来描述它们。</li>
<li>判定分类文本T所属的类别可以通过计算文本T的模糊集FT分别与其他每个文本类的模糊集Fk的关联度SR实现，两个类的关联度越大说明这两个类越贴近</li>
</ul>
<h3 id="1358-rocchio分类器">13.5.8 Rocchio分类器</h3>
<ul>
<li>Rocchio分类器是情报检索领域经典的算法</li>
<li>基本思想：
<ol>
<li>为每个训练文本C建立特征向量</li>
<li>用训练文本特征向量为每类建立原始向量（类向量）</li>
<li>对待分类文本，距离最近的类就是所属类别</li>
</ol>
</li>
<li>距离：向量点积、余弦相似度等</li>
<li>如果C类文本的原型向量为w1，已知一组训练文本，可以预测w1改进的第j个元素值为
$w_{1 j}^{\prime}=\alpha w_{1 j}+\beta \frac{\sum_{i \in C} x_{i j}}{n_{C}}-\gamma \frac{\sum_{i \in C} x_{i j}}{n-n_{C}}$
<ul>
<li>nC是训练样本中正例个数，即属于类别C的文本数；xij是第i个文本特征向量的第j个元素值；α、β、γ为控制参数。α控制了上一次计算所得的w对本次计算所产生的影响，β和γ分别控制正例训练集和反例训练集对结果的影响。</li>
</ul>
</li>
</ul>
<h3 id="1359-基于投票的分类方法">13.5.9 基于投票的分类方法</h3>
<ul>
<li>多分类器组合</li>
<li>核心思想：k个专家判断的有效组合应该优于某个专家个人的判断结果</li>
<li>投票算法：
<ol>
<li>Bagging算法（民主）
<ul>
<li>票数最多的作为最终类别</li>
</ul>
</li>
<li>Boosting算法（精英）
<ul>
<li>Boosting推进：每次将分类错误的样本加入下一个弱分类器的训练</li>
<li>Adaboosting自适应推进：提高错误点的权值，加权投票（精度高的弱分类器权重大）</li>
</ul>
</li>
</ol>
</li>
<li>Boosting
<ul>
<li>1984年Valiant提出的”可能近似正确”-PAC(Probably Approximately Correct)学习模型</li>
<li>强与弱
<ul>
<li>强学习：学习效果好</li>
<li>弱学习：仅比随机好</li>
</ul>
</li>
<li>Boost（Schapire 1990）：任意 弱学习算法 -&gt; （任意正确率）强学习算法，加强过程多项式复杂度</li>
<li>Adaboost（Freund and Schapire）：
<ul>
<li>不需要提前知道弱学习算法先验知识</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="136-文本分类性能测评">13.6 文本分类性能测评</h2>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="../../_resources/cb2c4045c6b94c84b38f4a6f01af5450.png"
        data-srcset="../../_resources/cb2c4045c6b94c84b38f4a6f01af5450.png, ../../_resources/cb2c4045c6b94c84b38f4a6f01af5450.png 1.5x, ../../_resources/cb2c4045c6b94c84b38f4a6f01af5450.png 2x"
        data-sizes="auto"
        alt="../../_resources/cb2c4045c6b94c84b38f4a6f01af5450.png"
        title="93d2643f71b5b3245deb26921e5fe355.png" /></p>
<ul>
<li>正确率（Precision）：$P=\frac{T P}{T P+F P}$</li>
<li>召回率（Recall）：$R=\frac{T P}{T P+F N}$</li>
<li>$F_{\beta}$值（P与R加权调和平均）：$F_{\beta}=\frac{\beta^{2}+1}{\frac{\beta^{2}}{r}+\frac{1}{p}}=\frac{\left(\beta^{2}+1\right) \times p \times r}{\beta^{2} \times p+r}$</li>
<li>$F_1$值（P与R调和平均值）：$F_{1}=\frac{1}{\frac{1}{2} \frac{1}{P}+\frac{1}{2} \frac{1}{R}}=\frac{2 P R}{P+R}$</li>
<li>宏平均（Macro-averaging）：先对每一个类统计指标值，然后在对所有类求算术平均值。</li>
<li>微平均（Micro-averaging）：对数据集中的每一个实例不分类别进行统计建立全局混淆矩阵，然后计算相应指标。
<ul>
<li>微平均更多地受分类器对一些常见类（这些类的语料通常比较多）分类效果的影响，而宏平均则可以更多地反映对一些特殊类的分类效果。在对多种算法进行对比时，通常采用微平均算法。</li>
</ul>
</li>
<li>平衡点（break-even point）评测法［Aas and Eikvil, 1999］：通过调整分类器的阈值，调整正确率和召回率的值，使其达到一个平衡点的评测方法</li>
<li>11点平均正确率方法［Taghva et al., 2004］：为了更加全面地评价一个分类器在不同召回率情况下的分类效果，调整阈值使得分类器的召回率分别为：0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1，然后计算出对应的11个正确率，取其平均值</li>
</ul>
<h2 id="137-情感分类">13.7 情感分类</h2>
<ul>
<li>情感分析（sentiment analysis）：借助计算机帮助用户快速获取、整理和分析相关评价信息，对带有情感色彩的主观性文本进行分析、处理、归纳和推理［Pang and Lee, 2008］。情感分析包含较多的任务，如情感分类（sentiment classification）、观点抽取（opinion extraction）、观点问答和观点摘要等。</li>
<li>情感分类是指根据文本所表达的含义和情感信息将文本划分成褒扬的或贬义的两种或几种类型，是对文本作者倾向性和观点、态度的划分，因此有时也称倾向性分析（opinion analysis）</li>
<li>情感分类的特殊性：情感的隐蔽性、多义性和极性不明显性</li>
</ul>
<ol>
<li>按机器学习方法分类
<ul>
<li>有监督学习方法</li>
<li>半监督学习方法</li>
<li>无监督学习方法</li>
</ul>
</li>
<li>按照研究问题分类
<ul>
<li>领域相关性研究
<ul>
<li>领域适应性（domain adaptation）研究</li>
</ul>
</li>
<li>数据不平衡问题研究</li>
</ul>
</li>
</ol>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2021-01-20</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://binko.me/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/" data-title="《统计自然语言处理》第13章 - 文本分类与情感分类" data-hashtags="统计自然语言处理,NLP,statistics,notes"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://binko.me/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/" data-hashtag="统计自然语言处理"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://binko.me/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/" data-title="《统计自然语言处理》第13章 - 文本分类与情感分类"><i class="fab fa-hacker-news fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://binko.me/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/" data-title="《统计自然语言处理》第13章 - 文本分类与情感分类"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@2.14.0/icons/line.svg"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://binko.me/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/" data-title="《统计自然语言处理》第13章 - 文本分类与情感分类"><i class="fab fa-weibo fa-fw"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">统计自然语言处理</a>,&nbsp;<a href="/tags/nlp/">NLP</a>,&nbsp;<a href="/tags/statistics/">statistics</a>,&nbsp;<a href="/tags/notes/">Notes</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/snlp-ch9-%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/" class="prev" rel="prev" title="《统计自然语言处理》第9.1章 - 语义分析"><i class="fas fa-angle-left fa-fw"></i>《统计自然语言处理》第9.1章 - 语义分析</a>
            <a href="/nndl-book-ch1-%E7%BB%AA%E8%AE%BA/" class="next" rel="next" title="《神经网络与深度学习》第1章 - 绪论">《神经网络与深度学习》第1章 - 绪论<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
<div id="comments"><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container">

            <div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2019 - 2021</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">ZubinGou</a></span><span> | Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow">Hugo</a> & <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="external nofollow">LoveIt</a></span> 

                
            </div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery.js@1.2.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/smooth-scroll@16.1.3/dist/smooth-scroll.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.37.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/algoliasearch@4.2.0/dist/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.2.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery.js@1.2.0/dist/js/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lg-thumbnail.js@1.2.0/dist/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lg-zoom.js@1.2.0/dist/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.4.0/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":10},"comment":{"valine":{"appId":"QGzwQXOqs5JOhN4RGPOkR2mR-MdYXbMMI","appKey":"WBmoGyJtbqUswvfLh6L8iEBr","avatar":"mp","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@5.0.1/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":false,"highlight":true,"lang":"zh-cn","pageSize":10,"placeholder":"你的评论 ...","recordIP":true,"serverURLs":"https://leancloud.hugoloveit.com","visitor":true}},"lightGallery":{"actualSize":false,"exThumbImage":"data-thumbnail","hideBarsDelay":2000,"selector":".lightgallery","speed":400,"thumbContHeight":80,"thumbWidth":80,"thumbnail":true},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"BGWYRG74JP","algoliaIndex":"binko","algoliaSearchKey":"1048a43ee01931f87e76ac2d1955675f","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
