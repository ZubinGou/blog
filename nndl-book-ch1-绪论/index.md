# 《神经网络与深度学习》第1章 - 绪论


# 1 绪论
## 1.1 人工智能
- 领域
	- 感知：CV、Audio
	- 学习：监督、无监督、强化学习
	- 认知：知识表示、NLU、推理、规划、决策
- 历史
	- 推理期、知识期、学习期
- 流派
	- 符号主义
	- 连接主义

## 1.2 机器学习
- 步骤
	- 数据预处理
	- 特征提取
	- 特征转换
		- 升维
		- 降维
			- 特征选择
			- 特征抽取
				- 主成分分析PCA
				- 线性判别分析LDA
- 传统机器学习主要关注学习预测模型，可以看作浅层学习，不涉及特征学习
	- 特征靠人工经验或特征转换来抽取
	- 很多问题变成了特征工程问题

## 1.3 表示学习
- Representation：输入 -> 特征
- 表示学习：自动学习有效特征的算法
- 关键：语义鸿沟（Semantic Gap）问题，输入的底层特征与高层语义的差异
- 核心问题
	- 什么是好的表示
	- 如何学习好的表示
- 表示方式
	- 局部表示/离散表示/符号表示
		- one-hot、高维稀疏二值向量
	- 分布式表示
		- 一种语义分散到低维基向量、低维稠密向量
		- 分散过程成为嵌入（Embedding）

## 1.4 深度学习
- 主要目的：自动学习特征表示，避免特征工程
- ![f9e59f8f13c5ec309ad7f47de0a4b555.png](/blog/_resources/ce038e15e5304abc93139528213b00e1.png)
- 关键问题：贡献度分配问题（Credit Assignment Problem，CAP）
- 看作特殊的强化学习
	- 每个内部组件间接、延迟地从最终奖励中得到监督信息
- 主要模型：
	- 神经网络：误差反向传播，解决贡献度分配问题
- 端到端学习：
	- 传统切割任务：模块单独优化与总目标不一致、错误传播

## 1.5 神经网络
- 学习能力
	- 赫布网络：基于赫布规则无监督
	- 感知器：无法扩展到多层
	- 反向传播
- 两层网络逼近任何函数

## 1.6 知识体系
![cb4ff294f8d789d04a35668b361749ef.png](/blog/_resources/c5a28234242e4e96b7769a6f50853314.png)
- 机器学习
	- 监督学习
	- 无监督学习
	- 强化学习
- 神经网络
	- 前馈神经网络
	- 卷积神经网络
	- 循环神经网络
	- 图网络


