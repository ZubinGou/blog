<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>《神经网络与深度学习》第5章 - 卷积神经网络 - Zubin`s Blog</title><meta name="Description" content="关于 LoveIt 主题"><meta property="og:title" content="《神经网络与深度学习》第5章 - 卷积神经网络" />
<meta property="og:description" content="卷积神经网络（Convolutional Neural Network，CNN 或 ConvNet）：一种具有局部连接、权重共享等特性的深层前馈神经网络 FC处" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://zubingou.github.io/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" /><meta property="og:image" content="https://zubingou.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-02-15T13:56:11+08:00" />
<meta property="article:modified_time" content="2021-02-15T13:56:11+08:00" /><meta property="og:site_name" content="My cool site" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://zubingou.github.io/logo.png"/>

<meta name="twitter:title" content="《神经网络与深度学习》第5章 - 卷积神经网络"/>
<meta name="twitter:description" content="卷积神经网络（Convolutional Neural Network，CNN 或 ConvNet）：一种具有局部连接、权重共享等特性的深层前馈神经网络 FC处"/>
<meta name="application-name" content="LoveIt">
<meta name="apple-mobile-web-app-title" content="LoveIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://zubingou.github.io/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" /><link rel="prev" href="https://zubingou.github.io/blog/nnlp-notes/" /><link rel="next" href="https://zubingou.github.io/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/" /><link rel="stylesheet" href="/blog/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "《神经网络与深度学习》第5章 - 卷积神经网络",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/zubingou.github.io\/blog\/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/zubingou.github.io\/blog\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","keywords": "神经网络与深度学习, NLP, notes, ML","wordcount":  3521 ,
        "url": "https:\/\/zubingou.github.io\/blog\/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\/","datePublished": "2021-02-15T13:56:11+08:00","dateModified": "2021-02-15T13:56:11+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "ZubinGou","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/zubingou.github.io\/blog\/images\/avatar.png",
                    "width":  304 ,
                    "height":  304 
                }},"author": {
                "@type": "Person",
                "name": "ZubinGou"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/blog/" title="Zubin`s Blog">Zubin`s <span class="header-title-post"><i class='fas fa-paw'></i></span></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/blog/posts/"> 所有文章 </a><a class="menu-item" href="/blog/tags/"> 标签 </a><a class="menu-item" href="/blog/categories/"> 分类 </a><a class="menu-item" href="https://zubingou.github.io" rel="noopener noreffer" target="_blank"> 关于 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/blog/" title="Zubin`s Blog">Zubin`s <span class="header-title-post"><i class='fas fa-paw'></i></span></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/blog/posts/" title="">所有文章</a><a class="menu-item" href="/blog/tags/" title="">标签</a><a class="menu-item" href="/blog/categories/" title="">分类</a><a class="menu-item" href="https://zubingou.github.io" title="" rel="noopener noreffer" target="_blank">关于</a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">《神经网络与深度学习》第5章 - 卷积神经网络</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://binko.me" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>ZubinGou</a></span>&nbsp;<span class="post-category">收录于 <a href="/blog/categories/deep-learning/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Deep Learning</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2021-02-15">2021-02-15</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 3521 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 8 分钟&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#51-卷积">5.1 卷积</a>
      <ul>
        <li><a href="#511-卷积的定义">5.1.1 卷积的定义</a>
          <ul>
            <li><a href="#一维卷积">一维卷积</a></li>
            <li><a href="#二维卷积">二维卷积</a></li>
          </ul>
        </li>
        <li><a href="#512-互相关">5.1.2 互相关</a></li>
        <li><a href="#513-卷积的变种">5.1.3 卷积的变种</a></li>
        <li><a href="#514-卷积的数学性质">5.1.4 卷积的数学性质</a></li>
      </ul>
    </li>
    <li><a href="#52-卷积神经网络">5.2 卷积神经网络</a>
      <ul>
        <li><a href="#521-卷积代替全连接">5.2.1 卷积代替全连接</a></li>
        <li><a href="#522-卷积层">5.2.2 卷积层</a></li>
        <li><a href="#523-汇聚层">5.2.3 汇聚层</a></li>
        <li><a href="#524-卷积网络的整体结构">5.2.4 卷积网络的整体结构</a></li>
      </ul>
    </li>
    <li><a href="#53-参数学习">5.3 参数学习</a>
      <ul>
        <li><a href="#531-卷积神经网络的反向传播算法">5.3.1 卷积神经网络的反向传播算法</a>
          <ul>
            <li><a href="#汇聚层">汇聚层</a></li>
            <li><a href="#卷积层">卷积层</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#54-几种典型的cnn">5.4 几种典型的CNN</a>
      <ul>
        <li><a href="#541-lenet-5">5.4.1 LeNet-5</a></li>
        <li><a href="#542-alexnet">5.4.2 AlexNet</a></li>
        <li><a href="#543-inception网络">5.4.3 Inception网络</a></li>
        <li><a href="#残差网络">残差网络</a></li>
      </ul>
    </li>
    <li><a href="#55-其他卷积方式">5.5 其他卷积方式</a>
      <ul>
        <li><a href="#551-转置卷积">5.5.1 转置卷积</a></li>
        <li><a href="#552-空洞卷积">5.5.2 空洞卷积</a></li>
      </ul>
    </li>
    <li><a href="#习题">习题</a>
      <ul>
        <li>
          <ul>
            <li><a href="#习题-5-1-1证明公式-56-可以近似为离散信号序列-𝑥𝑡-关于-𝑡-的二阶微分2对于二维卷积设计一种滤波器来近似实现对二维输入信号的二阶微分">习题 5-1 1）证明公式 (5.6) 可以近似为离散信号序列 𝑥(𝑡) 关于 𝑡 的二阶微分；2）对于二维卷积，设计一种滤波器来近似实现对二维输入信号的二阶微分．</a></li>
            <li><a href="#习题-5-2-证明宽卷积具有交换性即公式-513">习题 5-2 证明宽卷积具有交换性，即公式 (5.13)．</a></li>
            <li><a href="#习题-5-3-分析卷积神经网络中用-1--1-的卷积核的作用">习题 5-3 分析卷积神经网络中用 1 × 1 的卷积核的作用．</a></li>
            <li><a href="#习题5-4-对于一个输入为100--100--256的特征映射组使用3--3的卷积核输出为-100--100--256-的特征映射组的卷积层求其时间和空间复杂度如果引入一个-1--1-卷积核先得到-100--100--64-的特征映射再进行-3--3-的卷积得到100--100--256-的特征映射组求其时间和空间复杂度">习题5-4 对于一个输入为100 × 100 × 256的特征映射组，使用3 × 3的卷积核，输出为 100 × 100 × 256 的特征映射组的卷积层，求其时间和空间复杂度．如果引入一个 1 × 1 卷积核，先得到 100 × 100 × 64 的特征映射，再进行 3 × 3 的卷积，得到100 × 100 × 256 的特征映射组，求其时间和空间复杂度．</a></li>
            <li><a href="#习题-5-5-对于一个二维卷积输入为-3--3卷积核大小为-2--2试将卷积操作重写为仿射变换的形式-参见公式545">习题 5-5 对于一个二维卷积，输入为 3 × 3，卷积核大小为 2 × 2，试将卷积操作重写为仿射变换的形式． 参见公式(5.45)．</a></li>
            <li><a href="#习题-5-6-计算函数-𝑦--max𝑥1---𝑥𝐷-和函数-𝑦--arg-max𝑥1---𝑥𝐷-的梯度">习题 5-6 计算函数 𝑦 = max(𝑥1, ⋯ , 𝑥𝐷) 和函数 𝑦 = arg max(𝑥1, ⋯ , 𝑥𝐷) 的梯度．</a></li>
            <li><a href="#习题-5-7-忽略激活函数分析卷积网络中卷积层的前向计算和反向传播公式539是一种转置关系">习题 5-7 忽略激活函数，分析卷积网络中卷积层的前向计算和反向传播（公式(5.39)）是一种转置关系．</a></li>
            <li><a href="#习题5-8-在空洞卷积中当卷积核大小为𝐾膨胀率为𝐷-时如何设置零填充𝑃-的值以使得卷积为等宽卷积">习题5-8 在空洞卷积中，当卷积核大小为𝐾，膨胀率为𝐷 时，如何设置零填充𝑃 的值以使得卷积为等宽卷积．</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p>卷积神经网络（Convolutional Neural Network，CNN 或 ConvNet）：一种具有局部连接、权重共享等特性的深层前馈神经网络</p>
<p>FC处理图像问题：</p>
<ol>
<li>参数过多</li>
<li>局部不变性</li>
</ol>
<p>启发：感受野（Receptive Filed），视觉神经元只接收视网膜特定区域的信号。</p>
<p>CNN一般组成：卷积层 + 汇聚层 + 全连接层，交叉堆叠</p>
<p>CNN结构特性：局部连接、权重共享、汇聚</p>
<ul>
<li>一定程度的平移、缩放和旋转不变性（？）</li>
</ul>
<h2 id="51-卷积">5.1 卷积</h2>
<h3 id="511-卷积的定义">5.1.1 卷积的定义</h3>
<h4 id="一维卷积">一维卷积</h4>
<p>长度K的滤波器与信号序列$x_1,x_2,&hellip;$的卷积：
$$
y_{t}=\sum_{k=1}^{K} w_{k} x_{t-k+1}
$$</p>
<ul>
<li>滤波器（Filter）或卷积核（Convolution Kernel）：$w_{1}, w_{2}, \cdots$</li>
<li>简化：$y=w * x$</li>
<li>$\boldsymbol{w}=[1 / K, \cdots, 1 / K]$时，卷积相当于简单移动平均</li>
<li>$\boldsymbol{w}=[1,-2,1]$时，近似信号序列的二阶微分
$$
x^{\prime \prime}(t)=x(t+1)+x(t-1)-2 x(t)
$$
<img
        class="lazyload"
        src="/blog/svg/loading.min.svg"
        data-src="/blog/_resources/cccb0297140349709aa69b71464c4e04.png"
        data-srcset="/blog/_resources/cccb0297140349709aa69b71464c4e04.png, /blog/_resources/cccb0297140349709aa69b71464c4e04.png 1.5x, /blog/_resources/cccb0297140349709aa69b71464c4e04.png 2x"
        data-sizes="auto"
        alt="/blog/_resources/cccb0297140349709aa69b71464c4e04.png"
        title="42392b24f2b3bfe5966f35ec1078d264.png" /></li>
<li>上图卷积分别检测信号序列的低频和高频信息</li>
</ul>
<h4 id="二维卷积">二维卷积</h4>
<p>给定图像 $\boldsymbol{X} \in \mathbb{R}^{M \times N}$ 和一个滤波器 $\boldsymbol{W} \in \mathbb{R}^{U \times V}$，卷积为：
$$
y_{i j}=\sum_{u=1}^{U} \sum_{v=1}^{V} w_{u v} x_{i-u+1, j-v+1}
$$</p>
<ul>
<li>简化：$\boldsymbol{Y}=\boldsymbol{W} * \boldsymbol{X}$</li>
<li>均值滤波（Mean Filter）即是一种二维卷积</li>
</ul>
<p><img
        class="lazyload"
        src="/blog/svg/loading.min.svg"
        data-src="/blog/_resources/cedc8d2749484c6d8e2e683780776bdf.png"
        data-srcset="/blog/_resources/cedc8d2749484c6d8e2e683780776bdf.png, /blog/_resources/cedc8d2749484c6d8e2e683780776bdf.png 1.5x, /blog/_resources/cedc8d2749484c6d8e2e683780776bdf.png 2x"
        data-sizes="auto"
        alt="/blog/_resources/cedc8d2749484c6d8e2e683780776bdf.png"
        title="9d165e65f8c62b38985d0315ffa315e4.png" /></p>
<p><img
        class="lazyload"
        src="/blog/svg/loading.min.svg"
        data-src="/blog/_resources/d7b3859aa0d14db0930e5eca4bdf398a.png"
        data-srcset="/blog/_resources/d7b3859aa0d14db0930e5eca4bdf398a.png, /blog/_resources/d7b3859aa0d14db0930e5eca4bdf398a.png 1.5x, /blog/_resources/d7b3859aa0d14db0930e5eca4bdf398a.png 2x"
        data-sizes="auto"
        alt="/blog/_resources/d7b3859aa0d14db0930e5eca4bdf398a.png"
        title="d42af1e0a6c946685e264d2cdbe0a736.png" /></p>
<ul>
<li>第一个为高斯滤波器，后俩用来提取边缘特征</li>
</ul>
<h3 id="512-互相关">5.1.2 互相关</h3>
<p>互相关（Cross-Correlation），衡量两个序列相关性的函数，也称为不翻转卷积（相比卷积，卷积核不同翻转）：
$$
y_{i j}=\sum_{u=1}^{U} \sum_{v=1}^{V} w_{u v} x_{i+u-1, j+v-1}
$$
即：
$$
\begin{aligned}
\boldsymbol{Y} &amp;=\boldsymbol{W} \otimes \boldsymbol{X} \\
&amp;=\operatorname{rot} 180(\boldsymbol{W}) * \boldsymbol{X}
\end{aligned}
$$</p>
<p>卷积核可学习时，卷积和互相关能力等价，为了实现方便，一般用互相关代替卷积。</p>
<h3 id="513-卷积的变种">5.1.3 卷积的变种</h3>
<p>步长（Stride）：卷积核滑动的时间间隔
零填充（Zero Padding）：输入向量两端补零
<img
        class="lazyload"
        src="/blog/svg/loading.min.svg"
        data-src="/blog/_resources/69c49ec1b0d1492c9142f5b31cb66b08.png"
        data-srcset="/blog/_resources/69c49ec1b0d1492c9142f5b31cb66b08.png, /blog/_resources/69c49ec1b0d1492c9142f5b31cb66b08.png 1.5x, /blog/_resources/69c49ec1b0d1492c9142f5b31cb66b08.png 2x"
        data-sizes="auto"
        alt="/blog/_resources/69c49ec1b0d1492c9142f5b31cb66b08.png"
        title="49ead6aa388852c7484a5d93b64da0a9.png" /></p>
<p>设卷积大小为K，常用卷积（步长S均为1）：</p>
<ol>
<li>窄卷积（Narrow Convolution）：P=1</li>
<li>宽卷积（Wide Convolution）：P=K-1</li>
<li>等宽卷积（Equal-Width Convolution）：P=(K-1)/2</li>
</ol>
<h3 id="514-卷积的数学性质">5.1.4 卷积的数学性质</h3>
<ol>
<li>交换性</li>
</ol>
<ul>
<li>不限制长度的两个卷积信号的卷积具有交换性：
$$y=y * x$$</li>
<li>固定长度信息和卷积核的宽卷积也具有交换性（？证明）：
$$\operatorname{rot} 180(\boldsymbol{W}) \tilde{\otimes} \boldsymbol{X}=\operatorname{rot} 180(\boldsymbol{X}) \tilde{\otimes} \boldsymbol{W}$$</li>
</ul>
<ol start="2">
<li>导数
假设 $\boldsymbol{Y}=\boldsymbol{W} \otimes \boldsymbol{X}$，其中 $\boldsymbol{X} \in \mathbb{R}^{M \times N}, \boldsymbol{W} \in \mathbb{R}^{U \times V}, \boldsymbol{Y} \in \mathbb{R}^{(M-U+1) \times(N-V+1)}$，函数  $f(\boldsymbol{Y}) \in \mathbb{R}$ 为一个标量函数, 则：
$$
\begin{aligned}
\frac{\partial f(\boldsymbol{Y})}{\partial w_{u v}} &amp;=\sum_{i=1}^{M-U+1 N-V+1} \frac{\partial y_{i j}}{\partial w_{u v}} \frac{\partial f(\boldsymbol{Y})}{\partial y_{i j}} \\
&amp;=\sum_{i=1}^{M-U+1} \sum_{j=1}^{N-V+1} x_{i+u-1, j+v-1} \frac{\partial f(\boldsymbol{Y})}{\partial y_{i j}} \\
&amp;=\sum_{i=1}^{M-U+1} \sum_{j=1}^{N-V+1} \frac{\partial f(\boldsymbol{Y})}{\partial y_{i j}} x_{u+i-1, v+j-1}
\end{aligned}
$$</li>
</ol>
<p>即$f(Y)$关于$W$的偏导数为$X$和$\frac{\partial f(\boldsymbol{Y})}{\partial \boldsymbol{Y}}$的卷积（互相关）：
$$
\frac{\partial f(\boldsymbol{Y})}{\partial \boldsymbol{W}}=\frac{\partial f(\boldsymbol{Y})}{\partial \boldsymbol{Y}} \otimes \boldsymbol{X}
$$</p>
<p>同理：
$$
\begin{aligned}
\frac{\partial f(\boldsymbol{Y})}{\partial x_{s t}} &amp;=\sum_{i=1}^{M-U+1} \sum_{j=1}^{N-V+1} \frac{\partial y_{i j}}{\partial x_{s t}} \frac{\partial f(\boldsymbol{Y})}{\partial y_{i j}} \\
&amp;=\sum_{i=1}^{M-U+1 N-V+1} \sum_{j=1}^{M} w_{S-i+1, t-j+1} \frac{\partial f(\boldsymbol{Y})}{\partial y_{i j}}
\end{aligned}
$$</p>
<p>即$f(Y)$关于$X$的偏导数为$W$和$\frac{\partial f(\boldsymbol{Y})}{\partial \boldsymbol{Y}}$的宽卷积（互相关）：
$$
\begin{aligned}
\frac{\partial f(\boldsymbol{Y})}{\partial \boldsymbol{X}} &amp;=\operatorname{rot} 180\left(\frac{\partial f(\boldsymbol{Y})}{\partial \boldsymbol{Y}}\right) \tilde{\otimes} \boldsymbol{W} \\
&amp;=\operatorname{rot} 180(\boldsymbol{W}) \tilde{\otimes} \frac{\partial f(\boldsymbol{Y})}{\partial \boldsymbol{Y}}
\end{aligned}
$$</p>
<h2 id="52-卷积神经网络">5.2 卷积神经网络</h2>
<h3 id="521-卷积代替全连接">5.2.1 卷积代替全连接</h3>
<p>$$
\boldsymbol{z}^{(l)}=\boldsymbol{w}^{(l)} \otimes \boldsymbol{a}^{(l-1)}+b^{(l)}
$$</p>
<p>卷积层性质：</p>
<ul>
<li>局部连接</li>
<li>权重共享：可以理解为一个卷积核捕捉一种局部特征</li>
</ul>
<p><img
        class="lazyload"
        src="/blog/svg/loading.min.svg"
        data-src="/blog/_resources/3f67c0bd3c0a4977b0a9b74feebe5092.png"
        data-srcset="/blog/_resources/3f67c0bd3c0a4977b0a9b74feebe5092.png, /blog/_resources/3f67c0bd3c0a4977b0a9b74feebe5092.png 1.5x, /blog/_resources/3f67c0bd3c0a4977b0a9b74feebe5092.png 2x"
        data-sizes="auto"
        alt="/blog/_resources/3f67c0bd3c0a4977b0a9b74feebe5092.png"
        title="b362404d149f1a523d826c16fdc86a29.png" /></p>
<p>卷积层参数只有K维权重$w^{(l)}$和一维偏置$b^{(l)}$
神经元数量满足（默认步长1，无零填充）：
$$
\text { } M_{l}=M_{l-1}-K+1
$$</p>
<h3 id="522-卷积层">5.2.2 卷积层</h3>
<p>特征映射（Feature Map）：一幅图像（或其他特征映射）在经过卷积提取到的特征，每个特征映射可以作为一类抽取的图像特征
<img
        class="lazyload"
        src="/blog/svg/loading.min.svg"
        data-src="/blog/_resources/8c84ce55a7ac4cf78d5764c21be0799f.png"
        data-srcset="/blog/_resources/8c84ce55a7ac4cf78d5764c21be0799f.png, /blog/_resources/8c84ce55a7ac4cf78d5764c21be0799f.png 1.5x, /blog/_resources/8c84ce55a7ac4cf78d5764c21be0799f.png 2x"
        data-sizes="auto"
        alt="/blog/_resources/8c84ce55a7ac4cf78d5764c21be0799f.png"
        title="6c90f149f197867324e0a040dee5ff21.png" /></p>
<p>$$
X \in \mathbb{R}^{M \times N \times D}
$$
$$
y \in \mathbb{R}^{M^{\prime} \times N^{\prime} \times P}
$$
$$
\mathcal{W} \in \mathbb{R}^{U \times V \times P \times D}
$$
$$
\begin{array}{l}
\boldsymbol{Z}^{p}=\boldsymbol{W}^{p} \otimes \boldsymbol{X}+b^{p}=\sum_{d=1}^{D} \boldsymbol{W}^{p, d} \otimes \boldsymbol{X}^{d}+b^{p} \\
\boldsymbol{Y}^{p}=f\left(\boldsymbol{Z}^{p}\right)
\end{array}
$$</p>
<p><img
        class="lazyload"
        src="/blog/svg/loading.min.svg"
        data-src="/blog/_resources/2b93459095c741919e90314518612270.png"
        data-srcset="/blog/_resources/2b93459095c741919e90314518612270.png, /blog/_resources/2b93459095c741919e90314518612270.png 1.5x, /blog/_resources/2b93459095c741919e90314518612270.png 2x"
        data-sizes="auto"
        alt="/blog/_resources/2b93459095c741919e90314518612270.png"
        title="539df3c8e2119c3438127509404ab4fd.png" /></p>
<p>参数个数：$P \times D \times(U \times V)+P$</p>
<h3 id="523-汇聚层">5.2.3 汇聚层</h3>
<p>汇聚层（Pooling Layer）也叫子采样层（Subsampling Layer）：促进特征选择，降低特征/参数数量，避免过拟合</p>
<p>汇聚（Pooling）是指对每个区域进行下采样（Down Sampling）得到一个值，作为这个区域的概括</p>
<p>常用汇聚函数：</p>
<ol>
<li>最大汇聚（Maximum Pooling/ Max Pooling）：$y_{m, n}^{d}=\max _{i \in R_{m, n}^{d}} x_{i}$</li>
<li>平均汇聚（Mean Pooling）：$y_{m, n}^{d}=\frac{1}{\left|R_{m, n}^{d}\right|} \sum_{i \in R_{m, n}^{d}} x_{i}$</li>
</ol>
<p><img
        class="lazyload"
        src="/blog/svg/loading.min.svg"
        data-src="/blog/_resources/0f26c6a440c44d15a4e7b6a4ec57de7d.png"
        data-srcset="/blog/_resources/0f26c6a440c44d15a4e7b6a4ec57de7d.png, /blog/_resources/0f26c6a440c44d15a4e7b6a4ec57de7d.png 1.5x, /blog/_resources/0f26c6a440c44d15a4e7b6a4ec57de7d.png 2x"
        data-sizes="auto"
        alt="/blog/_resources/0f26c6a440c44d15a4e7b6a4ec57de7d.png"
        title="56d213933845758d9f138d631f4c58a2.png" /></p>
<p>汇聚层可以看作特殊的卷积层：卷积核大小为$K\times K$，步长为$S\times S$，卷积核为max函数或mean函数</p>
<h3 id="524-卷积网络的整体结构">5.2.4 卷积网络的整体结构</h3>
<p>一个典型的卷积网络是由卷积层、汇聚层、全连接层交叉堆叠而成</p>
<p><img
        class="lazyload"
        src="/blog/svg/loading.min.svg"
        data-src="/blog/_resources/073f38173b2b4f2687d04323a8529e4e.png"
        data-srcset="/blog/_resources/073f38173b2b4f2687d04323a8529e4e.png, /blog/_resources/073f38173b2b4f2687d04323a8529e4e.png 1.5x, /blog/_resources/073f38173b2b4f2687d04323a8529e4e.png 2x"
        data-sizes="auto"
        alt="/blog/_resources/073f38173b2b4f2687d04323a8529e4e.png"
        title="52be94f6d0a339e8782a8c5263e84833.png" />
（N为1~100或更大，K一般为0~2）</p>
<p>目前，趋向于使用小卷积核、深结构、少汇聚层的全卷积网络</p>
<h2 id="53-参数学习">5.3 参数学习</h2>
<p>CNN参数只有卷积核和偏置，对于：
$$
\boldsymbol{Z}^{(l, p)}=\sum_{d=1}^{D} \boldsymbol{W}^{(l, p, d)} \otimes \boldsymbol{X}^{(l-1, d)}+b^{(l, p)}
$$</p>
<p>偏导：
$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}^{(l, p, d)}} &amp;=\frac{\partial \mathcal{L}}{\partial \boldsymbol{Z}^{(l, p)}} \otimes \boldsymbol{X}^{(l-1, d)} \\
&amp;=\delta^{(l, p)} \otimes \boldsymbol{X}^{(l-1, d)}
\end{aligned}
$$
$$
\frac{\partial \mathcal{L}}{\partial b^{(l, p)}}=\sum_{i, j}\left[\delta^{(l, p)}\right]_{i, j}
$$</p>
<h3 id="531-卷积神经网络的反向传播算法">5.3.1 卷积神经网络的反向传播算法</h3>
<h4 id="汇聚层">汇聚层</h4>
<p>上采样与l层特征映射的激活值偏导数逐元素相乘：
$$
\begin{aligned}
\delta^{(l, p)} &amp; \triangleq \frac{\partial \mathcal{L}}{\partial Z^{(l, p)}} \\
&amp;=\frac{\partial X^{(l, p)}}{\partial Z^{(l, p)}} \frac{\partial Z^{(l+1, p)}}{\partial \boldsymbol{X}^{(l, p)}} \frac{\partial \mathcal{L}}{\partial \boldsymbol{Z}^{(l+1, p)}} \\
&amp;=f_{l}^{\prime}\left(\boldsymbol{Z}^{(l, p)}\right) \odot \operatorname{up}\left(\delta^{(l+1, p)}\right)
\end{aligned}
$$</p>
<h4 id="卷积层">卷积层</h4>
<p>$$
\begin{aligned}
\delta^{(l, p)} &amp; \triangleq \frac{\partial \mathcal{L}}{\partial Z^{(l, p)}} \\
&amp;=\frac{\partial X^{(l, p)}}{\partial Z^{(l, p)}} \frac{\partial Z^{(l+1, p)}}{\partial \boldsymbol{X}^{(l, p)}} \frac{\partial \mathcal{L}}{\partial \boldsymbol{Z}^{(l+1, p)}} \\
&amp;=f_{l}^{\prime}\left(\boldsymbol{Z}^{(l, p)}\right) \odot \operatorname{up}\left(\delta^{(l+1, p)}\right)
\end{aligned}
$$</p>
<h2 id="54-几种典型的cnn">5.4 几种典型的CNN</h2>
<h3 id="541-lenet-5">5.4.1 LeNet-5</h3>
<p>LeNet-5[LeCun et al., 1998]</p>
<p><img
        class="lazyload"
        src="/blog/svg/loading.min.svg"
        data-src="/blog/_resources/1bc54a1bb40d4c0c84a4a80d78d7a5b9.png"
        data-srcset="/blog/_resources/1bc54a1bb40d4c0c84a4a80d78d7a5b9.png, /blog/_resources/1bc54a1bb40d4c0c84a4a80d78d7a5b9.png 1.5x, /blog/_resources/1bc54a1bb40d4c0c84a4a80d78d7a5b9.png 2x"
        data-sizes="auto"
        alt="/blog/_resources/1bc54a1bb40d4c0c84a4a80d78d7a5b9.png"
        title="49c9858119839c413f3c8ccb2d069962.png" /></p>
<h3 id="542-alexnet">5.4.2 AlexNet</h3>
<p>AlexNet[Krizhevsky et al., 2012]是第一个现代深度的CNN</p>
<ul>
<li>2012 年ImageNet 图像分类冠军</li>
</ul>
<p><img
        class="lazyload"
        src="/blog/svg/loading.min.svg"
        data-src="/blog/_resources/71a1560d5ba5426ba28b6b4672cf0bc9.png"
        data-srcset="/blog/_resources/71a1560d5ba5426ba28b6b4672cf0bc9.png, /blog/_resources/71a1560d5ba5426ba28b6b4672cf0bc9.png 1.5x, /blog/_resources/71a1560d5ba5426ba28b6b4672cf0bc9.png 2x"
        data-sizes="auto"
        alt="/blog/_resources/71a1560d5ba5426ba28b6b4672cf0bc9.png"
        title="bd684df33fe25b753c520ff8d2080023.png" /></p>
<h3 id="543-inception网络">5.4.3 Inception网络</h3>
<p>Inception 网络是由有多个 Inception 模块和少量的汇聚层堆叠而成</p>
<p><img
        class="lazyload"
        src="/blog/svg/loading.min.svg"
        data-src="/blog/_resources/72b4839baca24cacb3b484e7ee6a6f17.png"
        data-srcset="/blog/_resources/72b4839baca24cacb3b484e7ee6a6f17.png, /blog/_resources/72b4839baca24cacb3b484e7ee6a6f17.png 1.5x, /blog/_resources/72b4839baca24cacb3b484e7ee6a6f17.png 2x"
        data-sizes="auto"
        alt="/blog/_resources/72b4839baca24cacb3b484e7ee6a6f17.png"
        title="81efad8436c1a73921b2cbf9cbb7ece9.png" />
Inception v1：即GoogLeNet[Szegedy et al., 2015]</p>
<ul>
<li>2014 年 ImageNet 图像分类冠军</li>
</ul>
<p><img
        class="lazyload"
        src="/blog/svg/loading.min.svg"
        data-src="/blog/_resources/a67bed52c7de4f489668a8d219f674b2.png"
        data-srcset="/blog/_resources/a67bed52c7de4f489668a8d219f674b2.png, /blog/_resources/a67bed52c7de4f489668a8d219f674b2.png 1.5x, /blog/_resources/a67bed52c7de4f489668a8d219f674b2.png 2x"
        data-sizes="auto"
        alt="/blog/_resources/a67bed52c7de4f489668a8d219f674b2.png"
        title="523e2cd40a48759d78e21d88cb9b4624.png" /></p>
<p>Inception v3 网络：用多层的小卷积核来替换大的卷积核</p>
<h3 id="残差网络">残差网络</h3>
<p>残差网络（Residual Network，ResNet）：通过给非线性的卷积层增加直连边（Shortcut Connection）（也称为残差连接 Residual Connection））的方式来提高信息的传播效率</p>
<p>将目标函数拆分为两部分：恒等函数（Identity Function）和残差函数（Residue Function）：
$$
h(\boldsymbol{x})=\underbrace{\boldsymbol{x}}_{\text {恒等函数 }}+\underbrace{(h(\boldsymbol{x})-\boldsymbol{x})}_{\text {残差函数 }}
$$</p>
<p><img
        class="lazyload"
        src="/blog/svg/loading.min.svg"
        data-src="/blog/_resources/7355f8f513b34545882c87b384d1e8a4.png"
        data-srcset="/blog/_resources/7355f8f513b34545882c87b384d1e8a4.png, /blog/_resources/7355f8f513b34545882c87b384d1e8a4.png 1.5x, /blog/_resources/7355f8f513b34545882c87b384d1e8a4.png 2x"
        data-sizes="auto"
        alt="/blog/_resources/7355f8f513b34545882c87b384d1e8a4.png"
        title="af4e41a34c3babe2823d011c9c969e9e.png" /></p>
<h2 id="55-其他卷积方式">5.5 其他卷积方式</h2>
<h3 id="551-转置卷积">5.5.1 转置卷积</h3>
<p>$$
\begin{aligned}
z &amp;=w \otimes x \\
&amp;=\left[\begin{array}{lllll}
w_{1} &amp; w_{2} &amp; w_{3} &amp; 0 &amp; 0 \\
0 &amp; w_{1} &amp; w_{2} &amp; w_{3} &amp; 0 \\
0 &amp; 0 &amp; w_{1} &amp; w_{2} &amp; w_{3}
\end{array}\right] \boldsymbol{x} \\
&amp;=\boldsymbol{C} \boldsymbol{x}
\end{aligned}
$$</p>
<p>反过来，仿射矩阵的转置：
$$
\begin{aligned}
\boldsymbol{x} &amp;=\boldsymbol{C}^{\top} \boldsymbol{z} \\
&amp;=\left[\begin{array}{lll}
w_{1} &amp; 0 &amp; 0 \\
w_{2} &amp; w_{1} &amp; 0 \\
w_{3} &amp; w_{2} &amp; w_{1} \\
0 &amp; w_{3} &amp; w_{2} \\
0 &amp; 0 &amp; w_{3}
\end{array}\right] \boldsymbol{z} \\
&amp;=\operatorname{rot} 180(\boldsymbol{w}) \tilde{\otimes} z
\end{aligned}
$$</p>
<p>我们将这种低维特征映射到高维特征的卷积操作成为转置卷积（Transposed Convolution），也称为反卷积（Deconvolution）</p>
<p>卷积层的前向计算和后向传播也是一种转置关系
<img
        class="lazyload"
        src="/blog/svg/loading.min.svg"
        data-src="/blog/_resources/9b5fe78043854a36a59bd7045bb656bb.png"
        data-srcset="/blog/_resources/9b5fe78043854a36a59bd7045bb656bb.png, /blog/_resources/9b5fe78043854a36a59bd7045bb656bb.png 1.5x, /blog/_resources/9b5fe78043854a36a59bd7045bb656bb.png 2x"
        data-sizes="auto"
        alt="/blog/_resources/9b5fe78043854a36a59bd7045bb656bb.png"
        title="e3a97d5124dde886cf6458eb45ddb5d5.png" /></p>
<p>微步卷积：步长 $S\lt 1$ 的转置卷积</p>
<h3 id="552-空洞卷积">5.5.2 空洞卷积</h3>
<p>空洞卷积（Atrous Convolution）：不增加参数，增加了输出单元感受野，也成为膨胀（Dilated Convolution）</p>
<p><img
        class="lazyload"
        src="/blog/svg/loading.min.svg"
        data-src="/blog/_resources/a32ab208685b436bbfc4f4d10f4b46a4.png"
        data-srcset="/blog/_resources/a32ab208685b436bbfc4f4d10f4b46a4.png, /blog/_resources/a32ab208685b436bbfc4f4d10f4b46a4.png 1.5x, /blog/_resources/a32ab208685b436bbfc4f4d10f4b46a4.png 2x"
        data-sizes="auto"
        alt="/blog/_resources/a32ab208685b436bbfc4f4d10f4b46a4.png"
        title="446b49a96d0043ab3741782300a0c351.png" /></p>
<blockquote>
<p>空洞卷积中选择D为半径的圆上的点，感受野是不是更加合理？考虑图像旋转后，方形的点位捕捉特征能力可能受到影响。
进而，用圆形的卷积效果是否会更好？运算速度呢？</p>
</blockquote>
<h2 id="习题">习题</h2>
<h4 id="习题-5-1-1证明公式-56-可以近似为离散信号序列-𝑥𝑡-关于-𝑡-的二阶微分2对于二维卷积设计一种滤波器来近似实现对二维输入信号的二阶微分">习题 5-1 1）证明公式 (5.6) 可以近似为离散信号序列 𝑥(𝑡) 关于 𝑡 的二阶微分；2）对于二维卷积，设计一种滤波器来近似实现对二维输入信号的二阶微分．</h4>
<ol>
<li>由导数定义：$x&rsquo;(t)=x(t)-x(t-1)$，进一步求二阶导数即可</li>
<li>Laplace operator
$$
\nabla^{2} f=\frac{\partial^{2} f}{\partial x^{2}}+\frac{\partial^{2} f}{\partial y^{2}}
$$
$$
\frac{\partial^{2} f}{\partial x^{2}}=f(x+1, y)+f(x-1, y)-2 f(x, y)
$$
$$
\frac{\partial^{2} f}{\partial y^{2}}=f(x, y+1)+f(x, y-1)-2 f(x, y)
$$
$$
\nabla^{2} f(x, y)=f(x+1, y)+f(x-1, y)+f(x, y+1)+f(x, y-1)-4 f(x, y)
$$</li>
</ol>
<p>所以：
$$
\mathrm{L} 0=\left[\begin{array}{ccc}
0 &amp; -1 &amp; 0 \\
-1 &amp; 4 &amp; -1 \\
0 &amp; -1 &amp; 0
\end{array}\right]
$$</p>
<h4 id="习题-5-2-证明宽卷积具有交换性即公式-513">习题 5-2 证明宽卷积具有交换性，即公式 (5.13)．</h4>
<p>抛开padding，只看有效的重叠部分，因为深度D相同，只考虑平面上观察：本质都是就是遍历了两个方形的所有重合摆放方式，将等式一边的图像均翻转180度，可以想象其遍历顺序完全相同。</p>
<h4 id="习题-5-3-分析卷积神经网络中用-1--1-的卷积核的作用">习题 5-3 分析卷积神经网络中用 1 × 1 的卷积核的作用．</h4>
<p>1x1卷积核，又称为网中网（Network in Network），对于三维输入时是一个正方形长条，可以用来升维/降维、接一个ReLU增加非线性、channal变换/通道信息交互</p>
<h4 id="习题5-4-对于一个输入为100--100--256的特征映射组使用3--3的卷积核输出为-100--100--256-的特征映射组的卷积层求其时间和空间复杂度如果引入一个-1--1-卷积核先得到-100--100--64-的特征映射再进行-3--3-的卷积得到100--100--256-的特征映射组求其时间和空间复杂度">习题5-4 对于一个输入为100 × 100 × 256的特征映射组，使用3 × 3的卷积核，输出为 100 × 100 × 256 的特征映射组的卷积层，求其时间和空间复杂度．如果引入一个 1 × 1 卷积核，先得到 100 × 100 × 64 的特征映射，再进行 3 × 3 的卷积，得到100 × 100 × 256 的特征映射组，求其时间和空间复杂度．</h4>
<ol>
<li>直接卷积
<ul>
<li>时间：$100\times 100\times 256\times 256\times 3\times 3$</li>
<li>空间：$100\times 100\times 256$</li>
<li>参数：$(3\times 3 + 1)\times 256 \times 256$</li>
</ul>
</li>
<li>先1x1卷积：
<ul>
<li>时间：$100\times 100\times 256\times 64\times 1\times 1+100\times 64\times 256\times 3 \times 3$</li>
<li>空间：$100\times 100\times 256+100\times 100\times 64$</li>
<li>参数：$(1\times 1+1)\times 256\times 64+(3\times 3 + 1)\times 64 \times 256$</li>
</ul>
</li>
</ol>
<h4 id="习题-5-5-对于一个二维卷积输入为-3--3卷积核大小为-2--2试将卷积操作重写为仿射变换的形式-参见公式545">习题 5-5 对于一个二维卷积，输入为 3 × 3，卷积核大小为 2 × 2，试将卷积操作重写为仿射变换的形式． 参见公式(5.45)．</h4>
<p>$$
\left[\begin{array}{lcccccccc}
w_{11} &amp; w_{12} &amp; 0 &amp; w_{21} &amp; w_{22} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; w_{11} &amp; w_{12} &amp; 0 &amp; w_{21} &amp; w_{22} &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; w_{11} &amp; w_{12} &amp; 0 &amp; w_{21} &amp; w_{22} &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; w_{11} &amp; w_{12} &amp; 0 &amp; w_{21} &amp; w_{22}
\end{array}\right]
$$</p>
<h4 id="习题-5-6-计算函数-𝑦--max𝑥1---𝑥𝐷-和函数-𝑦--arg-max𝑥1---𝑥𝐷-的梯度">习题 5-6 计算函数 𝑦 = max(𝑥1, ⋯ , 𝑥𝐷) 和函数 𝑦 = arg max(𝑥1, ⋯ , 𝑥𝐷) 的梯度．</h4>
<p>TODO</p>
<h4 id="习题-5-7-忽略激活函数分析卷积网络中卷积层的前向计算和反向传播公式539是一种转置关系">习题 5-7 忽略激活函数，分析卷积网络中卷积层的前向计算和反向传播（公式(5.39)）是一种转置关系．</h4>
<p>前向：
$$
z^{(l+1)}=W^{(l+1)} z^{(l)}
$$</p>
<p>反向：
$$
\delta^{(l)}=\left(W^{(l+1)}\right)^{\top} \delta^{(l+1)}
$$</p>
<h4 id="习题5-8-在空洞卷积中当卷积核大小为𝐾膨胀率为𝐷-时如何设置零填充𝑃-的值以使得卷积为等宽卷积">习题5-8 在空洞卷积中，当卷积核大小为𝐾，膨胀率为𝐷 时，如何设置零填充𝑃 的值以使得卷积为等宽卷积．</h4>
<p>$$P=(K-1)/2\times(1+D)$$</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2021-02-15</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://zubingou.github.io/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" data-title="《神经网络与深度学习》第5章 - 卷积神经网络" data-hashtags="神经网络与深度学习,NLP,notes,ML"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://zubingou.github.io/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" data-hashtag="神经网络与深度学习"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://zubingou.github.io/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" data-title="《神经网络与深度学习》第5章 - 卷积神经网络"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://zubingou.github.io/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" data-title="《神经网络与深度学习》第5章 - 卷积神经网络"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://zubingou.github.io/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" data-title="《神经网络与深度学习》第5章 - 卷积神经网络"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/blog/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">神经网络与深度学习</a>,&nbsp;<a href="/blog/tags/nlp/">NLP</a>,&nbsp;<a href="/blog/tags/notes/">Notes</a>,&nbsp;<a href="/blog/tags/ml/">ML</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/blog/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/blog/nnlp-notes/" class="prev" rel="prev" title="NNLP: A Primer on Neural Network Models for Natural Language Processing"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>NNLP: A Primer on Neural Network Models for Natural Language Processing</a>
            <a href="/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/" class="next" rel="next" title="《神经网络与深度学习》第8章 - 注意力机制和外部记忆">《神经网络与深度学习》第8章 - 注意力机制和外部记忆<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="utterances" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://utteranc.es/">utterances</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">由 <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.94.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2019 - 2022</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/blog/" target="_blank">ZubinGou</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":10},"comment":{"utterances":{"darkTheme":"github-dark","issueTerm":"pathname","label":"","lightTheme":"github-light","repo":"ZubinGou/blog-comment"}},"lightgallery":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"BGWYRG74JP","algoliaIndex":"binko","algoliaSearchKey":"1048a43ee01931f87e76ac2d1955675f","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"}};</script><script type="text/javascript" src="/blog/js/theme.min.js"></script></body>
</html>
