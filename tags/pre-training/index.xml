<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Pre-training - 标签 - Zubin`s Blog</title>
        <link>https://zubingou.github.io/blog/tags/pre-training/</link>
        <description>Pre-training - 标签 - Zubin`s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>zebgou@gmail.com (ZubinGou)</managingEditor>
            <webMaster>zebgou@gmail.com (ZubinGou)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 24 Mar 2021 14:56:11 &#43;0800</lastBuildDate><atom:link href="https://zubingou.github.io/blog/tags/pre-training/" rel="self" type="application/rss+xml" /><item>
    <title>【NLP Papers】BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
    <link>https://zubingou.github.io/blog/nlp-papersbert_-pre-training-of-deep-bidirection/</link>
    <pubDate>Wed, 24 Mar 2021 14:56:11 &#43;0800</pubDate>
    <author>ZubinGou</author>
    <guid>https://zubingou.github.io/blog/nlp-papersbert_-pre-training-of-deep-bidirection/</guid>
    <description><![CDATA[[Devlin et al., NAACL 2019]
BERT: Bidirectional Encoder representations from Transformers
1 Introduction two pre-train strategies:
 feature-based  ELMo: task-specific architecture   fine-tuning  GPT     limitations: standard language models are unidirectional masked language model (MLM, inspired by Cloze task) use a &ldquo;next sentence prediction&rdquo; task that jointly pretain text-pair representations  2 Related Work 2.1 Unsupervised Feature-based Approaches from word2vec to ELMo&hellip;
2.2 Unsupervised Fine-tuning Approaches GPT use left-to-right language modeling and auto-encoder objectives]]></description>
</item>
<item>
    <title>【NLP Papers】Contextual Word Representations: A Contextual Introduction</title>
    <link>https://zubingou.github.io/blog/nlp-paperscontextual-word-representations_-a-con/</link>
    <pubDate>Wed, 24 Mar 2021 13:56:11 &#43;0800</pubDate>
    <author>ZubinGou</author>
    <guid>https://zubingou.github.io/blog/nlp-paperscontextual-word-representations_-a-con/</guid>
    <description><![CDATA[Word Representations 综述 [Noah A. Smith, 2020] 1 Preliminaries 两种word定义： word token：word observed in a piece of text word type: distinct word, rather than a specific instance 每个word type可能有多个word token实]]></description>
</item>
<item>
    <title>【NLP Papers】ELMo: Deep contextualized word representations</title>
    <link>https://zubingou.github.io/blog/nlp-paperselmo_-deep-contextualized-word-represe/</link>
    <pubDate>Wed, 24 Mar 2021 13:56:11 &#43;0800</pubDate>
    <author>ZubinGou</author>
    <guid>https://zubingou.github.io/blog/nlp-paperselmo_-deep-contextualized-word-represe/</guid>
    <description><![CDATA[[Peters et al., NAACL 2018a]
 use bidirectional language model to train contextual word vector. use these vector as pre-train part of existing models, improve SOTA across six tasks. analysis showing that exposing the deep internals of the pre-trained network is crucial  1 Introduction pre-trained word representations should model both:
 complex characteristic of word use (e.g., syntax and semantics) how these uses vary across linguistic contexts (i.e., to model polysemy)  ELMo: Embeddings from Language Models]]></description>
</item>
<item>
    <title>基于PyTorch实现word2vec模型及其优化</title>
    <link>https://zubingou.github.io/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/</link>
    <pubDate>Mon, 08 Mar 2021 13:46:57 &#43;0800</pubDate>
    <author>ZubinGou</author>
    <guid>https://zubingou.github.io/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/</guid>
    <description><![CDATA[SkipGram NegativeSampling implemented in PyTorch. GitHub: https://github.com/ZubinGou/SGNS-PyTorch Paper Efficient Estimation of Word Representations in Vector Space (original word2vec paper) Distributed Representations of Words and Phrases and their Compositionality (negative sampling paper) Notes Word2Vec是用无监督方式从文本中学习词向量来表征语义信息的模型，语义相]]></description>
</item>
</channel>
</rss>
