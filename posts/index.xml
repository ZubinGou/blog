<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>所有文章 - Zubin`s Site</title>
        <link>https://binko.me/posts/</link>
        <description>所有文章 | Zubin`s Site</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>zebgou@gmail.com (ZubinGou)</managingEditor>
            <webMaster>zebgou@gmail.com (ZubinGou)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 20 Jul 2021 17:00:11 &#43;0800</lastBuildDate><atom:link href="https://binko.me/posts/" rel="self" type="application/rss+xml" /><item>
    <title>基于PyTorch实现BiLSTM-CRF-NER模型及其改进</title>
    <link>https://binko.me/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/</link>
    <pubDate>Tue, 16 Mar 2021 13:46:57 &#43;0800</pubDate>
    <author>作者</author>
    <guid>https://binko.me/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/</guid>
    <description><![CDATA[PyTorch implementation of BiLSTM-CRF and Bi-LSTM-CNN-CRF models for named entity recognition. GitHub: https://github.com/ZubinGou/NER-BiLSTM-CRF-PyTorch Requirements Python 3 PyTorch 1.x Papers Bidirectional LSTM-CRF Models for Sequence Tagging (Huang et al., 2015) the first paper apply BiLSTM-CRF to NER Neural Architectures for Named Entity Recognition (Lample et al., 2016) introducing character-level features: pre-trained word embedding（skip-n-gr]]></description>
</item><item>
    <title>机器学习写诗项目-AI诗人</title>
    <link>https://binko.me/project-ai-poet-totoro/</link>
    <pubDate>Sun, 10 Feb 2019 13:46:57 &#43;0800</pubDate>
    <author>作者</author>
    <guid>https://binko.me/project-ai-poet-totoro/</guid>
    <description><![CDATA[]]></description>
</item><item>
    <title>虚无、意义与存在主义 -《未来简史》读罢的思考</title>
    <link>https://binko.me/nihilism/</link>
    <pubDate>Sat, 11 May 2019 13:16:38 &#43;0800</pubDate>
    <author>作者</author>
    <guid>https://binko.me/nihilism/</guid>
    <description><![CDATA[]]></description>
</item><item>
    <title>《神经网络与深度学习》第7章 - 网络优化与正则化</title>
    <link>https://binko.me/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/</link>
    <pubDate>Tue, 20 Jul 2021 17:00:11 &#43;0800</pubDate>
    <author>作者</author>
    <guid>https://binko.me/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/</guid>
    <description><![CDATA[ch7 网络优化与正则化 任何数学技巧都不能弥补信息的缺失． —— 科尼利厄斯·兰佐斯（Cornelius Lanczos） 匈牙利数学家、物理学家 神经网络]]></description>
</item><item>
    <title>【NLP Papers】BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
    <link>https://binko.me/nlp-papersbert_-pre-training-of-deep-bidirection/</link>
    <pubDate>Wed, 24 Mar 2021 14:56:11 &#43;0800</pubDate>
    <author>作者</author>
    <guid>https://binko.me/nlp-papersbert_-pre-training-of-deep-bidirection/</guid>
    <description><![CDATA[[Devlin et al., NAACL 2019]
BERT: Bidirectional Encoder representations from Transformers
1 Introduction two pre-train strategies:
 feature-based  ELMo: task-specific architecture   fine-tuning  GPT     limitations: standard language models are unidirectional masked language model (MLM, inspired by Cloze task) use a &ldquo;next sentence prediction&rdquo; task that jointly pretain text-pair representations  2 Related Work 2.1 Unsupervised Feature-based Approaches from word2vec to ELMo&hellip;
2.2 Unsupervised Fine-tuning Approaches GPT use left-to-right language modeling and auto-encoder objectives]]></description>
</item><item>
    <title>【NLP Papers】Contextual Word Representations: A Contextual Introduction</title>
    <link>https://binko.me/nlp-paperscontextual-word-representations_-a-con/</link>
    <pubDate>Wed, 24 Mar 2021 13:56:11 &#43;0800</pubDate>
    <author>作者</author>
    <guid>https://binko.me/nlp-paperscontextual-word-representations_-a-con/</guid>
    <description><![CDATA[Word Representations 综述 [Noah A. Smith, 2020] 1 Preliminaries 两种word定义： word token：word observed in a piece of text word type: distinct word, rather than a specific instance 每个word type可能有多个word token实]]></description>
</item><item>
    <title>【NLP Papers】ELMo: Deep contextualized word representations</title>
    <link>https://binko.me/nlp-paperselmo_-deep-contextualized-word-represe/</link>
    <pubDate>Wed, 24 Mar 2021 13:56:11 &#43;0800</pubDate>
    <author>作者</author>
    <guid>https://binko.me/nlp-paperselmo_-deep-contextualized-word-represe/</guid>
    <description><![CDATA[[Peters et al., NAACL 2018a]
 use bidirectional language model to train contextual word vector. use these vector as pre-train part of existing models, improve SOTA across six tasks. analysis showing that exposing the deep internals of the pre-trained network is crucial  1 Introduction pre-trained word representations should model both:
 complex characteristic of word use (e.g., syntax and semantics) how these uses vary across linguistic contexts (i.e., to model polysemy)  ELMo: Embeddings from Language Models]]></description>
</item><item>
    <title>《神经网络与深度学习》第15章 - 序列生成模型</title>
    <link>https://binko.me/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</link>
    <pubDate>Tue, 23 Mar 2021 13:56:11 &#43;0800</pubDate>
    <author>作者</author>
    <guid>https://binko.me/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</guid>
    <description><![CDATA[类似一般概率模型，序列概率模型的两个基本问题： 概率密度估计 样本生成 15.1 序列概率模型 序列数据的概率密度估计可以转换为单变量的条件概率估计问题： $$]]></description>
</item><item>
    <title>【NLP Papers】NER：BiLSTM-CRF</title>
    <link>https://binko.me/nlp-papersnerbilstm-crf/</link>
    <pubDate>Wed, 10 Mar 2021 13:56:11 &#43;0800</pubDate>
    <author>作者</author>
    <guid>https://binko.me/nlp-papersnerbilstm-crf/</guid>
    <description><![CDATA[Neural Architectures for Named Entity Recognition [Lample et. al., 2016] 摘要 NER之前的SOTA：大量手工特征、领域知识，泛化能力差 介绍了两种模型： BiLSTM-CRF Stack-LSTM：类似移进-规约的 transition-based 方]]></description>
</item><item>
    <title>基于PyTorch实现word2vec模型及其优化</title>
    <link>https://binko.me/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/</link>
    <pubDate>Mon, 08 Mar 2021 13:46:57 &#43;0800</pubDate>
    <author>作者</author>
    <guid>https://binko.me/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/</guid>
    <description><![CDATA[SkipGram NegativeSampling implemented in PyTorch. GitHub: https://github.com/ZubinGou/SGNS-PyTorch Paper Efficient Estimation of Word Representations in Vector Space (original word2vec paper) Distributed Representations of Words and Phrases and their Compositionality (negative sampling paper) Notes Word2Vec是用无监督方式从文本中学习词向量来表征语义信息的模型，语义相]]></description>
</item></channel>
</rss>
