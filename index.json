[{"categories":["Project"],"content":"PyTorch implementation of BiLSTM-CRF and Bi-LSTM-CNN-CRF models for named entity recognition. GitHub: https://github.com/ZubinGou/NER-BiLSTM-CRF-PyTorch ","date":"2021-03-16","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/:0:0","tags":["PyTorch","LSTM","CRF","NER"],"title":"基于PyTorch实现BiLSTM-CRF-NER模型及其改进","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/"},{"categories":["Project"],"content":"Requirements Python 3 PyTorch 1.x ","date":"2021-03-16","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/:1:0","tags":["PyTorch","LSTM","CRF","NER"],"title":"基于PyTorch实现BiLSTM-CRF-NER模型及其改进","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/"},{"categories":["Project"],"content":"Papers Bidirectional LSTM-CRF Models for Sequence Tagging (Huang et al., 2015) the first paper apply BiLSTM-CRF to NER Neural Architectures for Named Entity Recognition (Lample et al., 2016) introducing character-level features: pre-trained word embedding（skip-n-gram）with character-based word embeddings trained by RNN F1: 90.94 in CoNLL 2003 (English) \u0026 91.47 in CoNLL++ End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF (Ma et al., 2016) character-level information trained by CNNs F1: 91.21 in CoNLL 2003 (English) \u0026 91.87 in CoNLL++ A Deep Neural Network Model for the Task of Named Entity Recognition （Le et al., 2018) capitalization features F1: 91.22 in CoNLL 2003 (English) ","date":"2021-03-16","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/:2:0","tags":["PyTorch","LSTM","CRF","NER"],"title":"基于PyTorch实现BiLSTM-CRF-NER模型及其改进","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/"},{"categories":["Project"],"content":"Dataset CoNLL 2003 (English) ","date":"2021-03-16","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/:3:0","tags":["PyTorch","LSTM","CRF","NER"],"title":"基于PyTorch实现BiLSTM-CRF-NER模型及其改进","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/"},{"categories":["Project"],"content":"Evaluation conlleval: Perl script used to calculate FB1 (phrase level) ","date":"2021-03-16","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/:3:1","tags":["PyTorch","LSTM","CRF","NER"],"title":"基于PyTorch实现BiLSTM-CRF-NER模型及其改进","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/"},{"categories":["Project"],"content":"Model Embeddings 100d pre-trained word embedding with Glove 25d charactor embedding trained by CNNs (Ma et al., 2016) BiLSTM-CRF (Lample et. al., 2016) ","date":"2021-03-16","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/:4:0","tags":["PyTorch","LSTM","CRF","NER"],"title":"基于PyTorch实现BiLSTM-CRF-NER模型及其改进","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/"},{"categories":["Project"],"content":"Results Trained with Tesla T4 for for one night (70 epochs), obtain 91.01% F1. ","date":"2021-03-16","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/:5:0","tags":["PyTorch","LSTM","CRF","NER"],"title":"基于PyTorch实现BiLSTM-CRF-NER模型及其改进","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/"},{"categories":["Project"],"content":"Future Works Next papers： BiLSTM-CRF+ELMo ((Peters et al., 2018) LM-LSTM-CRF (Liu et al., 2018) Flair … 中文 NER Batch training ","date":"2021-03-16","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/:6:0","tags":["PyTorch","LSTM","CRF","NER"],"title":"基于PyTorch实现BiLSTM-CRF-NER模型及其改进","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/"},{"categories":["Project"],"content":"References https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html https://github.com/ZhixiuYe/NER-pytorch ","date":"2021-03-16","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/:7:0","tags":["PyTorch","LSTM","CRF","NER"],"title":"基于PyTorch实现BiLSTM-CRF-NER模型及其改进","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0bilstm-crf-ner%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/"},{"categories":["Project"],"content":"这是我大一上学期写的AI诗人，我的第一个Python项目，也是我的深度学习和NLP启蒙项目。 项目主要涉及爬虫、自然语言处理、数据分析与可视化、规则作诗、基于TensorFlow的机器学习写诗（改改模版、调调参数、烧烧GPU），以下是实验报告的部分内容，新手诸多谬误，还望指正😂。 GitHub地址:https://github.com/ZubinGou/AI_Poet_Totoro ","date":"2019-02-10","objectID":"/blog/project-ai-poet-totoro/:0:0","tags":["Python","Seq2seq","Tensorflow","Crawler","Data Analysis"],"title":"机器学习写诗项目-AI诗人","uri":"/blog/project-ai-poet-totoro/"},{"categories":["Project"],"content":"一、作业概述 AI诗人-Totoro，她才华横溢，学富五车，能唐诗，善宋词，会元曲，可诗经，能模仿指定诗人，现代诗更是信手拈来，平仄押韵不在话下，更能情感识别、语义联想。 除却诗歌，满腹经纶的她一挥墨宝便是一曲媲美林夕的绝世歌词。 更加直观的了解她，可以参考思维导图。 ","date":"2019-02-10","objectID":"/blog/project-ai-poet-totoro/:1:0","tags":["Python","Seq2seq","Tensorflow","Crawler","Data Analysis"],"title":"机器学习写诗项目-AI诗人","uri":"/blog/project-ai-poet-totoro/"},{"categories":["Project"],"content":"数据抓取 网易云音乐 数据类型：歌词 数据包括：网易云音乐所有华语歌手、乐队组合的共6945位歌手的全部总计大约23.4万首歌的歌词（也即网易云音乐几乎所有中文专辑的含有歌词的歌曲的歌词），并且分为男、女和乐队组合三类。 数据大小：歌词234870首，爬取数据309MB 中国诗歌网 数据类型：现代诗 数据包括：中国诗歌网几乎全部现代诗，约22.4万首。 数据大小：现代诗224332首，总计3755249行，爬取数据116.5MB 古诗文网 数据类型：古诗词 数据包括：古诗词网推荐栏的全部总计10000首诗，以及诗文栏的全部“猜你喜欢”部分包含的所有总计7593首最富盛名的诗篇。 数据大小：推荐栏10000首，诗文栏7593首，合并后一共14551首诗歌，数据处理后4647KB。 ","date":"2019-02-10","objectID":"/blog/project-ai-poet-totoro/:1:1","tags":["Python","Seq2seq","Tensorflow","Crawler","Data Analysis"],"title":"机器学习写诗项目-AI诗人","uri":"/blog/project-ai-poet-totoro/"},{"categories":["Project"],"content":"规则写诗 1. poetize.py 只需双击运行即可根据提示生成诗歌 输入：序号用来选择语料库、格式要求等 也可以自定义格式：输入词性对应的简码，自动生成古诗、词、诗经和现代诗，详见代码以及自由创作时根据提示输入h获取的帮助。 输出：符合要求的唐诗、宋词、宋诗、诗经、现代诗 2. poetize_plus.py 在之前的基础上加入了平仄和押韵，平仄每个字都严格遵守古诗平仄要求，押韵严格遵守十三辙要求，并且符合根据首句是否押韵而平仄不同的古诗音韵和谐的要求。 还自定义韵脚，方式为输入一串字符，程序将自动识别最后一个字的韵脚在十三辙中属于第几辙，生成押这个辙的古诗。 接入了自然语言处理库snowNLP，用于情感倾向分析。Demo： ","date":"2019-02-10","objectID":"/blog/project-ai-poet-totoro/:1:2","tags":["Python","Seq2seq","Tensorflow","Crawler","Data Analysis"],"title":"机器学习写诗项目-AI诗人","uri":"/blog/project-ai-poet-totoro/"},{"categories":["Project"],"content":"机器学习写诗 1. chinese_poem 根据输入的诗人，读模仿其风格创作诗词 输入：python main.py -m train/test/head -p 李白/杜甫/苏轼/白居易...... 输出：模仿该诗人风格的诗词 创作前首先进行train训练，250个epoch左右就基本拟合了。理论上可以模仿任何产量不是太低的诗人，只需简单修改dataset内的“提取诗人.py”文件，即可提取其诗歌，然后按照上述输入格式运行训练即可，不用更改main.py文件。 2. lyrics_writer 输入语料库类型和开头，自动写歌词/现代诗 输入：python main.py 0/1 -cd 华语男歌手/华语女歌手/modern/华语乐队与组合 输出：给定开头的歌词/现代诗 0表示训练，1表示创作，-cd传入训练的内容，无需更改文件。 3. poet_master 机器学习创作自由诗、押韵诗、藏头诗、藏字诗 输入： 自由诗： python main.py 1 押韵诗： python main.py 2 藏头诗： python main.py 3 四个汉字 藏字诗： python main.py 4 随意数量汉字，将尽可能全部覆盖 更改自github上star上百的在网页上写诗的小诗姬，改成了命令行版本。 ","date":"2019-02-10","objectID":"/blog/project-ai-poet-totoro/:1:3","tags":["Python","Seq2seq","Tensorflow","Crawler","Data Analysis"],"title":"机器学习写诗项目-AI诗人","uri":"/blog/project-ai-poet-totoro/"},{"categories":["Project"],"content":"二、数据分析 ","date":"2019-02-10","objectID":"/blog/project-ai-poet-totoro/:2:0","tags":["Python","Seq2seq","Tensorflow","Crawler","Data Analysis"],"title":"机器学习写诗项目-AI诗人","uri":"/blog/project-ai-poet-totoro/"},{"categories":["Project"],"content":"数据集词云分析 现代诗词云：可见一开头的双字词最为现代诗人所喜爱，生命、落叶等表面现代诗的哲思性，以及思念、温暖、温柔展现了诗歌抒情的一面。 诗经词云：君子、四方、我心在意料之内，这个“上帝”出乎意料，查阅资料发现，诗经中绝大部分“上帝”都是指至高神。有几处“上帝”被汉儒解释为“君主”，也是借至高神上帝之地位、权势喻指君主。可以对古代的神学与宗教管窥蠡测一般。 部分歌词词云：主要是周杰伦的歌词，可以看到，Jay Chou最爱用“我们”、“没有”、“自己”、“知道”这类的字眼(未设置停用词的结果)。 唐诗词云 宋词词云 ","date":"2019-02-10","objectID":"/blog/project-ai-poet-totoro/:2:1","tags":["Python","Seq2seq","Tensorflow","Crawler","Data Analysis"],"title":"机器学习写诗项目-AI诗人","uri":"/blog/project-ai-poet-totoro/"},{"categories":["Project"],"content":"云音乐数据处理与分析 对网易云爬取的全部诗歌进行了清洗、繁简替换、去重并按照歌手类型整合、分词，接下来进行数据分析，由于生成的是html格式，并有动画效果和详尽标签，下面仅展示截图。 华语女歌手字频 华语男歌手字频 乐队\u0026组合字频 我们可以看到不论男女歌手还是乐队，前5高频字大致相同，很有趣的是男女歌手使用“爱”字频率较高，然而乐队组合使用“爱”字频率却大幅下降，难道一群人就不好意思谈情说爱了吗？ 再来看词频分析： 女歌手词频 男歌手词频 乐队组合词频 女歌手词云 男歌手词云 我们（胡乱）分析出以下特点： 乐队组合使用“我们”的频率明显高于男女歌手，可见乐队的团结 就形容词而言，女歌手爱用“永远”、“幸福”、”快乐”、”寂寞”、”美丽”，而男歌手似乎没有那么善于描述,爱用“幸福”、“快乐”和生怕别人不相信的“真的”，而乐队很有趣的不谈“幸福”，爱说“快乐“，可见有一群小伙伴都忘却“爱情”和“幸福”而十分“快乐”了，嘿嘿。 整体来看，男歌手使用“不是”、“不要”、“就是”等理性的判断词要高于女歌手，的确我们的男同志们是要“理智”一点呀！ 女歌手的“心里”等表现出女生更喜欢表述自己，而男歌手的理性词则反映其更爱描述客观真理和事实。 整体上男女有理性和感性的倾向差别。 ","date":"2019-02-10","objectID":"/blog/project-ai-poet-totoro/:2:2","tags":["Python","Seq2seq","Tensorflow","Crawler","Data Analysis"],"title":"机器学习写诗项目-AI诗人","uri":"/blog/project-ai-poet-totoro/"},{"categories":["Project"],"content":"唐宋诗词文本挖掘 由于图表比较多，均为html文件，可以点击文件查看获取详尽数据。这里受制于篇幅，仅分析其中一部分。可以运行文件目录下的.py文件，生成图表，同时打印出对应的数据。 1. 作家产量分析-谁是产量之王 唐诗高产作家 宋诗高产作家 宋词高产作家 就唐诗而言，“七分剑气，三分月光，秀口一吐，半个盛唐”，我们的李杜诗篇万口传，但白居易写诗，产量之高，在唐代诗人中无人能敌。 再看宋诗，陆游一生笔耕不辍，作诗达万余首，自称“六十年间万首诗”，存世的诗仍有九千三百余首，是历史上存诗数量最多的诗人，当之无愧的榜首。但经查阅资料，发现一位英雄被我的数据所“埋没”了：历史上写诗数量最多的诗人—杨万里，屈居第三，杨万里一生极为勤奋，写诗达两万余首，为史上之最！可惜的是，其诗作传世的仅有四千余首，所有我得到的数据中他的诗也就没那么多了，相比之下反而让陆游拔得头筹！ 最后看宋词，山东诗人辛弃疾的传世作品共有800多首，堪称词作高产作家，而苏轼也不甘落后，位居第三。 2. 单字词字频分析-古代诗人最爱用的字眼 唐诗单字词字频 宋诗单字词字频 宋词单字词字频 从词云中看到，不论唐诗、宋词还是宋诗，最常用的单字词都是“人”、“月”、“风”、“云”、“梦“等，“人”字排行第一，这体现了《说文解字》里所讲的“人，天地之性最贵者也”，说明唐诗很好的秉承了“以人为本”的中华文化。而后续的“山”“风”“月”“日”“天”“云”“春”等都是在写景的诗句里经常出现的意象。 3. 多字词字频分析-诗人最爱用的词 唐诗词频 宋诗词频 宋词词频 可以看到，关于“风”的词是唐宋诗词人最爱用的，“东风”、“西风”、“春风”、“秋风”、“风月”、“风雨”甚至还有“风流”，看来诗人们对这些自然景物情有独钟，诸如“白云”、“梅花”、“芳草”等等。 4. 宋词最受欢迎的词牌 宋词最受欢迎的词牌 《浣溪沙》受到宋代婉约、豪放两派的共同青睐， 高居榜首，同时《水调歌头》、《鹧鸪天》、《满江红》等也是我们耳熟能详的。 5. 诗人最爱用的动词 唐诗动词排行 宋诗动词排行 宋词动词排行 我们很明显看到，诗人更偏爱“不”字系的词语，而词人更爱用“归”字系的词词，这可能和宋代婉约词派的有关。 从“相思”、“相逢”、“回首”、“别离”来看，表现了诗词的“人本”情怀，还是以抒发情感为主。 6. 诗人最爱用的形容词 唐诗形容词排行 宋词形容词排行 毋庸置疑，诗人最爱用的形容词是“寂寞”，诗人们还真都挺孤独的。 对比唐诗两朝诗词，很明显的发现宋代诗词中“富贵”一词的频率相比之下极高，为什么呢？根据时代背景推测，唐朝太平盛世，而宋朝战乱纷纷，黎民饥寒，饿殍遍野，尤其南宋更是迁都临安，几近灭亡，如此纷乱之世，自然激起了诗人们的同情，宋代诗词人们一方面描绘“凄凉”、“萧条”之现世，一方面也揭露王孙子弟的“富贵”背后的丑恶，也盼望庶民在乱世饥寒中能够“富贵”起来。（一本正经地扯蛋…） 7. 诗中最常见的地名 宋词人还是偏爱“江南”的，不禁让人想起小学的宋词：忆江南： 江南好，风景旧曾谙。日出江花红胜火，春来江水绿如蓝。能不忆江南？ 8. 诗中的四季 自古逢秋悲寂寥，我言秋日胜春朝。在古诗词里，伤春、惜春是常见的春诗题材，而萧瑟凄凉的秋日更是引起诗人“诗兴大发”的季节，它们占比如此之高也不足为奇，而对于炎热的夏天和寒冷的冬天，诗人似乎没有那么多情怀了，并且随着时代推移，“冬” 和“夏”的使用频率也越来越少，或许是诗人们更喜欢用“接天莲叶无穷碧，映日荷花别样红”这样的意象来描绘这两个季节吧！ ","date":"2019-02-10","objectID":"/blog/project-ai-poet-totoro/:2:3","tags":["Python","Seq2seq","Tensorflow","Crawler","Data Analysis"],"title":"机器学习写诗项目-AI诗人","uri":"/blog/project-ai-poet-totoro/"},{"categories":["Project"],"content":"三、算法比较 ","date":"2019-02-10","objectID":"/blog/project-ai-poet-totoro/:3:0","tags":["Python","Seq2seq","Tensorflow","Crawler","Data Analysis"],"title":"机器学习写诗项目-AI诗人","uri":"/blog/project-ai-poet-totoro/"},{"categories":["Project"],"content":"规则写诗 规则写诗用到了jieba的关键词提取算法textrank和extract_tags，将每种词性的词分别提取出来，然后采用一种词性模式匹配的算法，比如词性序列“aannv, vdsss”,最大长度地匹配相同字符长度，从“aa”开始，在语料库中随机匹配2个字长度的形容词，然后到继续匹配后面的序列，这样就可生成诗歌。 词性对应下图。每个词性都是一类词的全体集合。 采用这种“万能的”写诗方法，还可以在内部内置词牌的格式，如下图。用户也可以自由输入模式，自动生成诗歌。 押韵部分采用了严格的“十三辙”分类要求，如下图所示。 押韵也分为首句押和首句不押，对应的平仄不同，严格遵守了音韵学的要求，但是受制于语料库只提取了1000词左右，有些韵脚可能会无法匹配，我设置了超过一定匹配次数，就停止匹配。 对于平仄部分，同样完美符合古诗词平仄的要求。算法也是采用了模式匹配的办法。 以从语料库中匹配到的词为单位，这样算法复杂度更低。利用如下函数判断： 输入为词和对应的平仄要求，依次判断每个字是否满足要求。 另外由于诗个平仄音韵的要求根据首句是否押韵也不同，程序也巧妙地实现了，代码有详尽注释，不再赘述。 简要流程图如下： ","date":"2019-02-10","objectID":"/blog/project-ai-poet-totoro/:3:1","tags":["Python","Seq2seq","Tensorflow","Crawler","Data Analysis"],"title":"机器学习写诗项目-AI诗人","uri":"/blog/project-ai-poet-totoro/"},{"categories":["Project"],"content":"机器学习写诗 由于使用的模型较多，在此选择几个做简单分析。代码中加有注释。 1. chinese_poem 使用了LSTM神经网络，基于tensorflow的机器学习写诗模型。我利用它来做了“诗人模仿”功能。 首先利用一个简单的筛选判断，自动把需要模仿的诗人的数据提取出来： 只需修改poets列表内容，即可提取任何你想要的诗人数据。 使用的数据是我之前处理过的干净数据。 然后是训练模型，用[]标志诗位置，喂给搭建好的模型。 250个epoch即可基本拟合。 如上，按照字频来选择，这样概率大的输出概率也更加大。添加随机性的同时保证了诗歌质量。 自由写诗生成generateNum这么多首诗，每首诗以左中括号开始，以右中括号或空格结束，每次生成的prob用probsToWord方法转成字。 生成完整诗歌后使用output来使输出更美观： 藏头诗也很简单，根据标点符号指点诗歌开头为用户输入的字，在模型输出后面的字即可。 流程图如下： 2. lyrics_writer 训练数据使用的是爬取并处理后的歌词和现代诗。 使用的了sequence-to-sequence模型以及RNN网络进行歌词语义的学习以及建模，和之前的模型相似。 参数设置： 关于生成诗歌模块： 引入随机长度，增加诗歌和歌词的随机性，然后采用了正态分布的概率来生成每个字。 ","date":"2019-02-10","objectID":"/blog/project-ai-poet-totoro/:3:2","tags":["Python","Seq2seq","Tensorflow","Crawler","Data Analysis"],"title":"机器学习写诗项目-AI诗人","uri":"/blog/project-ai-poet-totoro/"},{"categories":["Project"],"content":"四、结果分析 ","date":"2019-02-10","objectID":"/blog/project-ai-poet-totoro/:4:0","tags":["Python","Seq2seq","Tensorflow","Crawler","Data Analysis"],"title":"机器学习写诗项目-AI诗人","uri":"/blog/project-ai-poet-totoro/"},{"categories":["Project"],"content":"规则写诗结果分析 1．选择五言绝句、从头到尾每个字都符合平仄的唐诗： 2．自定义韵脚、每个字都严格符合平仄要求的宋诗： 3．指定词牌名作的宋词： 4．指定行数的诗经： 5． 指定句数的现代诗 ： 我最喜欢的是这首： 6．自定义模式的唐诗、宋词、宋诗、诗经和现代诗： 自定义格式藏头、藏尾、藏字诗： 自定义格式的宋词： 自定义格式的诗经： 自定义格式的现代诗： ","date":"2019-02-10","objectID":"/blog/project-ai-poet-totoro/:4:1","tags":["Python","Seq2seq","Tensorflow","Crawler","Data Analysis"],"title":"机器学习写诗项目-AI诗人","uri":"/blog/project-ai-poet-totoro/"},{"categories":["Project"],"content":"机器学习写诗结果分析 Chinese_poem –模仿诗人 模仿李白风格： 的确从遣词造句上有一股诗仙的侠气和酒气。 李白的藏头词： 再来学习杜甫： 能够明显读出忧国忧民、感时伤事的情怀。 程序默认诗人为李白，传入 -p 参数即可指定其他诗人。 lyrics_writer –作词家 Ta分类学习了网易云所有男歌手、女歌手和乐队组合的歌曲，也可以进行现代诗创作。 歌词风格华语男歌手： 女歌手： 0表示训练，1表示写歌词。cd参数传入语料库名称。 这个model训练次数比较少（主要因为数据量太大，训练巨慢），尚未拟合。 因为歌词里面关于爱情的字眼非常多，所以这个模型训练出来最喜欢“谈情说爱”，也就显得有那么一点“人工智障”的意味；生成歌词的长度也比较短。 poet_master 机器学习创作自由诗、押韵诗、藏头诗、藏字诗。 - 自由创作： 上面这首诗读起来颇有一番韵味，似乎在感叹嫦娥的身世浮沉。 - 押韵诗： - 藏头押韵诗： 上面这首诗也颇有几分年老体病的诗人，在人生弥留之际，回忆往事纷扰，感慨人生飞逝、世殊时异的伤怀意味。 - 藏字押韵诗： 可以看到，算法最大化的将输入的句子嵌入了诗中。 生成的诗歌不具有深刻语义、且上下文联系不紧密。 ","date":"2019-02-10","objectID":"/blog/project-ai-poet-totoro/:4:2","tags":["Python","Seq2seq","Tensorflow","Crawler","Data Analysis"],"title":"机器学习写诗项目-AI诗人","uri":"/blog/project-ai-poet-totoro/"},{"categories":["Project"],"content":"总结 综合这上面的demo，比较之下，我们的规则写诗做到了：指定格式、押韵、平仄、藏头、藏尾、藏字、自定义格式等。 而机器学习写诗做到了指定格式、押韵、藏头、藏字、模仿诗人、以及更高级的语义分析、联想作诗还有更复杂文本的生成等。 参考项目： https://github.com/hjptriplebee/Chinese_poem_generator https://github.com/norybaby/poet 更高级的机器学习写诗，可以参考清华的九歌。 ","date":"2019-02-10","objectID":"/blog/project-ai-poet-totoro/:4:3","tags":["Python","Seq2seq","Tensorflow","Crawler","Data Analysis"],"title":"机器学习写诗项目-AI诗人","uri":"/blog/project-ai-poet-totoro/"},{"categories":["Essay"],"content":"我曾一度陷入虚无主义。 曾几何时，思索、彷徨、恐惧。寄蜉蝣于天地，渺沧海之一粟。哀吾生之须臾，羡长江之无穷。我们终究在茫茫宇宙中消逝，不激起一丝涟漪，荡然无存哉！ 这是在幼年和少年的我脑中徘徊、周旋的一大命题。 当我认真思索，往往发现所谓的“人生的意义”往往是虚构的、人为设定的，“我与我周旋久”，却不知如何做我。邂逅存在主义，加缪、萨特都甚得我心，“人是在无意义的宇宙中生活，人的存在本身也没有意义，但人可以在原有存在的基础上自我塑造、自我成就，活得精彩，从而拥有意义。”我认为人生本质上没有意义，是我们赋予了其意义，那么我们终将面临死亡，人活着有什么意义呢？对死亡的焦虑与恐惧，在虚无中竭力嘶吼，却发现被扼住了喉咙，恰如一种鬼压床的惊悚体验。从心理学的角度，这或许只是低多巴胺水平者在无所事事时的臆想，但这十足困扰了我很多年。 知乎上关于人生意义的答案往往诉诸享乐主义，做疯狂的智者，还是健全的蠢货？而《存在主义心理治疗》却在选择了“逃避”—“面对无意义，人只能让自己沉浸在生活的洪流之中，让疑问随水流逝。”而这不是我要的答案。 可我愈是寻觅，愈是”求而不得“。而身边的朋友似乎很少有这种困惑，我亦难觅共鸣。读《月亮与六便士》的一段话，感触颇深: 我们每个人生在世界上都是孤独的。每个人都被囚禁在一座铁塔里，只能靠一些符号同别人传达自己的思想；而这些符号并没有共同的价值，因此它们的意义是模糊的、不确定的。我们非常可怜地想把自己心中的财富传送给别人，但是他们却没有接受这些财富的能力。因此我们只能孤独的行走，尽管身体相互依傍却并不在一起，既不了解别的人也不能为别人所了解。 我们仿佛生来就是无意义的孤岛。 心理学课的老师探讨了“人生脚本”，让我印象深刻，“人生脚本”是我们童年时针对自己的一生所作出的无意识的计划和决定，作为一种生存策略，包括了一些核心信念，被父母所强化，从生活的经验中得到“证实”，经过选择而达到高潮。如果没有个人和外界的有意干预，它会走向预定的结局。这不是在谈“宿命论”，而是说环境作用于潜意识，而潜意识规划人生。这更加印证了我的想法。 后来，我接触了尤瓦尔·赫拉利这位犀利的历史学家，我发现他比我想得通透得多。首先是他击碎了我关于人类自由意志的残余幻想，他指出了自由意志的假象（即物理主义）：简单来讲，人是环境的产物，是自然规律的一部分，镂刻在深处的基因与环境的输入加上一些随机因素，产生的生化反应决定了我们的行为，然后我们再以为这是自己的“自由意志”所决定的行为（著名而骇人的裂脑实验）。人类和其他生物一样，都知识一堆算法，所有的生命都是在进行数据处理。仿佛人类像个机器学习model，在训练中不断拟合，由广义的应激反应所主宰（诚然我们大多数行为都是潜意识的不自觉行为）。 更有甚者，他直白的谈到: 人类没有灵魂，也没有心灵。事实上，人类有的就是一条意识流，欲望会在这条意识流中起伏来去，并没有什么永远不变的自我能够拥有这些欲望。 我们自诩万物之长，实则同鸟兽无异。 为什么思而不得，愈是思考与寻求意义，愈是不得、愈是迷惘呢？ 因为，从本质上讲，人生没有任何意义，而追寻意义这一行为也是荒诞的。这样每个人都在无意义的基础上赋予了生命以自己构想的“意义”（以及集体构建的种种“故事”），故而每个人甚至能有不同的意义（意义依附于人，存在先与本质）。 寻求意义这一行为来自有了“我”之后，并不是因为某种意义而存在，而是因为存在而且寻找意义。意义的存在，很大程度上源于对终将消亡的宿命的恐惧。 愈是被赋予巨大意义的事物，往往愈是缺乏意义、趋于消亡而没有本身没有意义（譬如爱情、贫穷）。因为意义的存在，不过是为了欺骗低多巴胺水平者（还是自然选择），抵抗其内在的空虚与自我价值的迷失，使之能继续存在而传递基因（“爱情不过是一种肮脏的诡计，它欺骗我们去完成传宗接代的任务”，瞧瞧毛姆这毒舌!🤣）。这些本身都是自然规律的一部分，我们按照自然规律存在与消亡，而人类为了自欺强行赋予了其伟大的意义。 存在主义所谓在无意义之上”赋予意义”，其实是心理需求的外在体现（马斯洛），终究是为基因服务。这样来看，萨特的自由、存在与个人，尼采的“强力”与超人，嘿嘿，也不过尔尔。（望哲人海涵业余人士在此“妄加评判”。） 而我也发现，这些想法与《自私的基因》中解剖的人类行为相契合，我们的存在，不过是服务于演化，我们的行为，也深刻的桎梏于基因。 由此观之，从某种程度上讲，人生脚本即是人这一自然规律通过所谓的“基本驱力”驱动的外在体现。我们通过构想“意义”来解释脚本，实则因果倒置了，有趣的是，这种事后解释而不自知的行为恰如从“裂脑人”更底层的自我欺骗中分形出来的。 所以就真的“没意义”了么？或者说，没有意义是可怕、令人悲伤的吗？ 恰恰相反。没有所谓的意义，正如加缪所言“人生越没有意义越值得过”，白纸上作画，我们会更加自由，摆脱种种囹圄，超脱即是如斯。王小波说“人都是为了要表演，失去了自己的存在”，而接纳人生在本质上不具有意义，接纳自己不需要意义，“我”本是自然规律的一部分，这样，我们会从本真上热爱生活。 后记 这些思考恐怕难以引起广泛的共鸣。首先是不同人看待问题的方式和角度不同，因为这个”意义“的定义因人而异，有人硬要说他存在的意义就是追求xx（大多是多巴胺成瘾，反作用驱动并强化行为，为了解释行为而赋予意义）,而这也无可厚非；其次，因为自然选择淘汰了认为人生”无意义“者，而留下了”快乐“的人们，加上现代社会泛娱乐、享乐主义与消费主义的盛行，绝大多数人从来不缺多巴胺，以至于他们不会去思索、纠结这类问题。 这种“无意义”价值观并非站在鄙视链顶端否定高多巴胺水平者为自己的生命所赋予的种种意义（享乐主义、虚无主义和自由主义等及其副产物–“充实感”），在我看来，每个人都可以选择自己的价值观念和生活方式。 参考 尤瓦尔, 赫拉利. 未来简史[J]. 全国新书目, 2017 (2): 26-27. R.道金斯, 道金斯, 卢允中,等. 自私的基因[M]. 1956. 肖静宁. “裂脑人”的研究及其哲学思考[J]. 武汉大学学报：人文科学版, 1985(4):39-45. ","date":"2019-05-11","objectID":"/blog/nihilism/:0:0","tags":["books","philosophy"],"title":"虚无、意义与存在主义 -《未来简史》读罢的思考","uri":"/blog/nihilism/"},{"categories":["Deep Learning"],"content":"10.1 集成学习 M 个模型在同一任务上的期望错误： $$ \\begin{aligned} \\mathcal{R}\\left(f_{m}\\right) \u0026=\\mathbb{E}_{\\boldsymbol{x}}\\left[\\left(f_{m}(\\boldsymbol{x})-h(\\boldsymbol{x})\\right)^{2}\\right] \\\\ \u0026=\\mathbb{E}_{\\boldsymbol{x}}\\left[\\epsilon_{m}(\\boldsymbol{x})^{2}\\right] \\end{aligned} $$ 则所有模型平均错误： $$ \\overline{\\mathcal{R}}(f)=\\frac{1}{M} \\sum_{m=1}^{M} \\mathbb{E}_{\\boldsymbol{x}}\\left[\\epsilon_{m}(\\boldsymbol{x})^{2}\\right] $$ 集成学习（Ensemble Learning）：群体决策提高准确率。 集成策略 直接平均： $$ F(\\boldsymbol{x})=\\frac{1}{M} \\sum_{m=1}^{M} f_{m}(\\boldsymbol{x}) $$ 可以证明： $$ \\overline{\\mathcal{R}}(f) \\geq \\mathcal{R}(F) \\geq \\frac{1}{M} \\overline{\\mathcal{R}}(f) $$ 有效的集成需要基模型的差异尽可能大： Bagging 类方法 Bagging（Bootstrap Aggregating）：对原训练集有放回采用得到 M 个较小数据集，并训练 M 个模型 随机森林（Random Forest）：在 Bagging 基础上再引入随机特征，每个基模型都是一棵决策树 Boosing 类方法：后面的模型对前序模型的错误进行专门训练，即根据前序模型结果增加分错训练样本权重。eg. AdaBoost ","date":"2021-08-01","objectID":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/:1:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第10章 - 模型独立的学习方式","uri":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/"},{"categories":["Deep Learning"],"content":"10.1.1 AdaBoost 方法 Boosting 类学习目标：加性模型（Additive Model），即弱分类器加权得到强分类器： $$ F(\\boldsymbol{x})=\\sum_{m=1}^{M} \\alpha_{m} f_{m}(\\boldsymbol{x}) $$ AdaBoost（Adaptive Boosting）算法：加法模型，指数损失函数，前向分步（Stage-Wise）优化的二类分类学习方法。 ","date":"2021-08-01","objectID":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/:1:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第10章 - 模型独立的学习方式","uri":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/"},{"categories":["Deep Learning"],"content":"10.2 自训练和协同训练 半监督学习（Semi-Supervised Learning，SSL）：利用少量标注数据和大量无标注数据学习。 ","date":"2021-08-01","objectID":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/:2:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第10章 - 模型独立的学习方式","uri":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/"},{"categories":["Deep Learning"],"content":"10.2.1 自训练 自训练（Self-Training，或 Self-Teaching）：自举法（Bootstrapping），将预测置信度高的样本及其伪标签加入训练集，然后重新训练，不断反复。 自训练与密度估计中 EM 算法相似，都是通过不断迭代提高模型能力。 自训练缺点：无法保证伪标签正确性，可能反倒损害模型能力。关键是设置挑选样本的标准。 ","date":"2021-08-01","objectID":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/:2:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第10章 - 模型独立的学习方式","uri":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/"},{"categories":["Deep Learning"],"content":"10.2.2 协同训练 协同训练（Co-Training）：自训练的一种改进，两个基于不同**视角（View）**的模型相互促进。 视角：如网页分类，采用文字还是链接进行分类 条件独立性：给定标签 y，两种特征条件独立，$p\\left(\\boldsymbol{x}_{1}, \\boldsymbol{x}_{2} \\mid y\\right)=p\\left(\\boldsymbol{x}_{1} \\mid y\\right) p\\left(\\boldsymbol{x}_{2} \\mid y\\right)$ 充足和冗余性：数据充足时每个视角都可以训练出正确分类器。 协同训练步骤：训练不同视角两个模型，在无标注数据集预测并各自选取置信度较高的样本加入训练集，重新训练两个模型，不断反复。 ","date":"2021-08-01","objectID":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/:2:2","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第10章 - 模型独立的学习方式","uri":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/"},{"categories":["Deep Learning"],"content":"10.3 多任务学习 相关任务的共享知识：表示（即特征，主要关注点）、模型参数、学习算法 多任务学习（Multi-task Learning）：学习多个任务共享知识，利用任务相关性提高各任务性能、泛化能力。 多任务学习可以看作一种归纳迁移学习（Inductive Transfer Learning）：利用包含在相关任务中的信息作为归纳偏置（Inductive Bias）来提高泛化能力 主要挑战：设计共享机制 学习步骤： 通常使用交替训练近似同时学习，联合目标函数为各任务目标函数加权。 学习流程： 联合训练阶段 单任务精调阶段（可选） ","date":"2021-08-01","objectID":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/:3:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第10章 - 模型独立的学习方式","uri":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/"},{"categories":["Deep Learning"],"content":"10.4 迁移学习 领域（Domain）：$\\mathcal{D}=(\\mathcal{X}, y, p(\\boldsymbol{x}, y))$，输入空间、输出空间、概率分布，任意一个不同即是不同领域。从统计学习的观点来看，一个机器学习任务 𝒯 定义为在一个领域 𝒟 上的条件概率 𝑝(𝑦|𝒙) 的建模问题。 迁移学习：两个不同领域的知识迁移过程 根据迁移方式分类： 归纳迁移学习（Inductive Transfer Learning）：在源领域和任务上学习出一般的规律 相同输入空间、不同目标任务（概率分布) 转导迁移学习（Transductive Transfer Learning）：样本到样本的迁移，直接利用源领域和目标领域的样本学习 分别对应两个机器学习范式：：归纳学习（Inductive Learning）和转导学习（Transductive Learning） 归纳学习：期望风险最小（真实数据分布错误率） 转导学习：给定测试集错误率最小，训练时可以利用测试集信息 ","date":"2021-08-01","objectID":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/:4:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第10章 - 模型独立的学习方式","uri":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/"},{"categories":["Deep Learning"],"content":"10.4.1 归纳迁移学习 要求源领域和目标领域是相关的，并且源领域有大量的训练样本（标注或无标注） 迁移方式： 特征提取：将预训练模型输出或中间隐层输出作为特征直接加入目标任务模型。 精调：复用预训练模型部分组件，并精调 归纳迁移学习 vs. 多任务学习 多任务学习同时学习，归纳迁移学习分阶段学习 多任务学习希望提高所有任务表现，归纳迁移学习是单向迁移，希望提高目标任务性能。 ","date":"2021-08-01","objectID":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/:4:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第10章 - 模型独立的学习方式","uri":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/"},{"categories":["Deep Learning"],"content":"10.4.2 转导迁移学习 转导迁移学习：从样本到样本的迁移，直接利用源领域和目标领域的样本进行迁移学习。 转导迁移学习通常假设源领域有大量的标注数据，而目标领域没有（或只有少量）标注数据，但是有大量的无标注数据．目标领域的数据在训练阶段是可见的。 常见子问题：领域适应（Domain Adaptation），相同样本空间、不同数据分布 $p_{S}(\\boldsymbol{x}, y) \\neq p_{T}(\\boldsymbol{x}, y)$ 协变量偏移（Covariate Shift）：输入边际分布不同 $p_{S}(\\boldsymbol{x}) \\neq p_{T}(\\boldsymbol{x})$ 概念偏移（Concept Shift）：后验分布不同 $p_{S}(y \\mid \\boldsymbol{x}) \\neq p_{T}(y \\mid \\boldsymbol{x})$，即学习任务不同 先验偏移（Prior Shift）：输出标签 y 的先验分布不同 $p_{S}(y) \\neq p_{T}(y)$ 多数领域适应问题主要关注协变量偏移，关键在于如何学习领域无关（Domain-Invariant）的表示 领域适应的目标，学习模型使得： $$\\begin{aligned} \\mathcal{R}_{T}\\left(\\theta_{f}\\right) \u0026=\\mathbb{E}_{(x, y) \\sim p_{T}(\\boldsymbol{x}, y)}\\left[\\mathcal{L}\\left(f\\left(\\boldsymbol{x} ; \\theta_{f}\\right), y\\right)\\right] \\\\ \u0026=\\mathbb{E}_{(x, y) \\sim p_{S}(x, y)} \\frac{p_{T}(\\boldsymbol{x}, y)}{p_{S}(\\boldsymbol{x}, y)}\\left[\\mathcal{L}\\left(f\\left(\\boldsymbol{x} ; \\theta_{f}\\right), y\\right)\\right] \\\\ \u0026=\\mathbb{E}_{(x, y) \\sim p_{S}(\\boldsymbol{x}, y)} \\frac{p_{T}(\\boldsymbol{x})}{p_{S}(\\boldsymbol{x})}\\left[\\mathcal{L}\\left(f\\left(\\boldsymbol{x} ; \\theta_{f}\\right), y\\right)\\right] \\end{aligned}$$ 如果可以学习一个映射函数，使得映射后特征空间中源领域和目标领域的边际分布相同 $p_{S}\\left(g\\left(\\boldsymbol{x} ; \\theta_{g}\\right)\\right)=p_{T}\\left(g\\left(\\boldsymbol{x} ; \\theta_{g}\\right)\\right)$，设 $\\theta_g$ 为映射函数的参数，则目标函数可以近似为： $$ \\begin{aligned} \\mathcal{R}_{T}\\left(\\theta_{f}, \\theta_{g}\\right) \u0026=\\mathbb{E}_{(\\boldsymbol{x}, y) \\sim p_{S}(\\boldsymbol{x}, y)}\\left[\\mathcal{L}\\left(f\\left(g\\left(\\boldsymbol{x} ; \\theta_{\\mathrm{g}}\\right) ; \\theta_{f}\\right), y\\right)\\right]+\\gamma d_{\\mathrm{g}}(S, T) \\\\ \u0026=\\mathcal{R}_{S}\\left(\\theta_{f}, \\theta_{\\mathrm{g}}\\right)+\\gamma d_{\\mathrm{g}}(S, T), \\end{aligned} $$ 学习目标：1. 提取特征是领域无关的 2. 源领域损失最小 分布差异计算： MMD（Maximum Mean Discrepancy）[Gretton et al., 2007] CMD（Central Moment Discrepancy）[Zellinger et al., 2017] 对抗学习：引入领域判别器，若无法判断则认为该特征领域无关 源和目标领域训练数据： $$ \\begin{aligned} \u0026\\mathcal{D}_{S}=\\{(\\boldsymbol{x}_{S}^{(n)}, y_{S}^{(n)})\\}_{n=1}^{N} \\sim p_{S}(\\boldsymbol{x}, y) \\\\ \u0026\\mathcal{D}_{T}=\\{\\boldsymbol{x}_{T}^{(m)}\\}_{m=1}^{M} \\sim p_{T}(\\boldsymbol{x}, y) \\end{aligned} $$ ","date":"2021-08-01","objectID":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/:4:2","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第10章 - 模型独立的学习方式","uri":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/"},{"categories":["Deep Learning"],"content":"10.5 终身学习 终身学习（Lifelong Learning），也叫持续学习（Continuous Learning）：像人一样持续不断学习，根据历史学习经验帮助学习新任务，并不断积累知识和经验，不会因为新任务而忘记旧知识 按：人也会忘记长久不再接触的知识、技能，往往需要一定时间才能重新掌握 终身学习 vs. 归纳迁移学习：终身学习通过前 m 个任务帮助第 m + 1 个任务的设定与归纳迁移学习类似，但终身学习关注持续学习积累。 终身学习 vs. 多任务学习：多任务学习同时学习多个任务，终身学习持续一个一个学习。 关键问题：避免灾难性遗忘（Catastrophic Forgetting），即不忘记旧任务。 灾难性遗忘解决方法：eg. 弹性权重巩固（ElasticWeight Consolidation）方法 [Kirkpatrick et al., 2017] 给定两个任务时模型参数 $\\theta$ 的后验分布为： $$ \\log p(\\theta \\mid \\mathcal{D})=\\log p(\\mathcal{D} \\mid \\theta)+\\log p(\\theta)-\\log p(\\mathcal{D}) $$ 其中 $\\mathcal{D}=\\mathcal{D}_{A} \\cup \\mathcal{D}_{B}$ ，根据独立同分布假设，上式可以写为： $$ \\begin{aligned} \\log p(\\theta \\mid \\mathcal{D}) \u0026=\\underline{\\log p\\left(\\mathcal{D}_{A} \\mid \\theta\\right)}+\\log p\\left(\\mathcal{D}_{B} \\mid \\theta\\right)+\\underline{\\log p(\\theta)}-\\log p\\left(\\mathcal{D}_{A}\\right)-\\log p\\left(\\mathcal{D}_{B}\\right) \\\\ \u0026=\\log p\\left(\\mathcal{D}_{B} \\mid \\theta\\right)+\\underline{\\log p\\left(\\theta \\mid \\mathcal{D}_{A}\\right)}-\\log p\\left(\\mathcal{D}_{B}\\right) \\end{aligned} $$ 其中 $p\\left(\\theta \\mid \\mathcal{D}_{A}\\right)$ 包含所有在任务 $\\mathcal{J}_{A}$ 上学到的信息，所以顺序学习任务 $\\mathcal{J}_{B}$ 时，参数后验分布与其在任务 $\\mathcal{J}_{A}$ 上的后验分布有关。 后验分布比较难以及建模，可以近似估计： 假设 $p\\left(\\theta \\mid \\mathcal{D}_{A}\\right)$ 为高斯分布，期望为任务 $\\mathcal{J}_{A}$ 上学习到的参数矩阵 $\\theta_{A}^{*}$ 精度矩阵（协方差矩阵的逆）用参数 $\\theta$ 在数据集 $\\mathcal{D}_{A}$ 上的 Fisher 信息矩阵来近似： $$ p\\left(\\theta \\mid \\mathcal{D}_{A}\\right)=\\mathcal{N}\\left(\\theta_{A}^{*}, F^{-1}\\right) $$ 详见 [Bishop, 2007] 中第 4 章中的拉普拉斯近似。 ","date":"2021-08-01","objectID":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/:5:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第10章 - 模型独立的学习方式","uri":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/"},{"categories":["Deep Learning"],"content":"10.6 元学习 元学习（Meta-Learning）：学习的学习（Learning to Learn），可以对不同任务动态调整学习方式 元学习 vs. 归纳迁移学习：元学习倾向于从不同（甚至是不相关）任务中归纳学习方法 元学习与小样本学习（Few-shot Learning）比较相关。 ","date":"2021-08-01","objectID":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/:6:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第10章 - 模型独立的学习方式","uri":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/"},{"categories":["Deep Learning"],"content":"10.6.1 基于优化器的元学习 不同的优化算法的区别在于更新参数的规则不同，因此一种很自然的元学习是自动学习参数更新规则，即通过另一个神经网络建模梯度下降过程。 用函数 $g_{t}(\\cdot)$ 输入梯度预测参数的更新差值 $\\Delta \\theta_{t}$，第 t 步更新规则： $$ \\theta_{t+1}=\\theta_{t}+g_{t}\\left(\\nabla \\mathcal{L}\\left(\\theta_{t}\\right) ; \\phi\\right) $$ 学习优化器可以看作元学习过程，目标是找到适用不同任务的优化器，每步迭代目标是 $\\mathcal{L}(\\theta)$ 最小： $$ \\begin{aligned} \\mathcal{L}(\\phi) \u0026=\\mathbb{E}_{f}\\left[\\sum_{t=1}^{T} w_{t} \\mathcal{L}\\left(\\theta_{t}\\right)\\right] \\\\ \\theta_{t} \u0026=\\theta_{t-1}+g_{t}, \\\\ \\left[g_{t} ; \\boldsymbol{h}_{t}\\right] \u0026=\\operatorname{LSTM}\\left(\\nabla \\mathcal{L}\\left(\\theta_{t-1}\\right), \\boldsymbol{h}_{t-1} ; \\phi\\right), \\end{aligned} $$ 因为网络参数非常多，LSTM 输入输出维度非常高，可以简化采用共享 LSTM 对每个参数进行更新。 ","date":"2021-08-01","objectID":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/:6:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第10章 - 模型独立的学习方式","uri":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/"},{"categories":["Deep Learning"],"content":"10.6.2 模型无关的元学习（MAML） 模型无关的元学习（Model-Agnostic Meta-Learning，MAML）[Finn et al., 2017] 假设所有任务来自同一任务空间，可以学习所有任务的通用表示，然后经过梯度下降在特定单任务上精调，模型 $f_{\\theta}$ 在新任务 $\\mathcal{T}_{m}$ 上学习到的任务适配参数： $$ \\theta_{m}^{\\prime}=\\theta-\\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}_{m}}\\left(f_{\\theta}\\right) $$ MAML的目标是学习一个参数𝜃 使得其经过一个梯度迭代就可以在新任务上达到最好的性能，即 $$ \\min _{\\theta} \\sum_{\\mathcal{T}_{m} \\sim p(\\mathcal{T})} \\mathcal{L}_{\\mathcal{T}_{m}}\\left(f_{\\theta_{m}^{\\prime}}\\right)=\\min _{\\theta} \\sum_{\\mathcal{T}_{m} \\sim p(\\mathcal{T})} \\mathcal{L}_{\\mathcal{T}_{m}}\\left(f(\\underbrace{\\theta-\\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}_{m}}\\left(f_{\\theta}\\right)}_{\\theta_{m}^{\\prime}})\\right) . $$ 用梯度下降在所有任务上元优化（Meta-Optimization）： $$ \\begin{aligned} \\theta \u0026 \\leftarrow \\theta-\\beta \\nabla_{\\theta} \\sum_{m=1}^{M} \\mathcal{L}_{\\mathcal{T}_{m}}\\left(f_{\\theta_{m}^{\\prime}}\\right) \\\\ \u0026=\\theta-\\beta \\sum_{m=1}^{M} \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}_{m}}\\left(f_{\\theta_{m}}\\right)\\left(I-\\alpha \\nabla_{\\theta}^{2} \\mathcal{L}_{\\mathcal{T}_{m}}\\left(f_{\\theta_{m}}\\right)\\right) \\end{aligned} $$ $\\beta$ 为元学习率，$\\alpha$ 较小时 MAML 近似为普通多任务学习优化方法。MAML 需要计算二阶梯度，可以用一阶方法近似。 ","date":"2021-08-01","objectID":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/:6:2","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第10章 - 模型独立的学习方式","uri":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/"},{"categories":["Deep Learning"],"content":"习题选做 习题 10-2 集成学习是否可以避免过拟合？ 过拟合：模型学到了很多与任务无关的 feature，在新数据上泛化能力差。 overfitting：high variance, low bias under-fitting：low variance, high bias 集成学习： Bagging（Bagging、Random Forest）通过投票找到共性，减少 variance（方差），避免过拟合 注意：有些情况，如随机森林中树过多也会导致 overfitting Boosting 增加模型复杂度，降低 bias（偏差），相对容易过拟合 所以 Boosting 相比 Bagging 更容易过拟合，一般还是需要 Cross-validation 方法来验证是否过拟合。 ","date":"2021-08-01","objectID":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/:7:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第10章 - 模型独立的学习方式","uri":"/blog/nndl-book-ch10-%E6%A8%A1%E5%9E%8B%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E5%BC%8F/"},{"categories":["Deep Learning"],"content":"ch9 无监督学习 ","date":"2021-07-29","objectID":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:0:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第9章 - 无监督学习","uri":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"categories":["Deep Learning"],"content":"9.1 无监督特征学习 无监督学习问题分类： 无监督特征学习（Unsupervised Feature Learning） 降维、可视化、监督学习前的预处理 概率密度估计（Probabilistic Density Estimation） 聚类（Clustering） K-Means、谱聚类 监督学习、无监督学习三要素： 模型 学习准则 最大似然估计（密度估计常用）、最小重构错误（无监督特征学习常用） 优化算法 ","date":"2021-07-29","objectID":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:1:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第9章 - 无监督学习","uri":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"categories":["Deep Learning"],"content":"9.1.1 主成分分析 主成分分析（Principal Component Analysis，PCA）：数据降维，使转换后的空间中数据方差最大。 样本投影方差： $$ \\begin{aligned} \\sigma(\\boldsymbol{X} ; \\boldsymbol{w}) \u0026=\\frac{1}{N} \\sum_{n=1}^{N}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}-\\boldsymbol{w}^{\\top} \\overline{\\boldsymbol{x}}\\right)^{2} \\\\ \u0026=\\frac{1}{N}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{X}-\\boldsymbol{w}^{\\top} \\overline{\\boldsymbol{X}}\\right)\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{X}-\\boldsymbol{w}^{\\top} \\overline{\\boldsymbol{X}}\\right)^{\\top} \\\\ \u0026=\\boldsymbol{w}^{\\top} \\boldsymbol{\\Sigma} \\boldsymbol{w} \\end{aligned} $$ 其中： $$ \\boldsymbol{\\Sigma}=\\frac{1}{N}(\\boldsymbol{X}-\\overline{\\boldsymbol{X}})(\\boldsymbol{X}-\\overline{\\boldsymbol{X}})^{\\top} $$ 即原样本的协方差矩阵。 拉格朗日方法转化为无约束优化： $$ \\max _{\\boldsymbol{w}} \\boldsymbol{w}^{\\top} \\boldsymbol{\\Sigma} \\boldsymbol{w}+\\lambda\\left(1-\\boldsymbol{w}^{\\top} \\boldsymbol{w}\\right) $$ 求导令导数为0： $$ \\boldsymbol{\\Sigma} \\boldsymbol{w}=\\lambda \\boldsymbol{w} $$ 𝒘 是协方差矩阵 𝚺 的特征向量，𝜆 为特征值．同时 $$ \\sigma(\\boldsymbol{X} ; \\boldsymbol{w})=\\boldsymbol{w}^{\\top} \\boldsymbol{\\Sigma} \\boldsymbol{w}=\\boldsymbol{w}^{\\top} \\lambda \\boldsymbol{w}=\\lambda $$ 因此，PCA 可以转换为矩阵特征值分解，投影向量 𝒘 为矩阵 𝚺 的最大特征值对应的特征向量。取前 $D’$ 个特征向量： $$ \\boldsymbol{\\Sigma} \\boldsymbol{W}=\\boldsymbol{W} \\operatorname{diag}(\\lambda) $$ PCA 减少了数据相关性，但不能保证投影后数据类别可分性更好。提高可分类性的方法一般为监督方法，如线性判别分析（Linear Discriminant Analysis，LDA） PCA 一个明显的缺点是失去了特征的可解释性。 ","date":"2021-07-29","objectID":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:1:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第9章 - 无监督学习","uri":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"categories":["Deep Learning"],"content":"9.1.2 稀疏编码 稀疏编码（Sparse Coding） 启发：哺乳动物视觉细胞感受野，每个神经元仅对其感受野的特定刺激做出响应，外界刺激子在视觉神经系统的表示具有稀疏性，符合生物低功耗特性。 线性编码：将输入的样本表示为一组基向量的线性组合，在 P 维空间中表示 D 维空间的样本 x： $$ \\begin{aligned} \\boldsymbol{x} \u0026=\\sum_{m=1}^{M} z_{m} \\boldsymbol{a}_{m} \\\\ \u0026=\\boldsymbol{A z}, \\end{aligned} $$ 基向量 A 也称为字典 编码的关键：找到一组完备基向量，如通过PCA。PCA得到的编码通常是稠密向量，没有稀疏性。 完备：基向量数等于其支撑维度（组成满秩方阵） 过完备：基向量数大于其支撑的维度。 为了得到稀疏编码，可以找一组“过完备”的基向量，加上稀疏性限制，得到“唯一”稀疏编码。 对一组输入 x 的稀疏编码目标函数： $$ \\mathcal{L}(\\boldsymbol{A}, \\boldsymbol{Z})=\\sum_{n=1}^{N}\\left(\\left|\\boldsymbol{x}^{(n)}-A \\boldsymbol{z}^{(n)}\\right|^{2}+\\eta \\rho\\left(\\boldsymbol{z}^{(n)}\\right)\\right), $$ 𝜌(⋅) 是一个稀疏性衡量函数，𝜂 是一个超参数，用来控制稀疏性的强度 稀疏性定义：向量非零元素的比例。大多数元素接近零的向量也成为稀疏向量。 衡量稀疏性 $\\ell_{0}$ 范数： $$ \\rho(\\boldsymbol{z})=\\sum_{m=1}^{M} \\mathbf{I}\\left(\\left|z_{m}\\right|\u003e0\\right) $$ 不满足连续可导，很难优化，所以稀疏性衡量函数常使用 $\\ell_{1}$ 范数： $$ \\rho(\\boldsymbol{z})=\\sum_{m=1}^{M}\\left|z_{m}\\right| $$ 或对数函数： $$ \\rho(\\boldsymbol{z})=\\sum_{m=1}^{M} \\log \\left(1+z_{m}^{2}\\right) $$ 或指数函数： $$ \\rho(z)=\\sum_{m=1}^{M}-\\exp \\left(-z_{m}^{2}\\right) $$ 稀疏表示的本质：用尽可能少的资源表示尽可能多的知识，人脑皮质层学习输入表征采用了这一方法，对熟练的东西会调用更少的脑区域。 训练方法 训练目标：基向量A、每个输入的表示 优化方法：交替优化 固定基向量，优化编码： $$ \\min _{z^{(n)}}\\left|\\boldsymbol{x}^{(n)}-\\boldsymbol{A} \\boldsymbol{z}^{(n)}\\right|^{2}+\\eta \\rho\\left(\\boldsymbol{z}^{(n)}\\right), \\forall n \\in[1, N] $$ 固定编码，优化基向量： $$ \\min _{\\boldsymbol{A}} \\sum_{n=1}^{N}\\left(\\left|\\boldsymbol{x}^{(n)}-\\boldsymbol{A} \\boldsymbol{z}^{(n)}\\right|^{2}\\right)+\\lambda \\frac{1}{2}|\\boldsymbol{A}|^{2} $$ 稀疏编码优点（相比稠密向量的分布式表示） 计算量小 可解释性强：编码对应少数特征 特征选择：自动选择和输入相关的少数特征，降低噪声，减少过拟合。 ","date":"2021-07-29","objectID":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:1:2","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第9章 - 无监督学习","uri":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"categories":["Deep Learning"],"content":"9.1.3 自编码器 自编码器（Auto-Encoder，AE）：通过无监督方法学习一组数据的有效编码 思路：将 x 通过编码器转换为中间变量 y，再将 y 通过解码器转换为输出 $\\bar{x}$，目标是使得输出和输入无限接近。 作用：使用其中的编码器进行特征降维，作为 ML 模型的输入。 优化目标：最小重构错误（Reconstrcution Error）： $$ \\begin{aligned} \\mathcal{L} \u0026=\\sum_{n=1}^{N}\\left|\\boldsymbol{x}^{(n)}-g\\left(f\\left(\\boldsymbol{x}^{(n)}\\right)\\right)\\right|^{2} \\\\ \u0026=\\sum_{n=1}^{N}\\left|\\boldsymbol{x}^{(n)}-f \\circ g\\left(\\boldsymbol{x}^{(n)}\\right)\\right|^{2} . \\end{aligned} $$ 特征空间维度 M 一般小于原始空间维度，AE 相当于是降维/特征抽取。 当 $M \\geq D$ 时，存在解使得 $f \\circ g$ 为单位函数，使得损失为0，解就没有太多意义。 当加上限制，如编码稀疏性、取值范围、f和g的形式等，可以得到有意义的解 如让编码只能去 K 个不同的值，则变为了 K 聚类问题。 编码器： $$ \\boldsymbol{z}=f\\left(\\boldsymbol{W}^{(1)} \\boldsymbol{x}+\\boldsymbol{b}^{(1)}\\right) $$ 解码器： $$ \\boldsymbol{x}^{\\prime}=f\\left(\\boldsymbol{W}^{(2)} \\boldsymbol{z}+\\boldsymbol{b}^{(2)}\\right) $$ 捆绑权重（Tied Weight）：令 $\\boldsymbol{W}^{(2)}=\\boldsymbol{W}^{(1)^{\\top}}$ ，参数更少，更容易学习，同时有一定正则化作用。 重构错误： $$ \\mathcal{L}=\\sum_{n=1}^{N} | \\boldsymbol{x}^{(n)}-\\boldsymbol{x}^{\\prime(n)}|^{2}+\\lambda| \\boldsymbol{W} |_{F}^{2} $$ ","date":"2021-07-29","objectID":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:1:3","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第9章 - 无监督学习","uri":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"categories":["Deep Learning"],"content":"9.1.4 稀疏自编码器 稀疏自编码器（Sparse Auto-Encoder）：让特征维度 M 大于输入维度 D，并使特征尽量稀疏的自编码器。 目标函数： $$ \\mathcal{L}=\\sum_{n=1}^{N} | \\boldsymbol{x}^{(n)}-\\boldsymbol{x}^{\\prime(n)}|^{2}+\\eta \\rho(\\boldsymbol{Z})+\\lambda| \\boldsymbol{W} |^{2} $$ 𝜌(𝒁) 为稀疏性度量函数，可以用稀疏编码的稀疏衡量函数，也可以定义为一组训练样本中每个神经元激活的概率，用平均活性值近似： $$ \\hat{\\rho}_{j}=\\frac{1}{N} \\sum_{n=1}^{N} z_{j}^{(n)} $$ 我们希望稀疏度接近实现给定的值 $\\rho^$，如0.05，用 KL 距离衡量： $$ \\mathrm{KL}\\left(\\rho^{} | \\hat{\\rho}_{j}\\right)=\\rho^{} \\log \\frac{\\rho^{}}{\\hat{\\rho}_{j}}+\\left(1-\\rho^{}\\right) \\log \\frac{1-\\rho^{}}{1-\\hat{\\rho}_{j}} $$ 稀疏性度量函数定义为： $$ \\rho(\\boldsymbol{Z})=\\sum_{j=1}^{p} \\mathrm{KL}\\left(\\rho^{*} | \\hat{\\rho}_{j}\\right) $$ ","date":"2021-07-29","objectID":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:1:4","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第9章 - 无监督学习","uri":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"categories":["Deep Learning"],"content":"9.1.5 堆叠自编码器 堆叠自编码器（Stacked Auto-Encoder，SAE）：使用逐层堆叠的方式训练深层的自编码器，可以采用逐层训练（Layer-Wise Training）来学习参数。 ","date":"2021-07-29","objectID":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:1:5","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第9章 - 无监督学习","uri":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"categories":["Deep Learning"],"content":"9.1.6 降噪自编码器 降噪自编码器（Denoising Auto-Encoder）：通过引入噪声来增加编码鲁棒性的自编码器。 ","date":"2021-07-29","objectID":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:1:6","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第9章 - 无监督学习","uri":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"categories":["Deep Learning"],"content":"9.2 概率密度估计 概率密度估计（Probabilistic Density Estimation）：简称密度估计，即基于样本估计随机变量的概率密度函数。 ","date":"2021-07-29","objectID":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:2:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第9章 - 无监督学习","uri":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"categories":["Deep Learning"],"content":"9.2.1 参数密度估计 参数密度估计（Parametric Density Estimation）：根据先验知识假设随机变量服从某种分布，然后用训练样本估计分布的参数。 对样本 D 的对数似然函数： $$ \\log p(\\mathcal{D} ; \\theta)=\\sum_{n=1}^{N} \\log p\\left(\\boldsymbol{x}^{(n)} ; \\theta\\right) $$ 可以使用最大似然估计（MLE）来寻找参数，参数估计问题转变为最优化问题： $$ \\theta^{M L}=\\underset{\\theta}{\\arg \\max } \\sum_{n=1}^{N} \\log p\\left(\\boldsymbol{x}^{(n)} ; \\theta\\right) . $$ 正态分布 多项分布 求导数为0得： $$ \\mu_{k}^{M L}=\\frac{m_{k}}{N}, \\quad 1 \\leq k \\leq K $$ 参数密度估计的问题： 模型选择：实际分布往往复杂 不可观测：一些关键变量无法观测，很难准确估计数据真实分布 维度灾难：高维数据参数估计困难，需要大量样本避免过拟合。 ","date":"2021-07-29","objectID":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:2:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第9章 - 无监督学习","uri":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"categories":["Deep Learning"],"content":"9.2.2 非参数密度估计 非参数密度估计（Nonparametric Density Estimation）：不假设数据服从某种分布，通过样本空间划分为不同的区域并估计每个区域概率来近似概率密度函数。 高维空间中随机向量 x，假设其服从未知分布 p(x)，则 x 落入小区域 R 的概率为： $$ P=\\int_{\\mathcal{R}} p(\\boldsymbol{x}) d \\boldsymbol{x} . $$ N 个样本中落入 R 的数量 K 服从二项分布： $$ P_{K}=(\\begin{array}{l} N \\\\ K \\end{array}) P^{K}(1-P)^{1-K} $$ N 很大时，可以近似认为： $$ P \\approx \\frac{K}{N} $$ 假设 R 足够小，内部概率均匀： $$ P \\approx p(\\boldsymbol{x}) V $$ 综上： $$ p(\\boldsymbol{x}) \\approx \\frac{K}{N V} $$ 非参数密度估计常用方法： 固定区域 V，统计落入不同区域的数量 直方图方法 核方法 改变区域大小，使得落入每个区域的样本数量为 K：K邻近法 直方图方法（Histogram Method） 直观可视化低维数据分布，很难扩展到高维变量（维度灾难） 核密度估计（Kernel Density Estimation） 也叫 Parzen 窗方法 定义超立方体核函数： $$ \\phi\\left(\\frac{\\boldsymbol{z}-\\boldsymbol{x}}{H}\\right)= \\begin{cases}1 \u0026 \\text { if }\\left|z_{i}-x_{i}\\right|\u003c\\frac{H}{2}, 1 \\leq i \\leq D \\\\ 0 \u0026 \\text { else }\\end{cases} $$ 求和得到落入 R 区域的样本数量： $$ K=\\sum_{n=1}^{N} \\phi\\left(\\frac{\\boldsymbol{x}^{(n)}-\\boldsymbol{x}}{H}\\right) $$ x 点概率密度估计： $$ p(\\boldsymbol{x})=\\frac{K}{N H^{D}}=\\frac{1}{N H^{D}} \\sum_{n=1}^{N} \\phi\\left(\\frac{\\boldsymbol{x}^{(n)}-\\boldsymbol{x}}{H}\\right) $$ 也可以采用更加平滑的高斯核函数： $$ \\phi\\left(\\frac{z-x}{H}\\right)=\\frac{1}{(2 \\pi)^{1 / 2} H} \\exp \\left(-\\frac{|z-x|^{2}}{2 H^{2}}\\right) $$ 则 x 点概率密度估计： $$ p(\\boldsymbol{x})=\\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{(2 \\pi)^{1 / 2} H} \\exp \\left(-\\frac{|\\boldsymbol{z}-\\boldsymbol{x}|^{2}}{2 H^{2}}\\right) $$ K 近邻方法（K-Nearest Neighbor Method） 估计 x 点密度： 找到以 x 为中心的球体，使得落入球体的样本数量为 K 利用下式计算密度： $$ p(\\boldsymbol{x}) \\approx \\frac{K}{N V} $$ ","date":"2021-07-29","objectID":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:2:2","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第9章 - 无监督学习","uri":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"categories":["Deep Learning"],"content":"9.3 总结和深入阅读 概率密度估计与后文关联： ch11：通过概率图模型介绍更一般的参数密度估计方法，包括含隐变量的参数估计方法 ch12：两种比较复杂的生成模型：玻尔兹曼机、深度信念网络 ch13：两种深度生成模型：变分自编码器、对抗生成网络 ch15：序列生成模型 生成模型：根据参数估计出的模型来生成数据。 无监督学习没有监督学习成功的原因：缺少有效的客观评价方法，无监督方法好坏需要代入下游任务中验证。 ","date":"2021-07-29","objectID":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:3:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第9章 - 无监督学习","uri":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"categories":["Deep Learning"],"content":"习题选做 习题 9-1 分析主成分分析为什么具有数据降噪能力？ PCA的核心思想是：将数据集映射到用一组特征向量（基）来表示，数据集在某个基上的投影即是特征值。噪声与主要特征一般不相关，所以较小的特征值往往对应着噪声的方差，去掉较小的特征可以减小噪声。 习题 9-3 对于一个二分类问题，试举例分析什么样的数据分布会使得主成分分析得到的特征反而会使得分类性能下降． 不满足方差越大，信息量越多的假设时，如下图： PCA 会按照 y 轴降维，使得数据两类数据混在一起而不可分（这里可以使用有监督的 LDA 降维）。 同样，当噪声过大时、数据维度本身较小时，也不适合用PCA。 习题 9-5 举例说明，K 近邻方法估计的密度函数不是严格的概率密度函数，其在整个空间上的积分不等于 1． exercise 2.61: Show that the K-nearest-neighbor density model defines an improper distribution whose integral over all space is divergent. – Bishop’s pattern recognition and machine learning 证明思路：概率密度函数在 $(-\\infty, \\infty)$ 上求积分不收敛到1，而是 $\\infty$ 假设一维条件下 $K=1$ 的 KNN 密度估计，有一个点 $x=0$，则 $x$ 处密度估计为： $$ p(x)=\\frac{K}{N V}=\\frac{1}{|x|} $$ 其中 𝑉 为区域 ℛ 的体积。 当 $N=1$ 时满足： $$ \\int_{-\\infty}^{\\infty} p(x) \\mathrm{d} x=\\infty $$ 当 $N \\gt 1$ 时，假设有一系列点： $$ X_{1} \\leq X_{2} \\leq \\ldots \\leq X_{N} $$ 对 $x\\leq X_1$ 的部分： $$ p(x)=\\frac{K}{N\\left(X_{k}-x\\right)}, \\quad x \\leq X_{1} $$ 我们只计算这部分的积分： $$ \\int_{-\\infty}^{X_{1}} \\frac{K}{N\\left(X_{k}-x\\right)} \\mathrm{d} x=\\left[\\frac{K}{N} \\ln \\left|X_{k}-x\\right|\\right]_{-\\infty}^{X_{1}}=\\infty $$ 由于密度为正，所以在 $(-\\infty, \\infty)$ 上的积分也发散，从而说明了 KNN 密度估计并不严格。 ","date":"2021-07-29","objectID":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:4:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第9章 - 无监督学习","uri":"/blog/nndl-book-ch9-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"categories":["Deep Learning"],"content":"ch7 网络优化与正则化 任何数学技巧都不能弥补信息的缺失． —— 科尼利厄斯·兰佐斯（Cornelius Lanczos） 匈牙利数学家、物理学家 神经网络应用两大问题： 优化问题 非凸损失函数难以找到全局最优解 参数多、数据大，使得二阶优化方法（牛顿法等）代价过高、一阶优化方法训练效率低 梯度消失和梯度爆炸使基于梯度的训练方法失效 泛化问题 深度方法拟合能力强，容易过拟合，采用正则化方法改进模型泛化能力 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:0:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.1 网络优化 网络优化：寻找一个神经网络模型来使得经验（或结构）风险最小化的过程。 DNN 是高度非线性模型，其风险函数为非凸函数，因此风险最小化为非凸优化问题。 神经网络结构具有多样性，很难找到通用的优化方法。 高维变量的非凸优化 低维空间的非凸优化：主要问题是存在局部最优点，难点是初始化参数和逃离局部最优点。 高维空间的非凸优化：难点是逃离鞍点（Saddle Point），即既是某些维的最高点，又是另一些维的最低点。 高维空间中大部分驻点（Stationary Point）都是鞍点 局部最小值（Local Minima）：每个维度都是最低点，概率非常低 随机梯度下降可以有效逃离鞍点 平坦最小值（Flat Minima）和尖锐最小值（Sharp Minma）：深度网络参数多且冗余，局部最小解通常是平坦最小值，鲁棒性、抗扰动能力较好。 局部最小解等价性：大神经网络中，大部分局部最小解等价，且接近全局最小解的训练损失。没有必要找全局最小值，这反而可能过拟合。 神经网络优化的改善方法 更有效优化算法：动态学习率、梯度估计修正等 更好的参数初始化、数据预处理 修改网络结构得到更好的优化地形（Optimization Landscape）：ReLU激活、残差、逐层归一化等 更好的超参数优化方法 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:1:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.2 优化算法 DNN 主要通过梯度下降法寻找最小化结构风险的参数，分为： 批量梯度下降 随机梯度下降 小批量梯度下降 优化算法分类： 调整学习率，使优化更稳定 梯度估计修正，优化训练速度 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:2:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.2.1 小批量梯度下降 小批量梯度下降法（Mini-Batch Gradient Descent）：每次迭代 K 个训练样本 第 t 次迭代偏导数： $$ \\mathfrak{g}_{t}(\\theta)=\\frac{1}{K} \\sum_{(\\boldsymbol{x}, \\boldsymbol{y}) \\in \\mathcal{S}_{t}} \\frac{\\partial \\mathcal{L}(\\boldsymbol{y}, f(\\boldsymbol{x} ; \\theta))}{\\partial \\theta} $$ 参数更新 $$ \\theta_{t} \\leftarrow \\theta_{t-1}-\\alpha g_{t} $$ 影响小批量梯度下降的因素： 批量大小 K 学习率 $\\alpha$ 梯度估计 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:2:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.2.2 批量大小选择 Batch Size 不影响梯度期望，但影响方差，越大越稳定 Batch Size 较小时候可以采用线性缩放规则（Linear Scaling Rule）：Batch Size 和学习率同比率增大 [Goyal et al., 2017] 大批量稳定，小批量收敛快 大批量越可能收敛到尖锐最小值，小批量越可能收敛到平坦最小值（泛化更好）[Keskar et al., 2016] （应与小批量随机性更大有关） ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:2:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.2.3 学习率调整 常用 lr 调整方法： 学习率衰减 学习率预热 周期性学习率调整 自适应调整学习率方法（AdaGrad、RMSprop、AdaDelta，对每个参数设置不同学习率） 学习率衰减（Learning Rate Decay）/ 学习率退火（Learning Rate Anealing） 分段常数衰减（Piecewise Constant Decay）/ 阶梯衰减（Step Decay） 逆时衰减（Inverse Time Decay） $$ \\alpha_{t}=\\alpha_{0} \\frac{1}{1+\\beta \\times t} $$ 指数衰减（Exponential Decay） $$ \\alpha_{t}=\\alpha_{0} \\beta^{t} $$ 自然指数衰减（Natural Exponential Decay） $$ \\alpha_{t}=\\alpha_{0} \\exp (-\\beta \\times t) $$ 余弦衰减（Cosine Decay） $$ \\alpha_{t}=\\frac{1}{2} \\alpha_{0}\\left(1+\\cos \\left(\\frac{t \\pi}{T}\\right)\\right) $$ 学习率预热 常用：逐渐预热（Gradual Warmup）[Goyal et al., 2017] $$ \\alpha_{t}^{\\prime}=\\frac{t}{T^{\\prime}} \\alpha_{0}, \\quad 1 \\leq t \\leq T^{\\prime} $$ 周期性学习率调整 循环学习率（Cyclic Learning Rate） 三角循环学习率（Triangular Cyclic Learning Rate） 带热重启的随机梯度下降（Stochastic Gradient Descent with Warm Restarts，SGDR）[Loshchilov et al., 2017a] 重启之后再余弦衰减 AdaGrad 算法 AdaGrad 算法（Adaptive Gradient Algorithm）[Duchi et al., 2011] ：借鉴 l2 正则化思想，每次迭代自适应调整每个参数的学习率。 每个参数梯度平方累计值： $$ G_{t}=\\sum_{\\tau=1}^{t} \\boldsymbol{g}_{\\tau} \\odot \\boldsymbol{g}_{\\tau} $$ 参数更新差值： $$ \\Delta \\theta_{t}=-\\frac{\\alpha}{\\sqrt{G_{t}+\\epsilon}} \\odot \\mathbf{g}_{t} $$ Hung-yi Lee: 用梯度平方和近似二次微分 AdaGrad 缺点：一定次数迭代后如果没有到最优点，而学习率已经非常小，难以再继续优化。 RMSprop 算法 避免 AdaGrad 学习率过早衰减到零。 梯度平方的指数衰减移动平均： $$ \\begin{aligned} G_{t} \u0026=\\beta G_{t-1}+(1-\\beta) g_{t} \\odot g_{t} \\\\ \u0026=(1-\\beta) \\sum_{\\tau=1}^{t} \\beta^{t-\\tau} g_{\\tau} \\odot g_{\\tau} \\end{aligned} $$ 参数更新差值： $$ \\Delta \\theta_{t}=-\\frac{\\alpha}{\\sqrt{G_{t}+\\epsilon}} \\odot \\mathbf{g}_{t} $$ AdaDelta 算法 AdaDelta 在 RMSprop 基础上引入参数更新差值的平方指数衰减移动平均，抑制了学习率的波动 $$ \\Delta X_{t-1}^{2}=\\beta_{1} \\Delta X_{t-2}^{2}+\\left(1-\\beta_{1}\\right) \\Delta \\theta_{t-1} \\odot \\Delta \\theta_{t-1} $$ 参数更新差值： $$ \\Delta \\theta_{t}=-\\frac{\\sqrt{\\Delta X_{t-1}^{2}+\\epsilon}}{\\sqrt{G_{t}+\\epsilon}} \\mathrm{g}_{t} $$ ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:2:3","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.2.4 梯度估计修正 梯度估计（Gradient Estimation）的修正 动量法 每次迭代计算负梯度的“加权移动平均”作为参数更新方向，增加稳定性： $$ \\Delta \\theta_{t}=\\rho \\Delta \\theta_{t-1}-\\alpha g_{t}(\\theta_{t-1})=-\\alpha \\sum_{\\tau=1}^{t} \\rho^{t-\\tau} g_{\\tau} $$ 其中 $\\rho$ 为动量因子，通常设为0.9，$\\alpha$ 为学习率 当前梯度叠加部分上次梯度，可以近似看作二阶梯度。 Nesterov 加速梯度 Nesterov 加速梯度（Nesterov Accelerated Gradient，NAG）/ Neserov 动量法（Nesterov Momentum）：对动量法的改进，在根据历史梯度更新后的位置计算梯度更新，更加合理。 $$ \\Delta \\theta_{t}=\\rho \\Delta \\theta_{t-1}-\\alpha \\mathfrak{g}_{t}\\left(\\theta_{t-1}+\\rho \\Delta \\theta_{t-1}\\right) $$ Adam 算法 Adam算法（Adaptive Moment Estimation Algorithm）[Kingma et al., 2015]：梯度平方指数加权平均（RMSprop）+ 梯度指数加权平均（Momentum） $$ \\begin{gathered} M_{t}=\\beta_{1} M_{t-1}+\\left(1-\\beta_{1}\\right) g_{t} \\\\ G_{t}=\\beta_{2} G_{t-1}+\\left(1-\\beta_{2}\\right) g_{t} \\odot g_{t} \\end{gathered} $$ 可以分别看作梯度的均值（一阶矩）和未减去均值的方差（二阶矩）。 需要进行偏差修正： $$ \\begin{aligned} \\hat{M}_{t} \u0026=\\frac{M_{t}}{1-\\beta_{1}^{t}} \\\\ \\hat{G}_{t} \u0026=\\frac{G_{t}}{1-\\beta_{2}^{t}} \\end{aligned} $$ 更新差值： $$ \\Delta \\theta_{t}=-\\frac{\\alpha}{\\sqrt{\\hat{G}_{t}+\\epsilon}} \\hat{M}_{t} $$ Nadam算法：用 Nesterov 加速梯度改进Adam 梯度截断 梯度截断（Gradient Clipping）[Pascanu et al., 2013] 按值截断：对所有参数设置范围 $$ \\boldsymbol{g}_{t}=\\max \\left(\\min \\left(\\boldsymbol{g}_{t}, b\\right), a\\right) $$ 按模截断：二范数超过阈值时整体缩放，适合 RNN。 $$ \\boldsymbol{g}_{t}=\\frac{b}{\\left|\\boldsymbol{g}_{t}\\right|}_2 \\boldsymbol{g}_{t} $$ 书中二范数符号不准确，一般范数下标应该在右下角，右上角容易误解为平方。 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:2:4","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.2.5 优化算法小结 优化算法公式概括： $$ \\begin{aligned} \\Delta \\theta_{t} \u0026=-\\frac{\\alpha_{t}}{\\sqrt{G_{t}+\\epsilon}} M_{t} \\\\ G_{t} \u0026=\\psi\\left(\\mathbf{g}_{1}, \\cdots, \\boldsymbol{g}_{t}\\right) \\\\ M_{t} \u0026=\\phi\\left(\\mathbf{g}_{1}, \\cdots, \\mathbf{g}_{t}\\right) \\end{aligned} $$ ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:2:5","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.3 参数初始化 来源：https://www.cnblogs.com/shine-lee/p/11908610.html 参数初始化非常关键，关系到网络优化效率和泛化能力。 初始化通常三种做法： 预训练初始化（Pre-trained Initialization） 不够灵活，网络架构调整不便 随机初始化（Random Initialization） 解决对称权重问题：相同初始值权重更新相同 三种随机初始化方法： 基于固定方差 基于方差缩放 正交初始化 固定值初始化：对特定参数用特殊值初始化 bias 设 0 LSTM 遗忘门初始化为 1 或 2，使时序梯度变大 使用 ReLU 的神经元，偏置可以设置为 0.01 使其初期更容易被激活 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:3:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.3.1 基于固定方差的参数初始化 高斯分布初始化 均匀分布初始化 均匀分布方差： $$ \\operatorname{var}(x)=\\frac{(b-a)^{2}}{12} $$ 区间为 $[-r, r]$ 则： $$ r=\\sqrt{3 \\sigma^{2}} $$ 关键是设置方差 $\\sigma^2$ ： 取值小：1. 神经元输出小，多层后信号消失 2. 使 Sigmoid 型函数丢失非线性激活能力 取值大：Sigmoid 型函数激活值饱和，导致梯度消失 一般配合逐层归一化使用 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:3:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.3.2 基于方差缩放的参数初始化 方差缩放（Variance Scaling）：为避免梯度爆炸或消失，应保持每个神经元输入和方差一致，根据连接数量自适应调整初始化分布的方差 Xavier 初始化 Xavier 初始化：根据每层神经元数量自动计算初始化参数方差。 l 层神经单元输出： $$ a^{(l)}=f\\left(\\sum_{i=1}^{M_{l-1}} w_{i}^{(l)} a_{i}^{(l-1)}\\right) $$ 假设激活函数为线性恒等函数，则均值： $$ \\mathbb{E}\\left[a^{(l)}\\right]=\\mathbb{E}\\left[\\sum_{i=1}^{M_{l-1}} w_{i}^{(l)} a_{i}^{(l-1)}\\right]=\\sum_{i=1}^{M_{l-1}} \\mathbb{E}\\left[w_{i}^{(l)}\\right] \\mathbb{E}\\left[a_{i}^{(l-1)}\\right]=0 $$ 方差： $$ \\begin{aligned} \\operatorname{var}\\left(a^{(l)}\\right) \u0026=\\operatorname{var}\\left(\\sum_{i=1}^{M_{l-1}} w_{i}^{(l)} a_{i}^{(l-1)}\\right) \\\\ \u0026=\\sum_{i=1}^{M_{l-1}} \\operatorname{var}\\left(w_{i}^{(l)}\\right) \\operatorname{var}\\left(a_{i}^{(l-1)}\\right) \\\\ \u0026=M_{l-1} \\operatorname{var}\\left(w_{i}^{(l)}\\right) \\operatorname{var}\\left(a_{i}^{(l-1)}\\right) . \\end{aligned} $$ 所以应设置： $$ \\operatorname{var}\\left(w_{i}^{(l)}\\right)=\\frac{1}{M_{l-1}} $$ 同理，反向传播应设置： $$ \\operatorname{var}\\left(w_{i}^{(l)}\\right)=\\frac{1}{M_{l}} $$ 折中设置： $$ \\operatorname{var}\\left(w_{i}^{(l)}\\right)=\\frac{2}{M_{l-1}+M_{l}} $$ 对 $[-r, r]$ 均匀分布初始化则： $$ r=\\sqrt{\\frac{6}{M_{l-1}+M_{l}}} $$ Xavier 初始化适用于 Logistic 和 Tanh 激活函数，因为输入往往处在激活函数线性区间。其中 Logistic 函数线性区间斜率约为 0.25，所以初始化方差为 $16 \\times \\frac{2}{M_{l-1}+M_{l}}$ He 初始化 对 ReLU 激活函数，通常一半神经元输出为 0， 因此输出方差也近似为恒等函数的一半 考虑前向传播，假设神经元输出： $$ z_i^{(l)}=\\sum_{i=1}^{M_{l-1}} w_{i}^{(l)} a_{i}^{(l-1)} $$ $$ a_{i}^{l}=\\operatorname{ReLU}(z_i^{l}) $$ 两个独立随机变量的方差： $$ \\begin{aligned} \\operatorname{Var}(X Y) \u0026=E\\left((X Y)^{2}\\right)-(E(X Y))^{2} \\\\ \u0026=E\\left(X^{2}\\right) E\\left(Y^{2}\\right)-(E(X) E(Y))^{2} \\\\ \u0026=\\left(\\operatorname{Var}(X)+(E(X))^{2}\\right)\\left(\\operatorname{Var}(Y)+(E(Y))^{2}\\right)-(E(X))^{2}(E(Y))^{2} \\\\ \u0026=\\operatorname{Var}(X) \\operatorname{Var}(Y)+(E(X))^{2} \\operatorname{Var}(Y)+\\operatorname{Var}(X)(E(Y))^{2} \\end{aligned} $$ 又： $$ \\begin{aligned} \\operatorname{var}(z) \u0026=\\int_{-\\infty}^{+\\infty}(z-0)^{2} p(z) d z \\\\ \u0026=2 \\int_{0}^{+\\infty} z^{2} p(z) d z \\\\ \u0026=2 E\\left(\\max (0, z)^{2}\\right) \\\\ \u0026=2 E\\left(a^2\\right) \\end{aligned} $$ 则 ReLU 输出方差： $$ \\begin{aligned} \\operatorname{var}\\left(z^{(l)}\\right) \u0026=\\operatorname{var}\\left(\\sum_{i=1}^{M_{l-1}} w_{i}^{(l)} a_{i}^{(l-1)}\\right) \\\\ \u0026=\\sum_{i=1}^{M_{l-1}} \\operatorname{var}\\left(w_{i}^{(l)} a_{i}^{(l-1)}\\right) \\\\ \u0026=M_{l-1} (\\operatorname{var}(w_{i}^{(l)}) \\operatorname{var}(a_{i}^{(l-1)})+E(w_{i}^{(l)})^{2} \\operatorname{var}(a_{i}^{(l-1)})+\\operatorname{var}(w_{i}^{(l)}) E(a_{i}^{(l-1)})^{2}) \\\\ \u0026=M_{l-1} (\\operatorname{var}(w_{i}^{(l)}) \\operatorname{var}(a_{i}^{(l-1)})+\\operatorname{var}(w_{i}^{(l)}) E(a_{i}^{(l-1)})^{2}) \\\\ \u0026=M_{l-1} \\operatorname{var}\\left(w_{i}^{(l)}\\right) E((a_{i}^{(l-1)})^2) \\\\ \u0026=\\frac{1}{2}M_{l-1} \\operatorname{var}\\left(w_{i}^{(l)}\\right) \\operatorname{var}(z_{i}^{(l-1)}) \\\\ \\end{aligned} $$ 所以： $$ \\operatorname{var}\\left(w_{i}^{(l)}\\right)=\\frac{2}{M_{l-1}} $$ ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:3:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.3.3 正交初始化 假设一个𝐿 层的等宽线性网络（激活函数为恒等函数）为 $$ \\boldsymbol{y}=\\boldsymbol{W}^{(L)} \\boldsymbol{W}^{(L-1)} \\ldots \\boldsymbol{W}^{(1)} \\boldsymbol{x} $$ 误差项反向传播公式： $$ \\delta^{(l-1)}=\\left(\\boldsymbol{W}^{(l)}\\right)^{\\top} \\delta^{(l)} $$ 为避免梯度消失或梯度爆炸，希望在误差项反向传播中具有范数保持性（Norm-Perserving）： $$ \\left|\\delta^{(l-1)}\\right|_{2}=\\left|\\delta^{(l)}\\right|_{2}=\\left|\\left(\\boldsymbol{W}^{(l)}\\right)^{\\top} \\delta^{(l)}\\right|_{2} $$ 二范数定义： $$ |A|_{2}=\\sqrt{\\lambda_{\\max }\\left(A^{*} A\\right)} $$ 可证明矩阵与正交矩阵相乘，二范数（谱范数）不变。 所以，可以将 $\\boldsymbol{W}^{(l)}$ 初始化为正交矩阵，即正交初始化（Orthogonal Initialization）： 用 $N(0,1)$ 初始化一个矩阵 对该矩阵奇异值分解，得到两个正交矩阵，使用其中一个作为权重矩阵 正交初始化常用于 RNN 循环边的权重矩阵。 对非线性神经网络，需要对正交矩阵乘以一个缩放系数。 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:3:3","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.4 数据预处理 尺度不变性（Scale Invariance）：机器学习算法在缩放特征后不影响学习和预测。 eg. 线性分类器尺度不变、KNN尺度敏感 对尺度敏感模型，需要对样本预处理：统一特征尺度、消除特征相关性。 理论上神经网络具有尺度不变性，通过参数调整适应尺度，但尺度不同会增加训练难度： 为防止 tanh 等函数进入饱和区而梯度消失，对每个特征尺度需要进行特定初始化 梯度下降方向不指向最优解 归一化（Normalization）：泛指同一数据特征尺度的方法。 最大最小值归一化（Min-Max Normalization） $$ \\hat{x}^{(n)}=\\frac{x^{(n)}-\\min _{n}\\left(x^{(n)}\\right)}{\\max _{n}\\left(x^{(n)}\\right)-\\min _{n}\\left(x^{(n)}\\right)} $$ 标准化（Standardizatin）/ Z值归一化（Z-Score Normalization） $$ \\hat{x}^{(n)}=\\frac{x^{(n)}-\\mu}{\\sigma} $$ 标准差为0则说明该维特征没有区分性，可直接删掉。 白化（Whitening）：降低输入数据之间的冗余性，并使所有特征具有相同方差。 主要实现方式：主成分分析（Principal Component Analysis，PCA）去掉各成分相关性。 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:4:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.5 逐层归一化 逐层归一化（Layer-wise Normalization）提高效率的原因： 更好的尺度不变性 内部协变量偏移（Internal Covariate Shift）：神经层输入分布改变后参数需要重新学习 更平滑的优化地形： 大部分神经元处于不饱和区，避免梯度消失； 优化地形（Optimization Landscape）更加平滑 常用的逐层归一化方法： 批量归一化 层归一化 权重归一化 局部相应归一化 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:5:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.5.1 批量归一化 批量归一化（Batch Normalization，BN）：在仿射变换之后、激活函数之前，将输入$\\boldsymbol{z}^{(l)}$ 每一维都归一到标准正态分布： $$ \\hat{z}^{(l)}=\\frac{z^{(l)}-\\mathbb{E}\\left[z^{(l)}\\right]}{\\sqrt{\\operatorname{var}\\left(z^{(l)}\\right)+\\epsilon}} $$ 这里的期望和方差一般使用小批量样本集的均值和方差进行估计。 归一到 0 附近在使用 Sigmoid 函数时，取值在线性区间，削弱了神经网络的非线性性质，可以通过缩放平移来改变取值区间： $$ \\begin{aligned} \\hat{\\boldsymbol{z}}^{(l)} \u0026=\\frac{\\boldsymbol{z}^{(l)}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2}+\\epsilon}} \\odot \\boldsymbol{\\gamma}+\\boldsymbol{\\beta} \\\\ \u0026 \\triangleq \\mathrm{B} \\mathrm{N}_{\\gamma, \\boldsymbol{\\beta}}\\left(\\boldsymbol{z}^{(l)}\\right) \\end{aligned} $$ 其中： $$ \\begin{aligned} \\mu_{\\mathcal{B}} \u0026=\\frac{1}{K} \\sum_{k=1}^{K} z^{(k, l)}, \\\\ \\sigma_{\\mathcal{B}}^{2} \u0026=\\frac{1}{K} \\sum_{k=1}^{K}\\left(\\boldsymbol{z}^{(k, l)}-\\mu_{\\mathcal{B}}\\right) \\odot\\left(\\boldsymbol{z}^{(k, l)}-\\mu_{\\mathcal{B}}\\right) \\end{aligned} $$ 批量归一化操作可以看作一个特殊的神经层，加在每一层非线性激活函数之前： $$ \\boldsymbol{a}^{(l)}=f\\left(\\mathrm{BN}_{\\gamma, \\beta}\\left(\\boldsymbol{z}^{(l)}\\right)\\right)=f\\left(\\mathrm{BN}_{\\gamma, \\beta}\\left(\\boldsymbol{W} \\boldsymbol{a}^{(l-1)}\\right)\\right) $$ 批量归一化不但提高优化效率，还是一种隐形的正则化方法：对样本的预测与批次中其他样本有关，不会过拟合某个特定样本。 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:5:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.5.2 层归一化 RNN 等的神经元输入分布是动态变化的，无法应用 BN 层归一化（Layer Normalization）：对中间一层的所有神经元进行归一化 $$ \\begin{aligned} \\hat{z}^{(l)} \u0026=\\frac{z^{(l)}-\\mu^{(l)}}{\\sqrt{\\sigma^{(l)^{2}+\\epsilon}} \\odot \\gamma+\\beta} \\\\ \u0026 \\triangleq \\mathrm{LN}_{\\gamma, \\beta}\\left(z^{(l)}\\right) \\end{aligned} $$ 其中： $$ \\begin{aligned} \\mu_{\\mathcal{B}} \u0026=\\frac{1}{K} \\sum_{k=1}^{K} z^{(k, l)}, \\\\ \\sigma_{\\mathcal{B}}^{2} \u0026=\\frac{1}{K} \\sum_{k=1}^{K}\\left(\\boldsymbol{z}^{(k, l)}-\\mu_{\\mathcal{B}}\\right) \\odot\\left(\\boldsymbol{z}^{(k, l)}-\\mu_{\\mathcal{B}}\\right) \\end{aligned} $$ RNN 的层归一化： $$ \\begin{aligned} \u0026\\boldsymbol{z}_{t}=\\boldsymbol{U} \\boldsymbol{h}_{t-1}+\\boldsymbol{W} \\boldsymbol{x}_{t} \\\\ \u0026\\boldsymbol{h}_{t}=f\\left(\\mathrm{LN}_{\\gamma, \\beta}\\left(\\boldsymbol{z}_{t}\\right)\\right) \\end{aligned} $$ 层归一化和批量归一化区别： 对于 𝐾 个样本的一个小批量集合 $\\boldsymbol{Z}^{(l)}=\\left[\\boldsymbol{z}^{(1, l)} ; \\cdots ; \\boldsymbol{z}^{(K, l)}\\right]$ 层归一化：对 $\\boldsymbol{Z}^{(l)}$ 每一列进行归一化 批量归一化：对 $\\boldsymbol{Z}^{(l)}$ 每一行进行归一化 一般批量归一化更好，当 Batch Size 比较小时，可以选择层归一化。 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:5:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.5.3 权重归一化 权重归一化（Weight Normalization）：通过再参数化（Reparameterization），将连接权重分解为长度和方向两种参数： $$ \\boldsymbol{a}^{(l)}=f\\left(\\boldsymbol{W} \\boldsymbol{a}^{(l-1)}+\\boldsymbol{b}\\right) $$ 则再参数化 $\\boldsymbol{W}$： $$ \\boldsymbol{W}_{i,:}=\\frac{g_{i}}{\\left|\\boldsymbol{v}_{i}\\right|} \\boldsymbol{v}_{i}, \\quad 1 \\leq i \\leq M_{l} $$ ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:5:3","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.5.4 局部响应归一化 局部响应归一化（Local Response Normalization，LRN），常用于卷积层， $$ \\begin{aligned} \\hat{\\boldsymbol{Y}}^{p} \u0026=\\boldsymbol{Y}^{p} /\\left(k+\\alpha \\sum_{j=\\max \\left(1, p-\\frac{n}{2}\\right)}^{\\min \\left(P, p+\\frac{n}{2}\\right)}\\left(\\boldsymbol{Y}^{j}\\right)^{2}\\right)^{\\beta} \\\\ \u0026 \\triangleq \\mathrm{LRN}_{n, k, \\alpha, \\beta}\\left(\\boldsymbol{Y}^{p}\\right) \\end{aligned} $$ 类似：生物神经元中的侧抑制（lateral inhibition），活跃神经元对相邻神经元有抑制作用。最大汇聚（Max Pooling）也具有侧抑制作用，区别（抑制维度不同）： 最大汇聚：对同一特征映射中邻近神经元抑制 局部响应归一化：对同一位置邻近特征映射的神经元抑制 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:5:4","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.6 超参数优化 常见超参数： 网络结构，包括神经元之间的连接关系、层数、每层的神经元数量、激 活函数的类型等． 优化参数，包括优化方法、学习率、小批量的样本数量等． 正则化系数 超参数优化（Hyperparameter Optimization）困难： 是组合优化问题，没有通用有效办法 评估一组超参数配置（Configuration）时间代价高，一些优化方法难以应用（如演化算法（Evolution Algorithm）） 主要优化方法： ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:6:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.6.1 网格搜索 网格搜索（Grid Search）：遍历所有超参数组合，在val集上选择性能最好的配置。 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:6:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.6.2 随机搜索 随机搜索（Random Search）：有些参数影响较小，用网格搜成本高，所以对超参数进行随机组合，然后选最好的配置。 网格搜索和随机搜索都没有考虑不同参数组合之间的相关性，所以提出自适应的超参数优化方法： 贝叶斯优化 动态资源分配 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:6:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.6.3 贝叶斯优化 贝叶斯优化（Bayesian optimization）：根据已实验的超参组合，预测下一个可能最大收益的组合。 常用：时序模型优化（Sequential Model-Based Optimization，SMBO） ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:6:3","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.6.4 动态资源分配 较早估计出一组配置的效果会比较差，就可以中止这组配置的评估，关键是将有限资源分配给更有可能带来收益的组合。 如：早期停止（Early-Stopping）终止对不收敛或收敛较差的配置的训练。 最优臂问题（Best-Arm Problem）：即在给定有限的机会次数下，如何玩这些赌博机并找到收益最大的臂． ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:6:4","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.6.5 神经架构调整 可以认为，深度学习使机器学习中的“特征工程”问题转变为“网络架构工程”问题。 神经架构搜索（Neural Architecture Search，NAS）[Zoph et al., 2017]：利用元学习的思想，神经架构搜索利用一个控制器来生成另一个子网络的架构描述，控制器可以用 RL 训练。 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:6:5","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.7 网络正则化 正则化（Regularization）是一类通过限制模型复杂度，从而避免过拟合，提高泛化能力的方法，比如引入约束、增加先验、提前停止等 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:7:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.7.1 l1 和 l2 正则化 $$ \\theta^{*}=\\underset{\\theta}{\\arg \\min } \\frac{1}{N} \\sum_{n=1}^{N} \\mathcal{L}\\left(y^{(n)}, f\\left(\\boldsymbol{x}^{(n)} ; \\theta\\right)\\right)+\\lambda \\ell_{p}(\\theta) $$ 则优化问题： $$ \\begin{aligned} \\theta^{*}=\u0026 \\underset{\\theta}{\\arg \\min } \\frac{1}{N} \\sum_{n=1}^{N} \\mathcal{L}\\left(y^{(n)}, f\\left(\\boldsymbol{x}^{(n)} ; \\theta\\right)\\right), \\\\ \\text { s.t. } \\quad \\ell_{p}(\\theta) \\leq 1 \\end{aligned} $$ 弹性网络正则化（Elastic Net Regularization）：同时加入 l1 和 l2 优化。 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:7:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.7.2 权重衰减 权重衰减（Weight Decay）： $$ \\theta_{t} \\leftarrow(1-\\beta) \\theta_{t-1}-\\alpha \\mathrm{g}_{t} $$ 在 SGD 中，权重衰减与 l2 效果相同，在 Adam 等较复杂优化中，则不等价。 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:7:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.7.3 提前停止 提前停止（Early Stop）：验证集错误不再下降则停止。 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:7:3","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.7.4 丢弃法 Dropout 丢弃法（Dropout Method）[Srivastava et al., 2014]：训练时随机丢弃一部分神经元。 $$ \\operatorname{mask}(\\boldsymbol{x})= \\begin{cases}\\boldsymbol{m} \\odot \\boldsymbol{x} \u0026 \\text { 当训练阶段时 } \\\\ p \\boldsymbol{x} \u0026 \\text { 当测试阶段时 }\\end{cases} $$ 对于隐藏层神经单元，保留率 p 取 0.5 效果最好，随机生成的网络结构最具多样性 对于输入层神经单元，通常保留率 p 更接近 1，使输入变化不会太大 集成学习角度的解释：假设共 n 个神经元，则 dropout 出了 $2^n$ 个子网络，每次迭代相当于训练不同的子网络，最终结果可以看作指数个模型集成。 贝叶斯学习角度的解释：dropout 可以看作一种贝叶斯学习的近似，即对要学习的网络多次采用后平均的结果： $$ \\begin{aligned} \\mathbb{E}_{q(\\theta)}[y] \u0026=\\int_{q} f(\\boldsymbol{x} ; \\theta) q(\\theta) d \\theta \\\\ \u0026 \\approx \\frac{1}{M} \\sum_{m=1}^{M} f\\left(\\boldsymbol{x}, \\theta_{m}\\right) \\end{aligned} $$ RNN 上的 Dropout 为避免损害时间维度上的记忆能力，不能对每个时刻的隐状态进行随机丢弃： Naive Dropout：可以对非时间维度的连接进行 Dropout： 变分丢弃法（Variational Dropout）：根据贝叶斯学习，每次 dropout 采样的参数在各个时间应该不变，所有时刻应该使用相同的掩码： ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:7:4","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.7.5 数据增强 数据增强（Data Augmentation）：目前主要在图像上使用 旋转（Rotation） 翻转（Flip） 缩放（Zoom In/Out） 平移（Shift） 加噪声（Noise） ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:7:5","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"7.7.6 标签平滑 laebl smoothing：在输出标签中添加随机噪声来避免过拟合。 One-hot 的标签是硬目标（Hard Target）： $$ \\boldsymbol{y}=[0, \\cdots, 0,1,0, \\cdots, 0]^{\\top} . $$ Motivation： 在 softmax 中，使某类概率趋向于 1 需要很大的归一化得分，可能导致其权重越来越大，并导致过拟合 标签错误时，会导致更加严重的过拟合 平滑后为软目标（Soft Target）： $$ \\tilde{\\boldsymbol{y}}=\\left[\\frac{\\epsilon}{K-1}, \\cdots, \\frac{\\epsilon}{K-1}, 1-\\epsilon, \\frac{\\epsilon}{K-1}, \\cdots, \\frac{\\epsilon}{K-1}\\right]^{\\top} $$ 这种标签平滑没有考虑标签之间的相关性，更好的办法是按照类别相关性赋予其他标签不同概率，如教师网络（Teacher Network）的输出作为软目标训练学生网络（Student Network），即知识蒸馏（Knowledge Distillation） ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:7:6","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["Deep Learning"],"content":"习题选做 习题 7-1 在小批量梯度下降中，试分析为什么学习率要和批量大小成正比 理论上，Batch Size 增大 k 倍，LR 应该增大 $\\sqrt{k}$ 使梯度保持不变，但实践发现 k 倍效果更好。 习题 7-5 证明公式(7.45)． He 初始化证明我在上文已给出。 习题 7-8 分析为什么批量归一化不能直接应用于循环神经网络 BN 不适用于 RNN 这种动态结构，对 Batch 中每个 postion 作标准化，需要估计每个 position 的 $\\mu$ 和 $\\sigma$： 样本长度不同，测试集中过长时间片的参数难以估计 Normalize 的对象来自不同的分布，多个 sequence 的同一个 position 很难服从相同分布 习题 7-10 试分析为什么不能在循环神经网络中的循环连接上直接应用丢弃法？ 对隐状态随机丢弃，会损失记忆的信息，可以只对非时间维度的参数进行丢弃或对时间维度丢弃相同的参数。 ","date":"2021-07-20","objectID":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/:8:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第7章 - 网络优化与正则化","uri":"/blog/nndl-book-ch7-%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"},{"categories":["NLP"],"content":"[Devlin et al., NAACL 2019] BERT: Bidirectional Encoder representations from Transformers ","date":"2021-03-24","objectID":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/:0:0","tags":["NLP","BERT","Pre-training","notes\""],"title":"【NLP Papers】BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","uri":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/"},{"categories":["NLP"],"content":"1 Introduction two pre-train strategies: feature-based ELMo: task-specific architecture fine-tuning GPT limitations: standard language models are unidirectional masked language model (MLM, inspired by Cloze task) use a “next sentence prediction” task that jointly pretain text-pair representations ","date":"2021-03-24","objectID":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/:1:0","tags":["NLP","BERT","Pre-training","notes\""],"title":"【NLP Papers】BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","uri":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/"},{"categories":["NLP"],"content":"2 Related Work ","date":"2021-03-24","objectID":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/:2:0","tags":["NLP","BERT","Pre-training","notes\""],"title":"【NLP Papers】BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","uri":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/"},{"categories":["NLP"],"content":"2.1 Unsupervised Feature-based Approaches from word2vec to ELMo… ","date":"2021-03-24","objectID":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/:2:1","tags":["NLP","BERT","Pre-training","notes\""],"title":"【NLP Papers】BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","uri":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/"},{"categories":["NLP"],"content":"2.2 Unsupervised Fine-tuning Approaches GPT use left-to-right language modeling and auto-encoder objectives ","date":"2021-03-24","objectID":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/:2:2","tags":["NLP","BERT","Pre-training","notes\""],"title":"【NLP Papers】BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","uri":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/"},{"categories":["NLP"],"content":"2.3 Transfer Learning from Supervised Data ","date":"2021-03-24","objectID":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/:2:3","tags":["NLP","BERT","Pre-training","notes\""],"title":"【NLP Papers】BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","uri":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/"},{"categories":["NLP"],"content":"3 BERT two steps: pretraining fine-tuning WordPiece embeddings ","date":"2021-03-24","objectID":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/:3:0","tags":["NLP","BERT","Pre-training","notes\""],"title":"【NLP Papers】BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","uri":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/"},{"categories":["NLP"],"content":"3.1 Pre-training BERT Task #1: Masked LM mask 15% of all WordPiece tokens in each sequence at random. mismatch if [MASK] between pre-training and fine-tuning 80%: [MASK] 10%: random token 10%: unchanged Task #2: Next Sentence Prediction(NSP) purpose: many tasks such as QA and NLI are based on two sentences. pre-train for a binarized next sentence prediction 50% IsNext 50% NotNext final model achieves 97%-98% accuracy on NSP BERT transfers all parameters to initialize end-task model parameters ","date":"2021-03-24","objectID":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/:3:1","tags":["NLP","BERT","Pre-training","notes\""],"title":"【NLP Papers】BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","uri":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/"},{"categories":["NLP"],"content":"3.2 Fine-tuning BERT input and output for different tasks ","date":"2021-03-24","objectID":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/:3:2","tags":["NLP","BERT","Pre-training","notes\""],"title":"【NLP Papers】BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","uri":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/"},{"categories":["NLP"],"content":"4 Experiments GLUE, SQuAD v1.1, SQuAD v2.0, SWAG ","date":"2021-03-24","objectID":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/:4:0","tags":["NLP","BERT","Pre-training","notes\""],"title":"【NLP Papers】BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","uri":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/"},{"categories":["NLP"],"content":"5 Ablation Studies Effect of Pre-training Tasks Effect of Model Size Feature-based Approach with BERT ","date":"2021-03-24","objectID":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/:5:0","tags":["NLP","BERT","Pre-training","notes\""],"title":"【NLP Papers】BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","uri":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/"},{"categories":["NLP"],"content":"6 Conclusion ","date":"2021-03-24","objectID":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/:6:0","tags":["NLP","BERT","Pre-training","notes\""],"title":"【NLP Papers】BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","uri":"/blog/nlp-papersbert_-pre-training-of-deep-bidirection/"},{"categories":["NLP"],"content":"Word Representations 综述 [Noah A. Smith, 2020] ","date":"2021-03-24","objectID":"/blog/nlp-paperscontextual-word-representations_-a-con/:0:0","tags":["NLP","Pre-training","notes\""],"title":"【NLP Papers】Contextual Word Representations: A Contextual Introduction","uri":"/blog/nlp-paperscontextual-word-representations_-a-con/"},{"categories":["NLP"],"content":"1 Preliminaries 两种word定义： word token：word observed in a piece of text word type: distinct word, rather than a specific instance 每个word type可能有多个word token实例。 ","date":"2021-03-24","objectID":"/blog/nlp-paperscontextual-word-representations_-a-con/:1:0","tags":["NLP","Pre-training","notes\""],"title":"【NLP Papers】Contextual Word Representations: A Contextual Introduction","uri":"/blog/nlp-paperscontextual-word-representations_-a-con/"},{"categories":["NLP"],"content":"2 Discrete Words simplest representation of text: sequence of characters integerized: give each word type a unique integer value \u003e=0, advantages: every word type was stored in the same amount of memory array-based data structures could be used to index other information by word types we refer to integer-based representations of word types as discrete representations ","date":"2021-03-24","objectID":"/blog/nlp-paperscontextual-word-representations_-a-con/:2:0","tags":["NLP","Pre-training","notes\""],"title":"【NLP Papers】Contextual Word Representations: A Contextual Introduction","uri":"/blog/nlp-paperscontextual-word-representations_-a-con/"},{"categories":["NLP"],"content":"3 Words as Vectors 非discrete起源，从实际应用出发： 文本分类 机器翻译：用 word token 作为翻译凭据 在上下文中给定evidence，选择 word type 输出 ？ discrete representations 无法在词之间共享信息，词之间无法比较相似性。 many strands: WordNet(Fellbaum, 1998)： a lexical database that stores words and relationships among them such as synonymy and hyponymy part of speech use program to draw informatin from corpora use these strands to derive a notion of a word type as a vector, dimensions are features: one hot use a dimension to mark a known class (e.g. days of the week) use a dimension to place variants of word types in a class. use surface attributes to “tie together” word type that look similar e.g. capitalization patterns, lenths, and the presence of a digit allocate dimensions to try to capture word types’ meanings e.g. in “typical weight” elephant get 12,000 and cat get 9. feartures from: experts derived using automated algorithms ","date":"2021-03-24","objectID":"/blog/nlp-paperscontextual-word-representations_-a-con/:3:0","tags":["NLP","Pre-training","notes\""],"title":"【NLP Papers】Contextual Word Representations: A Contextual Introduction","uri":"/blog/nlp-paperscontextual-word-representations_-a-con/"},{"categories":["NLP"],"content":"4. Words as Distributional Vectors: Context as Meaning idea: words used in similar ways are likely to have related meanings. distributional view of word meaning: looking at the full distribution of contexts in corpus where $w$ is found. approchs: hierarchical clustering, Brown et al. (1992) (highly successful) word vectors with each dimension corresponded to the frequency the word type occurred in some context (Deerwester et al., 1990), dimensionality reduction is applied. vector space semantics (see Turney and Pantel, 2010 for a survey): v(man) - v(woman) = v(king) - v(queen) cons of reduced-dimensionality vectors: features are not interpretable the word’s meaning is distributed across the whole vector, that is distributed representations scalability problems word2vec a common pattern: construct word vectors and publish them for everyone to use. interesting ideas: Finutuning rather than “learning from scratch” use expert-build data structures. e.g. retrofitting with WordNet use bilingual dictionaries to “align” the vectors use character sequence to build vectors ","date":"2021-03-24","objectID":"/blog/nlp-paperscontextual-word-representations_-a-con/:4:0","tags":["NLP","Pre-training","notes\""],"title":"【NLP Papers】Contextual Word Representations: A Contextual Introduction","uri":"/blog/nlp-paperscontextual-word-representations_-a-con/"},{"categories":["NLP"],"content":"5. Contextual Word Vectors idea: words have different meaning in different context（一词多义） from “word type vectors” to “word token vectors” similar meaning words are easy to find for word token in context ELMo: embeddings from language models (Peters et al., 2018a) use NN to contextualize word type vector to word token vector optimization task: language modeling (next word prediction) ULMFiT: (Howard and Ruder, 2018) benefit for text classification BERT: (Devlin et al., 2019) innovations to the learning method and learned from more data GPT-2 Radford et al. (2019) RoBERTa Liu et al. (2019b) T5 Raffel et al. (2019) XLM Lample and Conneau (2019) XLNet Yang et al. (2019) ","date":"2021-03-24","objectID":"/blog/nlp-paperscontextual-word-representations_-a-con/:5:0","tags":["NLP","Pre-training","notes\""],"title":"【NLP Papers】Contextual Word Representations: A Contextual Introduction","uri":"/blog/nlp-paperscontextual-word-representations_-a-con/"},{"categories":["NLP"],"content":"6. Cautionary Notes Word vectors are biased ME: Isn’t bias sometimes knowledge? Language is a lot more than words NLP is not a single problem evaluation is important ","date":"2021-03-24","objectID":"/blog/nlp-paperscontextual-word-representations_-a-con/:6:0","tags":["NLP","Pre-training","notes\""],"title":"【NLP Papers】Contextual Word Representations: A Contextual Introduction","uri":"/blog/nlp-paperscontextual-word-representations_-a-con/"},{"categories":["NLP"],"content":"7. What’s Next variations on contextual word vectors to new problems modifications to the learning methods improving preformance in setting where little supervision is available computatoinally less expensive characterize the generalizations that these meth- ods are learning in linguistic terms ","date":"2021-03-24","objectID":"/blog/nlp-paperscontextual-word-representations_-a-con/:7:0","tags":["NLP","Pre-training","notes\""],"title":"【NLP Papers】Contextual Word Representations: A Contextual Introduction","uri":"/blog/nlp-paperscontextual-word-representations_-a-con/"},{"categories":["NLP"],"content":"8. Further Reading linguistics: Emily M. Bender. Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax. Morgan \u0026 Claypool, 2013 Emily M. Bender and Alex Lascarides. Linguistic Fundamentals for Natural Language Processing II: 100 Essentials from Semantics and Pragmatics. Morgan \u0026 Claypool, 2019. (Sections 1–4 chapter 14 of) Jacob Eisenstein. Introduction to Natural Language Processing. MIT Press, 2019. contextual word vectors original papers: EMLo: Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of NAACL, 2018a. BERT: Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. of NAACL, 2019. ","date":"2021-03-24","objectID":"/blog/nlp-paperscontextual-word-representations_-a-con/:8:0","tags":["NLP","Pre-training","notes\""],"title":"【NLP Papers】Contextual Word Representations: A Contextual Introduction","uri":"/blog/nlp-paperscontextual-word-representations_-a-con/"},{"categories":["NLP"],"content":"其他 文本表示方法： bag-of-words：one-hot，tf-idf，textrank 主题模型：LSA(SVD)，pLSA，LDA 静态词向量：word2vec，fastText，GloVe 动态词向量：ELMo，GPT，BERT ","date":"2021-03-24","objectID":"/blog/nlp-paperscontextual-word-representations_-a-con/:9:0","tags":["NLP","Pre-training","notes\""],"title":"【NLP Papers】Contextual Word Representations: A Contextual Introduction","uri":"/blog/nlp-paperscontextual-word-representations_-a-con/"},{"categories":["NLP"],"content":"[Peters et al., NAACL 2018a] use bidirectional language model to train contextual word vector. use these vector as pre-train part of existing models, improve SOTA across six tasks. analysis showing that exposing the deep internals of the pre-trained network is crucial ","date":"2021-03-24","objectID":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/:0:0","tags":["NLP","ELMo","Pre-training","notes\""],"title":"【NLP Papers】ELMo: Deep contextualized word representations","uri":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/"},{"categories":["NLP"],"content":"1 Introduction pre-trained word representations should model both: complex characteristic of word use (e.g., syntax and semantics) how these uses vary across linguistic contexts (i.e., to model polysemy) ELMo: Embeddings from Language Models capture: higher-level LSTM states: context-dependent meaning lower-level LSTM states: syntax e.g., can be used for POS tagging ","date":"2021-03-24","objectID":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/:1:0","tags":["NLP","ELMo","Pre-training","notes\""],"title":"【NLP Papers】ELMo: Deep contextualized word representations","uri":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/"},{"categories":["NLP"],"content":"2 Related work “word type” embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) +subword (Wieting et al., 2016; Bojanowski et al., 2017) “word sense” embeddings (Neelakantan et al., 2014) context-dependent representations context2vec (Melamud et al., 2016) CoVe (McCann et al., 2017) (Peters et al., 2017) different layers of biRNNs encode different information (Hashimoto et al., 2017) (Søgaard and Goldberg, 2016) (Belinkov et al. 2017) (Melamud et al.,2016) pretrain (Dai and Le, 2015) (Ramachandran et al. 2017) ","date":"2021-03-24","objectID":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/:2:0","tags":["NLP","ELMo","Pre-training","notes\""],"title":"【NLP Papers】ELMo: Deep contextualized word representations","uri":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/"},{"categories":["NLP"],"content":"3 ELMo: Embeddings from Language Models ELMo word representations are functions of the entire input sentence, computed on top of two-layer biLMs with character convolutions, as a linear function of the internal network states. semi-supervised: biLM is pretrained as a large scale easily incorporated into a wide range of existing neural NLP architectures. ","date":"2021-03-24","objectID":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/:3:0","tags":["NLP","ELMo","Pre-training","notes\""],"title":"【NLP Papers】ELMo: Deep contextualized word representations","uri":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/"},{"categories":["NLP"],"content":"3.1 Bidirectional language models $$ \\begin{array}{l} \\sum_{k=1}^{N}(\\log p(t_{k} \\mid t_{1}, \\ldots, t_{k-1} ; \\Theta_{x}, \\vec{\\Theta}_{L S T M}, \\Theta_{s}). \\\\ .\\quad+\\log p(t_{k} \\mid t_{k+1}, \\ldots, t_{N} ; \\Theta_{x}, \\overleftarrow{\\Theta}_{L S T M}, \\Theta_{s})) \\end{array} $$ share some weights between directions ","date":"2021-03-24","objectID":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/:3:1","tags":["NLP","ELMo","Pre-training","notes\""],"title":"【NLP Papers】ELMo: Deep contextualized word representations","uri":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/"},{"categories":["NLP"],"content":"3.2 ELMo a L-layer biLM computes a set of $2L+1$ representations: $$ \\begin{aligned} R_{k} \u0026={\\mathbf{x}_{k}^{L M}, \\overrightarrow{\\mathbf{h}}_{k, j}^{L M}, \\overleftarrow{\\mathbf{h}}_{k, j}^{L M} \\mid j=1, \\ldots, L} \\\\ \u0026={\\mathbf{h}_{k, j}^{L M} \\mid j=0, \\ldots, L} \\end{aligned} $$ collapses all layers in $R$ int a single vector: $$ \\mathbf{E L M o}_{k}=E(R_{k} ; \\mathbf{\\Theta}_{e}) $$ simplest way (as in TagLM (Peters et al., 2017) and CoVe (McCann et al., 2017).): $$ E(R_{k})=\\mathbf{h}_{k, L}^{L M} $$ task specific weighting: $$ \\mathbf{E L M o}_{k}^{t a s k}=E(R_{k} ; \\Theta^{t a s k})=\\gamma^{t a s k} \\sum_{j=0}^{L} s_{j}^{t a s k} \\mathbf{h}_{k, j}^{L M} $$ $\\mathbf{s}^{\\text {task }}$ are softmax-normalized weights and scalar parameter $\\gamma^{\\text {task }}$ allows the task model to scale the entire ELMo vector. ","date":"2021-03-24","objectID":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/:3:2","tags":["NLP","ELMo","Pre-training","notes\""],"title":"【NLP Papers】ELMo: Deep contextualized word representations","uri":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/"},{"categories":["NLP"],"content":"3.3 Using biLMs for supervised NLP tasks run biLM to get layer representations for each word let the end task learn a linear combination of these representations. freeze weights of biLM and pass the ELMo enhanced representation $[\\mathbf{x}_{k} ; \\mathbf{E L M o}_{k}^{\\text {task }}]$ into the task RNN. observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing $\\mathbf{h}_k$ with $[\\mathbf{h}_{k} ; \\mathbf{E L M o}_{k}^{\\text {task }}]$ prevent overfit: moderate amount of dropout adding $\\lambda|\\mathbf{w}|_{2}^{2}$ to the loss ","date":"2021-03-24","objectID":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/:3:3","tags":["NLP","ELMo","Pre-training","notes\""],"title":"【NLP Papers】ELMo: Deep contextualized word representations","uri":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/"},{"categories":["NLP"],"content":"3.4 Pre-trained bidirectional language model architecture similar to architectures in J´ozefowicz et al. (2016) and Kim et al. (2015) modified to support joint training of both directions residual connection halved all embedding and hidden dimensions from the single best model CNN-BIG-LSTM in J´ozefowicz et al (2016). L = 2 biLSTM layers with 4096 units and 512 dimension projections residual connections context insensitive type representation: 2048 character n-gram convolutional filters followed by two highway layers (Srivastava et al., 2015) linear projection down to a 512 representation ","date":"2021-03-24","objectID":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/:3:4","tags":["NLP","ELMo","Pre-training","notes\""],"title":"【NLP Papers】ELMo: Deep contextualized word representations","uri":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/"},{"categories":["NLP"],"content":"4 Evaluation Question answering Textual entailment Semantic role labeling Coreference resolution Named entity extraction Sentiment analysis ","date":"2021-03-24","objectID":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/:4:0","tags":["NLP","ELMo","Pre-training","notes\""],"title":"【NLP Papers】ELMo: Deep contextualized word representations","uri":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/"},{"categories":["NLP"],"content":"5 Analysis deep contextual representations works better than just top layer syntactic information at lower layers while semantic information as higher layers ","date":"2021-03-24","objectID":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/:5:0","tags":["NLP","ELMo","Pre-training","notes\""],"title":"【NLP Papers】ELMo: Deep contextualized word representations","uri":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/"},{"categories":["NLP"],"content":"5.1 Alternate layer weighting schemes previous word on contextual representations: only used last layer, whether it be from biLM or MT encoder ablation study: ","date":"2021-03-24","objectID":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/:5:1","tags":["NLP","ELMo","Pre-training","notes\""],"title":"【NLP Papers】ELMo: Deep contextualized word representations","uri":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/"},{"categories":["NLP"],"content":"5.2 Where to include ELMo? word emebedings only as input to the lowest layers however, including ELMo at the output of biRNN improves for some tasks ","date":"2021-03-24","objectID":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/:5:2","tags":["NLP","ELMo","Pre-training","notes\""],"title":"【NLP Papers】ELMo: Deep contextualized word representations","uri":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/"},{"categories":["NLP"],"content":"5.3 What information is captured by the biLM’s representations? Word sense disambiguation POS tagging context representations as input to a linear classifier of POS Implications for supervised tasks including all biLM layers is important for downstream tasks ","date":"2021-03-24","objectID":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/:5:3","tags":["NLP","ELMo","Pre-training","notes\""],"title":"【NLP Papers】ELMo: Deep contextualized word representations","uri":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/"},{"categories":["NLP"],"content":"5.4 Sample efficiency using ELMo increases the sample efficiency. ELMo-enhanced models use smaller training sets more efficiently than those without. ","date":"2021-03-24","objectID":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/:5:4","tags":["NLP","ELMo","Pre-training","notes\""],"title":"【NLP Papers】ELMo: Deep contextualized word representations","uri":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/"},{"categories":["NLP"],"content":"5.5 Visualizatino of learned weights visualize the ELMo learned weights across the tasks ","date":"2021-03-24","objectID":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/:5:5","tags":["NLP","ELMo","Pre-training","notes\""],"title":"【NLP Papers】ELMo: Deep contextualized word representations","uri":"/blog/nlp-paperselmo_-deep-contextualized-word-represe/"},{"categories":["Deep Learning"],"content":"类似一般概率模型，序列概率模型的两个基本问题： 概率密度估计 样本生成 ","date":"2021-03-23","objectID":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/:0:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第15章 - 序列生成模型","uri":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"15.1 序列概率模型 序列数据的概率密度估计可以转换为单变量的条件概率估计问题： $$ p\\left(x_{t} \\mid \\boldsymbol{x}_{1}:(t-1)\\right) $$ 给定N个序列的数据集，序列概率模型学习模型$p_{\\theta}\\left(x \\mid \\boldsymbol{x}_{1:(t-1)}\\right)$来最大化整个数据集的对数似然函数： $$ \\max _{\\theta} \\sum_{n=1}^{N} \\log p_{\\theta}\\left(\\boldsymbol{x}_{1: T_{n}}^{(n)}\\right)=\\max _{\\theta} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} \\log p_{\\theta}\\left(x_{t}^{(n)} \\mid \\boldsymbol{x}_{1:(t-1)}^{(n)}\\right) $$ 这种每次将前一步的输出作为当前输入的方式称为自回归（AutoRegressive），这类模型称为自动回归生成模型（AutoRegressive Generative Model） 主流自回归生成模型： N-gram 深度序列模型 ","date":"2021-03-23","objectID":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/:1:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第15章 - 序列生成模型","uri":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"15.1.1 序列生成 搜索： greedy beam search ","date":"2021-03-23","objectID":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/:1:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第15章 - 序列生成模型","uri":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"15.2 N元统计模型 N 元模型（N-Gram Model）：假设每个词只依赖于前面N-1个词： $$ p\\left(x_{t} \\mid \\boldsymbol{x}_{1:(t-1)}\\right)=p\\left(x_{t} \\mid \\boldsymbol{x}_{(t-N+1):(t-1)}\\right) $$ N=1时，一元（Unigram）模型： $$ p\\left(\\boldsymbol{x}_{1: T} ; \\theta\\right)=\\prod_{t=1}^{T} p\\left(x_{t}\\right)=\\prod_{k=1}^{|\\mathcal{V}|} \\theta_{k}^{m_{k}} $$ 可以证明，其最大似然估计等于频率估计 N元时，条件概率也可以通过最大似然函数得到： $$ p\\left(x_{t} \\mid \\boldsymbol{x}_{(t-N+1):(t-1)}\\right)=\\frac{\\mathrm{m}\\left(\\boldsymbol{x}_{(t-N+1): t}\\right)}{\\mathrm{m}\\left(\\boldsymbol{x}_{(t-N+1):(t-1)}\\right)} $$ 平滑技术 N-gram一个主要问题：数据稀疏 直接解法：增加数据 Zipf 定律（Zipf’s Law）：给定自然语言数据集，单词出现频率与其频率排名成反比。 平滑技术：对未出现猜测词组赋予一定先验概率 加法平滑： $$ p\\left(x_{t} \\mid \\boldsymbol{x}_{(t-N+1):(t-1)}\\right)=\\frac{\\mathrm{m}\\left(\\boldsymbol{x}_{(t-N+1): t}\\right)+\\delta}{\\mathrm{m}\\left(\\boldsymbol{x}_{(t-N+1):(t-1)}\\right)+\\delta|\\mathcal{V}|} $$ 其中 $\\delta \\in (0, 1]$ ","date":"2021-03-23","objectID":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/:2:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第15章 - 序列生成模型","uri":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"15.3 深度序列模型 深度序列模型（Deep Sequence Model）：用神经网络估计条件概率： $$ p_{\\theta}\\left(x_{t} \\mid \\boldsymbol{x}_{1:(t-1)}\\right)=f_{k_{x_{t}}}\\left(\\boldsymbol{x}_{1:(t-1)} ; \\theta\\right) $$ ","date":"2021-03-23","objectID":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/:3:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第15章 - 序列生成模型","uri":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"15.3.1 模型结构 嵌入层 特征层 简单平均 FNN/CNN RNN 输出层 ","date":"2021-03-23","objectID":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/:3:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第15章 - 序列生成模型","uri":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"15.3.2 参数学习 给定训练序列，训练目标是找到参数 $\\theta$ （embed、weight、bias等）使得对数似然函数最大： $$ p_{\\theta}\\left(x_{t} \\mid \\boldsymbol{x}_{1:(t-1)}\\right)=f_{k_{x_{t}}}\\left(\\boldsymbol{x}_{1:(t-1)} ; \\theta\\right) $$ 一般通过梯度上升法学习： $$ \\theta \\leftarrow \\theta+\\alpha \\frac{\\partial \\log p_{\\theta}\\left(\\boldsymbol{x}_{1: T}\\right)}{\\partial \\theta} $$ ","date":"2021-03-23","objectID":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/:3:2","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第15章 - 序列生成模型","uri":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"15.4 评价方法 ","date":"2021-03-23","objectID":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/:4:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第15章 - 序列生成模型","uri":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"15.4.1 困惑度 困惑度（Perplexity）衡量分布的不确定性，随机变量 $X$ 的困惑度： $$ 2^{H(p)}=2^{-\\sum_{x \\in x} p(x) \\log _{2} p(x)} $$ 指数为分布p的熵。 困惑度也可以衡量两个分布的差异，对未知数据分布采样，则模型分布的困惑度为： $$ 2^{H\\left(\\tilde{p}_{r}, p_{\\theta}\\right)}=2^{-\\frac{1}{N} \\sum_{n=1}^{N} \\log _{2} p_{\\theta}\\left(x^{(n)}\\right)} $$ 指数为经验分布和模型分布交叉熵，也是所有样本的负对数似然函数。 困惑度衡量了模型分布和样本经验分布之间的契合程度，困惑度越低两个分布越接近。 对N个独立同分布的序列，测试集的联合概率为： $$ \\prod_{n=1}^{N} p_{\\theta}\\left(\\boldsymbol{x}_{1: T_{n}}^{(n)}\\right)=\\prod_{n=1}^{N} \\prod_{t=1}^{T_{n}} p_{\\theta}\\left(x_{t}^{(n)} \\mid \\boldsymbol{x}_{1:(t-1)}^{(n)}\\right) $$ 模型$p_\\theta(x)$的困惑度定义为： $$ \\begin{aligned} \\operatorname{PPL}(\\theta) \u0026=2^{-\\frac{1}{T} \\sum_{n=1}^{N} \\log _{2} p_{\\theta}\\left(x_{1: T_{n}}^{(n)}\\right)} \\\\ \u0026=2^{-\\frac{1}{T} \\sum_{n=1}^{N} \\sum_{t=1}^{T n} \\log _{2} p_{\\theta}\\left(x_{t}^{(n)} \\mid x_{1:(t-1)}^{(n)}\\right)} \\\\ \u0026=\\left(\\prod_{n=1}^{N} \\prod_{t=1}^{T_{n}} p_{\\theta}\\left(x_{t}^{(n)} \\mid x_{1:(t-1)}^{(n)}\\right)\\right)^{-1 / T} \\end{aligned} $$ 其中$T$为测试序列总长度。可以看到，困惑度为每个词的条件概率的几何平均数的倒数。 ","date":"2021-03-23","objectID":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/:4:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第15章 - 序列生成模型","uri":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"15.4.2 BLEU BLEU（BiLingual Evaluation Understudy）：衡量生成序列与参考序列之间N-Gram重合度。 N 元组合的精度（Precision）： $$ P_{N}(\\boldsymbol{x})=\\frac{\\sum_{w \\in \\mathcal{W}} \\min \\left(c_{w}(\\boldsymbol{x}), \\max _{k=1}^{K} c_{w}\\left(\\boldsymbol{s}^{(k)}\\right)\\right)}{\\sum_{w \\in \\mathcal{W}} c_{w}(\\boldsymbol{x})}, $$ 对每个N元组合$w$，累加$w$在K个参考序列中出现的最多次数，除以总N元组合个数，得到生成序列的N元组合在参考序列出现的比例。 由于生成序列越短，精度会越高，引入长度惩罚因子（Brevity Penalty）： $$ b(\\boldsymbol{x})=\\left{\\begin{array}{ccc} 1 \u0026 \\text { if } \u0026 l_{x}\u003el_{s} \\\\ \\exp \\left(1-l_{s} / l_{x}\\right) \u0026 \\text { if } \u0026 l_{x} \\leq l_{s} \\end{array}\\right. $$ BLEU是对不同长度的N元组合精度的几何加权平均： $$ \\operatorname{BLEU-N}(\\boldsymbol{x})=b(\\boldsymbol{x}) \\times \\exp \\left(\\sum_{N=1}^{N^{\\prime}} \\alpha_{N} \\log P_{N}\\right) $$ 注：BLEU只计算精度，不关心召回率。 ","date":"2021-03-23","objectID":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/:4:2","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第15章 - 序列生成模型","uri":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"15.4.3 ROUGE ROUGE（Recall-Oriented Understudy for Gisting Evaluation） 最早应用与文本摘要 计算召回率 $$ \\operatorname{ROUGE-N}(\\boldsymbol{x})=\\frac{\\sum_{k=1}^{K} \\sum_{w \\in \\mathcal{W}} \\min \\left(c_{w}(\\boldsymbol{x}), c_{w}\\left(\\boldsymbol{s}^{(k)}\\right)\\right)}{\\sum_{k=1}^{K} \\sum_{w \\in \\mathcal{W}} c_{w}\\left(\\boldsymbol{s}^{(k)}\\right)}, $$ ","date":"2021-03-23","objectID":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/:4:3","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第15章 - 序列生成模型","uri":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"15.5 序列生成模型中的学习问题 ","date":"2021-03-23","objectID":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/:5:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第15章 - 序列生成模型","uri":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"15.6 序列到序列 seq2seq：机器翻译、语音识别、文本摘要、对话系统、图像标题生成等 seq2seq模型目标是估计条件概率： $$ p_{\\theta}\\left(\\boldsymbol{y}_{1: T} \\mid \\boldsymbol{x}_{1: S}\\right)=\\prod_{t=1}^{T} p_{\\theta}\\left(y_{t} \\mid \\boldsymbol{y}_{1:(t-1)}, \\boldsymbol{x}_{1: S}\\right) $$ 用最大似然估计训练模型参数： $$ \\hat{\\theta}=\\underset{\\theta}{\\arg \\max } \\sum_{n=1}^{N} \\log p_{\\theta}\\left(\\boldsymbol{y}_{1: T_{n}} \\mid \\boldsymbol{x}_{1: S_{n}}\\right) $$ 根据输入序列生成最可能目标序列（greedy / beam search）： $$ \\hat{\\boldsymbol{y}}=\\underset{\\boldsymbol{y}}{\\arg \\max } p_{\\hat{\\theta}}(\\boldsymbol{y} \\mid \\boldsymbol{x}) $$ 条件概率 $p_{\\theta}\\left(y_{t} \\mid \\boldsymbol{y}_{1:(t-1)}, \\boldsymbol{x}_{1: S}\\right)$ 可以通过不同神经网络实现，如RNN、注意力模型等。 ","date":"2021-03-23","objectID":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/:6:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第15章 - 序列生成模型","uri":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"15.6.1 基于RNN的seq2seq 编码器-解码器（Encoder-Decoder）模型 缺点： 编码向量信息容量瓶颈 对长序列存在长程依赖问题，容易丢失输入序列的信息 ","date":"2021-03-23","objectID":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/:6:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第15章 - 序列生成模型","uri":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"15.6.1 基于注意力的seq2seq 解码过程中，将上一步的隐状态$h^{dec}_{t-1}$作为查询向量，对所用输入序列的隐状态中选择信息： $$ \\begin{aligned} \\boldsymbol{c}_{t} \u0026=\\operatorname{att}\\left(\\boldsymbol{H}^{\\mathrm{enc}}, \\boldsymbol{h}_{t-1}^{\\mathrm{dec}}\\right)=\\sum_{i=1}^{S} \\alpha_{i} \\boldsymbol{h}_{i}^{\\mathrm{enc}} \\\\ \u0026=\\sum_{i=1}^{S} \\operatorname{softmax}\\left(s\\left(\\boldsymbol{h}_{i}^{\\mathrm{enc}}, \\boldsymbol{h}_{t-1}^{\\mathrm{dec}}\\right)\\right) \\boldsymbol{h}_{i}^{\\mathrm{enc}} \\end{aligned} $$ 将从输入序列中选择的信息$c_t$也作为解码器的输入，得到t步骤的隐状态： $$ \\boldsymbol{h}_{t}^{\\mathrm{dec}}=f_{\\mathrm{dec}}\\left(\\boldsymbol{h}_{t-1}^{\\mathrm{dec}},\\left[\\boldsymbol{e}_{y_{t-1}} ; \\mathbf{c}_{t}\\right], \\theta_{\\mathrm{dec}}\\right) $$ 最后，将 $\\boldsymbol{h}_{t}^{\\mathrm{dec}}$ 输入到分类器得到每个词的概率。 ","date":"2021-03-23","objectID":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/:6:2","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第15章 - 序列生成模型","uri":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"15.6.1 基于自注意力的seq2seq 基于CNN的seq2seq除了长程依赖，还有无法并行计算的缺陷，自注意力模型解决了这个问题。这里主要介绍Transformer。 自注意力： $$ \\begin{array}{l} \\operatorname{self}-\\operatorname{att}(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V})=\\boldsymbol{V} \\operatorname{softmax}\\left(\\frac{\\boldsymbol{K}^{\\top} \\boldsymbol{Q}}{\\sqrt{D_{k}}}\\right), \\\\ \\boldsymbol{Q}=\\boldsymbol{W}_{q} \\boldsymbol{H}, \\boldsymbol{K}=\\boldsymbol{W}_{k} \\boldsymbol{H}, \\boldsymbol{V}=\\boldsymbol{W}_{v} \\boldsymbol{H} \\end{array} $$ 多头注意力： $$ \\begin{array}{c} \\operatorname{MultiHead}(\\boldsymbol{H})=\\boldsymbol{W}_{o}\\left[\\text { head }_{1} ; \\cdots ; \\text { head }_{M}\\right] \\\\ \\qquad \\text { head }_{m}=\\operatorname{self}-\\operatorname{att}\\left(\\boldsymbol{Q}_{m}, \\boldsymbol{K}_{m}, \\boldsymbol{V}_{m}\\right) \\\\ \\forall m \\in{1, \\cdots, M}, \\quad \\boldsymbol{Q}_{m}=\\boldsymbol{W}_{q}^{m} \\boldsymbol{H}, \\boldsymbol{K}=\\boldsymbol{W}_{k}^{m} \\boldsymbol{H}, \\boldsymbol{V}=\\boldsymbol{W}_{v}^{m} \\boldsymbol{H} \\end{array} $$ 基于self-attention的序列编码 由于self-attention忽略了未知信息，需要在初始输入序列中加入位置编码： $$ \\boldsymbol{H}^{(0)}=\\left[\\boldsymbol{e}_{x_{1}}+\\boldsymbol{p}_{1}, \\cdots, \\boldsymbol{e}_{x_{T}}+\\boldsymbol{p}_{T}\\right] $$ 其中$p_t$为位置编码，可以作为可学习参数，也可以预定义为： $$ \\begin{aligned} \\boldsymbol{p}_{t, 2 i} \u0026=\\sin \\left(t / 10000^{2 i / D}\\right) \\\\ \\boldsymbol{p}_{t, 2 i+1} \u0026=\\cos \\left(t / 10000^{2 i / D}\\right), \\end{aligned} $$ $\\boldsymbol{p}_{t, 2 i}$表示第t个位置编码向量的第$2i$维，D是编码向量的维度。 l层隐状态$H^{(l)}$可以通过l-1层的隐状态$H^{(l-1)}$获得： $$ \\begin{array}{l} \\boldsymbol{Z}^{(l)}=\\operatorname{norm}\\left(\\boldsymbol{H}^{(l-1)}+\\operatorname{MultiHead}\\left(\\boldsymbol{H}^{(l-1)}\\right)\\right) \\\\ \\boldsymbol{H}^{(l)}=\\operatorname{norm}\\left(\\boldsymbol{Z}^{(l)}+\\operatorname{FFN}\\left(\\boldsymbol{Z}^{(l)}\\right)\\right) \\end{array} $$ 这里的FFN为position-wise： $$ \\operatorname{FFN}(z)=W_{2} \\operatorname{ReLu}\\left(\\boldsymbol{W}_{1} \\boldsymbol{z}+\\boldsymbol{b}_{1}\\right)+\\boldsymbol{b}_{2} $$ 基于self-attention的序列编码可以看作全连接的FNN。 Transfermer 基于多头自注意力的seq2seq 编码器：多层的多头注意力，输入序列$\\boldsymbol{x}_{1: S}$，输出隐状态序列$\\boldsymbol{H}^{\\mathrm{enc}}=\\left[\\boldsymbol{h}_{1}^{\\mathrm{enc}}, \\cdots, \\boldsymbol{h}_{S}^{\\mathrm{en}}\\right]$，再映射为键值对供解码器使用： $$ \\begin{array}{l} \\boldsymbol{K}^{\\text {enc }}=\\boldsymbol{W}_{k}^{\\prime} \\boldsymbol{H}^{\\text {enc }}, \\\\ \\boldsymbol{V}^{\\text {enc }}=\\boldsymbol{W}_{v}^{\\prime} \\boldsymbol{H}^{\\text {enc }} \\end{array} $$ 解码器：自回归方式，三个部分： a. 掩蔽自注意模块：使用自注意力编码已知前缀序列$\\boldsymbol{y}_{0:(t-1)}$，得到$\\boldsymbol{H}^{\\mathrm{dec}}=\\left[\\boldsymbol{h}_{1}^{\\mathrm{dec}}, \\cdots, \\boldsymbol{h}_{t}^{\\mathrm{dec}}\\right]$ b. 解码器到编码器模块：将$h_t^{dec}$线性映射得到$q_t^{dec}$，从编码器得到的键值对查询相关信息 c. 逐位置的FNN：使用FNN综合所有信息 训练时对解码器输入处理的trick：掩蔽自注意力（Masked Self-Attention） 将右移的目标序列（Right-Shifted Output）$\\mathcal{Y}_{0}:(T-1)$ 作为输入 通过掩码来屏蔽后面的输入信息 ","date":"2021-03-23","objectID":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/:6:3","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第15章 - 序列生成模型","uri":"/blog/nndl-book-ch15-%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"Neural Architectures for Named Entity Recognition [Lample et. al., 2016] ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:0:0","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"摘要 NER之前的SOTA：大量手工特征、领域知识，泛化能力差 介绍了两种模型： BiLSTM-CRF Stack-LSTM：类似移进-规约的 transition-based 方法 word representations： character-based learned from supervised corpus word level learned from unannotated corpora 实验结果：BiLSTM-CRF和S-LSTM（表现稍逊）在四种语言（English, Dutch, German, and Spanish）的NER上SOTA ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:1:0","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"1. Introduction model旨在捕捉两种信息： 多个字符组合： a. BiLSTM-CRF b. S-LSTM “being a name\"的特征 a. orthographic evidence：属于name的word的特征 - chat-based word representations b. distributional evidence: name在句子中位置 - distributional representations 结合两种表示，并使用了dropout防止过度依赖其中一种。 ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:2:0","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"2. LSTM-CRF Model ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:3:0","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"2.1 LSTM $$ \\begin{aligned} \\mathbf{i}_{t}=\u0026 \\sigma\\left(\\mathbf{W}_{x i} \\mathbf{x}_{t}+\\mathbf{W}_{h i} \\mathbf{h}_{t-1}+\\mathbf{W}_{c i} \\mathbf{c}_{t-1}+\\mathbf{b}_{i}\\right) \\\\ \\mathbf{c}_{t}=\u0026\\left(1-\\mathbf{i}_{t}\\right) \\odot \\mathbf{c}_{t-1}+\\ \u0026 \\mathbf{i}_{t} \\odot \\tanh \\left(\\mathbf{W}_{x c} \\mathbf{x}_{t}+\\mathbf{W}_{h c} \\mathbf{h}_{t-1}+\\mathbf{b}_{c}\\right) \\\\ \\mathbf{o}_{t}=\u0026 \\sigma\\left(\\mathbf{W}_{x o} \\mathbf{x}_{t}+\\mathbf{W}_{h o} \\mathbf{h}_{t-1}+\\mathbf{W}_{c o} \\mathbf{c}_{t}+\\mathbf{b}_{o}\\right) \\\\ \\mathbf{h}_{t}=\u0026\\mathbf{o}_{t} \\odot \\tanh \\left(\\mathbf{c}_{t}\\right) \\end{aligned} $$ 相比于原始LSTM有两点改进： peephole连接：门也依赖与上一时刻记忆单元 耦合输入门和遗忘门：$\\boldsymbol{f}_{t}=1-\\boldsymbol{i}_{t}$ 使用了BiLSTM ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:3:1","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"2.2 CRF Tagging Models 对LSTM输出的$h_t$用CRF进行jointly model 输入： $$ \\mathbf{X}=\\left(\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{n}\\right) $$ BiLSTM输出的发射分值矩阵：$\\mathbf{P}$, size: $n\\times k$ k: # of tags 假设预测tag序列为： $$ \\mathbf{y}=\\left(y_{1}, y_{2}, \\ldots, y_{n}\\right) $$ tag转移矩阵：$\\mathbf{A}$, size: $(k+2)\\times (k+2)$ $y_0, y_n$分别表示句子的 start 和 end 标签 分数定义为： $$ s(\\mathbf{X}, \\mathbf{y})=\\sum_{i=0}^{n} A_{y_{i}, y_{i+1}}+\\sum_{i=1}^{n} P_{i, y_{i}} $$ softmax计算$y$的条件概率： $$ p(\\mathbf{y} \\mid \\mathbf{X})=\\frac{e^{s(\\mathbf{X}, \\mathbf{y})}}{\\sum_{\\widetilde{\\mathbf{y}} \\in \\mathbf{Y}_{\\mathbf{X}}} e^{s(\\mathbf{X}, \\widetilde{\\mathbf{y}})}} $$ 训练优化正确tag序列的log-probability： $$ \\begin{aligned} \\log (p(\\mathbf{y} \\mid \\mathbf{X})) \u0026=s(\\mathbf{X}, \\mathbf{y})-\\log \\left(\\sum_{\\tilde{\\mathbf{y}} \\in \\mathbf{Y}_{\\mathbf{X}}} e^{s(\\mathbf{X}, \\widetilde{\\mathbf{y}})}\\right) \\\\ \u0026=s(\\mathbf{X}, \\mathbf{y})-\\underset{\\widetilde{\\mathbf{y}} \\in \\mathbf{Y}_{\\mathbf{X}}}{\\operatorname{logadd}} s(\\mathbf{X}, \\widetilde{\\mathbf{y}}) \\end{aligned} $$ 其中，$\\mathbf{Y_X}$表示所有可能tag序列。 预测： $$ \\mathbf{y}^{*}=\\underset{\\tilde{\\mathbf{y}} \\in \\mathbf{Y}_{\\mathbf{X}}}{\\operatorname{argmax}} s(\\mathbf{X}, \\widetilde{\\mathbf{y}}) $$ ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:3:2","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"2.3 Parameterization and Training 网络的主要结构： BiLSRM的输出是每个词的各个词性的评分，即发射分值矩阵$\\mathbf{P}$，由BiLSTM 得到的（word-in-context词向量$c_i$）与 二元语法的转移评分计算（$\\mathbf{A}_{y,y’}$) 计算 ？ 将$c_i$线性映射到每个标签上，从而得到得分。 参数： bigram compatibility scores $\\mathbf{A}$ matrix $\\mathbf{P}$ BiLSTM参数 linear feature weights ? word embeddings $c_i$与CRF层之间加入一个隐层，效果略好。 ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:4:0","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"2.4 Tagging Schemes 没有使用IOB格式（Inside, Outside, Beginning)：I-label, O-label, B-label。 使用IOBES：singleton entities（S）、End（E） ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:5:0","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"3. Transition-Based Chunking Model 略。 ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:6:0","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"4. Input Word Embeddings Char-level representattion：对单词拼写(morphologically)敏感 Pretrained embeddings(skip-n-gram)：对词顺序敏感 ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:7:0","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"4.1 Character-based models of words 如上图，每个单词的embeddings由三组embedding拼接： char-level（表征suffix）: forward LSTM char-level（表征prefix）: backward LSTM word-level: lookup table 测试遇到 UNK 的处理：以0.5概率替换singletons，训练得到 UNK 的embedding 为什么LSTM比CNN能更好地建模word和char关系？ CNN捕捉位置无关信息（共享权重），而word的信息与char所在位置有关（如前后缀和词干stems编码了不同信息） ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:7:1","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"4.2 Pretrained embeddings 用预训练词向量初始化lookup table 预训练词向量采用 skip-n-gram (Ling et al., 2015a)：考虑了词顺序的 skip-gram (Mikolov et al., 2013a) ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:7:2","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"4.3 Dropout training use dropout (Hinton et al., 2012) to encourage the model to depend on both representations (significant improment). ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:7:3","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"5. Experiments ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:8:0","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"5.1 Training SGD with gradient clipping better than Adadelta/Adam :) ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:8:1","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"5.2 Data Sets CoNLL-2002 and CoNLL-2003 datasets 预处理：英文NER中替换所有数字为0 ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:8:2","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"5.3 Results BiLSTM-CRF 在CoNLL 2003 (English) 上的 F1 达到 90.94，在未使用额外语料的模型中达到SOTA ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:8:3","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"总结 首次将BiLSTM-CRF用于序列标注是 Bidirectional LSTM-CRF Models for Sequence Tagging [Huang et. al., 2015]，而本文最大特点是在 pre-trained word embedding（skip-n-gram）的基础上结合了character-based word embeddings，通过引入字符级特征提高了模型在NER任务中的表现； CRF考虑了序列标注中全局范围内转移概率； Dropout可以用于平衡多种特征比例。 论文描述不确切地方：2.3 ...adding a hidden layer between ci and the CRF layer...，应该是指在 c_i 和 token2tag 线性映射层之间添加一个隐层。 ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:9:0","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["NLP"],"content":"References https://createmomo.github.io/2017/09/12/CRF_Layer_on_the_Top_of_BiLSTM_1/ ","date":"2021-03-10","objectID":"/blog/nlp-papersnerbilstm-crf/:10:0","tags":["NLP","LSTM","NER","CRF","notes\""],"title":"【NLP Papers】NER：BiLSTM-CRF","uri":"/blog/nlp-papersnerbilstm-crf/"},{"categories":["Project"],"content":"SkipGram NegativeSampling implemented in PyTorch. GitHub: https://github.com/ZubinGou/SGNS-PyTorch ","date":"2021-03-08","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/:0:0","tags":["PyTorch","word2vec","NLP","Pre-training"],"title":"基于PyTorch实现word2vec模型及其优化","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/"},{"categories":["Project"],"content":"Paper Efficient Estimation of Word Representations in Vector Space (original word2vec paper) Distributed Representations of Words and Phrases and their Compositionality (negative sampling paper) ","date":"2021-03-08","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/:1:0","tags":["PyTorch","word2vec","NLP","Pre-training"],"title":"基于PyTorch实现word2vec模型及其优化","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/"},{"categories":["Project"],"content":"Notes Word2Vec是用无监督方式从文本中学习词向量来表征语义信息的模型，语义相近的词在嵌入空间中距离相近。类似于auto-encoder，Word2Vec训练的神经网络不用于处理新任务，真正需要的是模型参数，即隐层的权重矩阵。 Skip-gram是在给定目标单词的情况下，预测其上下文单词。 用两个word matrix，W表示目标单词向量矩阵(V*N)，W’表示上下文单词向量矩阵（N*V），词向量维度N，词汇表维度V。 模型： 投影：$h_i=Wx_k$ 计算相似度：$z=W’h_i$ 转换为概率分布：$\\hat y=\\text{softmax}(z)$ 高效训练的三个trick（来自第二篇paper）： subsampling of the frequent words nagative sampling (alternative to hierarchical softmax) treat word pairs / phases as one word ","date":"2021-03-08","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/:2:0","tags":["PyTorch","word2vec","NLP","Pre-training"],"title":"基于PyTorch实现word2vec模型及其优化","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/"},{"categories":["Project"],"content":"Subsampling 高频词数量远超训练所需，所以进行抽样，基于词频以一定概率丢弃词汇（论文中公式）： $$ P\\left(w_{i}\\right)=1-\\sqrt{\\frac{t}{f\\left(w_{i}\\right)}} $$ 作者实际使用的公式（t默认0.0001）： $$P\\left(w_{i}\\right)=\\sqrt{\\frac{t}{f\\left(w_{i}\\right)}} + \\frac{t}{f\\left(w_{i}\\right)}$$ ","date":"2021-03-08","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/:2:1","tags":["PyTorch","word2vec","NLP","Pre-training"],"title":"基于PyTorch实现word2vec模型及其优化","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/"},{"categories":["Project"],"content":"Negative Sampling 负采样使得每个训练样本仅更新一小部分权重。negative word指期望概率为0的单词，选取概率为： $$ P_n(w_i)=f(w_i)^{3 / 4} / Z $$ ","date":"2021-03-08","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/:2:2","tags":["PyTorch","word2vec","NLP","Pre-training"],"title":"基于PyTorch实现word2vec模型及其优化","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/"},{"categories":["Project"],"content":"训练 在 text8 语料上训练，默认采用词向量维数为100，词典大小为50000，window_size为5，负采样数为10。 ","date":"2021-03-08","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/:3:0","tags":["PyTorch","word2vec","NLP","Pre-training"],"title":"基于PyTorch实现word2vec模型及其优化","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/"},{"categories":["Project"],"content":"评估 基于词向量的语言学特性 similarity task 词相似 analogy task 词类比 (A-B=C-D) Task-specific 对具体任务的性能提升 这里基于词相似，在 WordSim-353、Stanford Rare Word (RW) 和 SimLex-999 上利用 Spearman's rank correlation coefficient 进行评估。 ","date":"2021-03-08","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/:4:0","tags":["PyTorch","word2vec","NLP","Pre-training"],"title":"基于PyTorch实现word2vec模型及其优化","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/"},{"categories":["Project"],"content":"结果 训练1小时（4个epoch），尚未完全拟合的情况下效果如下。对照 Gensim Word2vec 默认训练结果和 GoogleNews-vectors-negative300 ： WordSim353 RW SimLex-999 Corpus embed_dim vocab_size Time Gensim 0.624 0.320 0.250 text8 100 71290 1min SGNS-PyTorch 0.661 0.343 0.265 text8 100 50000 1h GoogleNews 0.659 0.553 0.436 GoogleNews 300 3000000 - 测试过程和结果在 test.ipynb ","date":"2021-03-08","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/:5:0","tags":["PyTorch","word2vec","NLP","Pre-training"],"title":"基于PyTorch实现word2vec模型及其优化","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/"},{"categories":["Project"],"content":"References https://github.com/Andras7/word2vec-pytorch https://github.com/fanglanting/skip-gram-pytorch ","date":"2021-03-08","objectID":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/:6:0","tags":["PyTorch","word2vec","NLP","Pre-training"],"title":"基于PyTorch实现word2vec模型及其优化","uri":"/blog/%E5%9F%BA%E4%BA%8Epytorch%E5%AE%9E%E7%8E%B0word2vec%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/"},{"categories":["Deep Learning"],"content":"用CNN等编码一个向量表示文本所有特征，存在信息瓶颈 网络容量（Network Capacity）：存储信息受限与神经元数量和网络复杂度 对于过载信息：引入注意力和记忆机制 ","date":"2021-02-26","objectID":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/:0:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第8章 - 注意力机制和外部记忆","uri":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/"},{"categories":["Deep Learning"],"content":"8.1 认知神经科学中的注意力 注意力（Attention）：从大量信息中选择小部分有用信息重点处理，忽略其他信息 注意力分类： 自上而下的有意识的注意力，称为聚焦式注意力（Focus Attention）/ 选择性注意力（Selective Attention）：有预定目的、依赖任务的，主动有意识地聚焦于某一对象的注 意力 自下而上的无意识的注意力，称为基于显著性的注意力（Saliency-Based Attention）：如果一个对象的刺激信息不同于其周围信息，一种无意识的“赢者通吃”（Winner-Take-All）或者门控（Gating）机制就可以把注意力转向这个对象 ","date":"2021-02-26","objectID":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/:1:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第8章 - 注意力机制和外部记忆","uri":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/"},{"categories":["Deep Learning"],"content":"8.2 注意力机制 可以将最大汇聚（Max Pooling）、门控（Gating）机制近似地看作自下而上的基于显著性的注意力机制 注意力机制计算步骤： 在所有输入信息上计算注意力分布 根据注意力分布计算输入信息的加权分布 注意力分布 $$ \\begin{aligned} \\alpha_{n} \u0026=p(z=n \\mid \\boldsymbol{X}, \\boldsymbol{q}) \\\\ \u0026=\\operatorname{softmax}\\left(s\\left(\\boldsymbol{x}_{n}, \\boldsymbol{q}\\right)\\right) \\\\ \u0026=\\frac{\\exp \\left(s\\left(\\boldsymbol{x}_{n}, \\boldsymbol{q}\\right)\\right)}{\\sum_{j=1}^{N} \\exp \\left(s\\left(\\boldsymbol{x}_{j}, \\boldsymbol{q}\\right)\\right)} \\end{aligned} $$ 查询向量（Query Vector）$q$ 注意力变量$z \\in[1, N]$，表示被选择信息的索引位置 注意力分布（Attention Distribution）$\\alpha_{n}$ 注意力打分函数$s(\\boldsymbol{x}, \\boldsymbol{q})$ $$ \\begin{aligned} \\text { 加性模型 } \u0026 \u0026 s(\\boldsymbol{x}, \\boldsymbol{q})\u0026=\\boldsymbol{v}^{\\top} \\tanh (\\boldsymbol{W} \\boldsymbol{x}+\\boldsymbol{U} \\boldsymbol{q})\\ \\text { 点积模型 } \u0026 \u0026 s(\\boldsymbol{x}, \\boldsymbol{q}) \u0026=\\boldsymbol{x}^{\\top} \\boldsymbol{q} \\\\ \\text { 缩放点积模型 } \u0026 \u0026 s(\\boldsymbol{x}, \\boldsymbol{q}) \u0026=\\frac{\\boldsymbol{x}^{\\top} \\boldsymbol{q}}{\\sqrt{D}}, \\\\ \\text { 双线性模型 } \u0026 \u0026 s(\\boldsymbol{x}, \\boldsymbol{q})\u0026=\\boldsymbol{x}^{\\top} \\boldsymbol{W} \\boldsymbol{q}, \\end{aligned} $$ 加权平均 软性注意力机制（Soft Attention Mechanism $$ \\begin{aligned} \\operatorname{att}(\\boldsymbol{X}, \\boldsymbol{q}) \u0026=\\sum_{n=1}^{N} \\alpha_{n} \\boldsymbol{x}_{n}, \\\\ \u0026=\\mathbb{E}_{\\boldsymbol{z} \\sim p(z \\mid \\boldsymbol{X}, \\boldsymbol{q})}\\left[\\boldsymbol{x}_{z}\\right] \\end{aligned} $$ ","date":"2021-02-26","objectID":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/:2:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第8章 - 注意力机制和外部记忆","uri":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/"},{"categories":["Deep Learning"],"content":"8.2.1 注意力机制的变体 硬性注意力 只关注某一个输入向量 两种实现方式： 最大采样：$\\operatorname{att}(\\boldsymbol{X}, \\boldsymbol{q})=\\boldsymbol{x}_{\\hat{n}}$，$\\hat{n}=\\underset{n=1}{\\arg \\max } \\alpha_{n}$ 随机采样（根据注意力分布） 硬性注意力缺点：损失函数和注意力分布函数关系不可导，无法用BP训练，通常采用强化学习训练。 键值对注意力 键计算注意力分布，值计算聚合信息 $$ \\begin{aligned} \\operatorname{att}((\\boldsymbol{K}, \\boldsymbol{V}), \\boldsymbol{q}) \u0026=\\sum_{n=1}^{N} \\alpha_{n} \\boldsymbol{v}_{n} \\\\ \u0026=\\sum_{n=1}^{N} \\frac{\\exp \\left(s\\left(\\boldsymbol{k}_{n}, \\boldsymbol{q}\\right)\\right)}{\\sum_{j} \\exp \\left(s\\left(\\boldsymbol{k}_{j}, \\boldsymbol{q}\\right)\\right)} \\boldsymbol{v}_{n}, \\end{aligned} $$ 多头注意力 多头注意力（Multi-Head Attention）是利用多个查询 𝑸 = [𝒒1, ⋯ , 𝒒𝑀]，来并行地从输入信息中选取多组信息： $$ \\operatorname{att}((\\boldsymbol{K}, \\boldsymbol{V}), \\boldsymbol{Q})=\\operatorname{att}\\left((\\boldsymbol{K}, \\boldsymbol{V}), \\boldsymbol{q}_{1}\\right) \\oplus \\cdots \\oplus \\operatorname{att}\\left((\\boldsymbol{K}, \\boldsymbol{V}), \\boldsymbol{q}_{M}\\right) $$ 结构化注意力 输入信息本身具有层次（Hierarchical）结构，比如文本可以分为词、句子、段落、篇章等不同粒度的层次。 可以使用层次化的注意力来进行更好的信息选择 [Yang et al.,2016] 可以假设注意力为上下文相关的二项分布，用一种图模型来构建更复杂的结构化注意力分布 [Kim et al., 2017] 指针网络 指针网络（Pointer Network）[Vinyals et al., 2015]是一种Seq2seq模型，输入$\\boldsymbol{X}=\\boldsymbol{x}_{1}, \\cdots, \\boldsymbol{x}_{N}$，输出$\\boldsymbol{c}_{1: M}=c_{1}, c_{2}, \\cdots, c_{M}, c_{m} \\in[1, N], \\forall m$ 输出为输入序列的下标 $$ \\begin{aligned} p\\left(c_{1: M} \\mid \\boldsymbol{x}_{1: N}\\right) \u0026=\\prod_{m=1}^{M} p\\left(c_{m} \\mid c_{1:(m-1)}, \\boldsymbol{x}_{1: N}\\right) \\\\ \u0026 \\approx \\prod_{m=1}^{M} p\\left(c_{m} \\mid \\boldsymbol{x}_{c_{1}}, \\cdots, \\boldsymbol{x}_{c_{m-1}}, \\boldsymbol{x}_{1: N}\\right), \\end{aligned} $$ $$ p\\left(c_{m} \\mid c_{1:(m-1)}, \\boldsymbol{x}_{1: N}\\right)=\\operatorname{softmax}\\left(s_{m, n}\\right) $$ $$ s_{m, n}=\\boldsymbol{v}^{\\top} \\tanh \\left(\\boldsymbol{W} \\boldsymbol{x}_{n}+\\boldsymbol{U} \\boldsymbol{h}_{m}\\right), \\forall n \\in[1, N] $$ ","date":"2021-02-26","objectID":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/:2:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第8章 - 注意力机制和外部记忆","uri":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/"},{"categories":["Deep Learning"],"content":"8.3 自注意力模型 自注意力模型（Self-Attention Model）：即内部注意力（Intra-Attention） 自注意力可以作为神经网络中的一层来使用，有效地建模长距离依赖问题 [Vaswani et al., 2017] 经常采用查询-键-值（Query-Key-Value，QKV）模式 $$ \\begin{array}{l} \\boldsymbol{Q}=\\boldsymbol{W}_{q} \\boldsymbol{X} \\in \\mathbb{R}^{D_{k} \\times N} \\\\ \\boldsymbol{K}=\\boldsymbol{W}_{k} \\boldsymbol{X} \\in \\mathbb{R}^{D_{k} \\times N} \\\\ \\boldsymbol{V}=\\boldsymbol{W}_{v} \\boldsymbol{X} \\in \\mathbb{R}^{D_{v} \\times N} \\end{array} $$ $$ \\begin{aligned} \\boldsymbol{h}_{n} \u0026=\\operatorname{att}\\left((\\boldsymbol{K}, \\boldsymbol{V}), \\boldsymbol{q}_{n}\\right) \\\\ \u0026=\\sum_{j=1}^{N} \\alpha_{n j} \\boldsymbol{v}_{j} \\\\ \u0026=\\sum_{j=1}^{N} \\operatorname{softmax}\\left(s\\left(\\boldsymbol{k}_{j}, \\boldsymbol{q}_{n}\\right)\\right) \\boldsymbol{v}_{j}, \\end{aligned} $$ 如使用缩放点积打分，输出可以简写为： $$ \\boldsymbol{H}=\\boldsymbol{V} \\operatorname{softmax}\\left(\\frac{\\boldsymbol{K}^{\\top} \\boldsymbol{Q}}{\\sqrt{D_{k}}}\\right) $$ 实线是可学习的权重 虚线是动态生成的权重，可以处理变长信息 ","date":"2021-02-26","objectID":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/:3:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第8章 - 注意力机制和外部记忆","uri":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/"},{"categories":["Deep Learning"],"content":"8.4 人脑中的记忆 信息作为一种整体效应（Collective Effect）存储在大脑组织中，即记忆在大脑皮层是分布式存储的，而不是存储于某个局部区域。 人脑记忆具有周期性和联想性 记忆周期 长期记忆（Long-Term Memory）：也称为结构记忆或知识（Knowledge），体现为神经元之间的连接形态，其更新速度比较慢 类比权重参数 短期记忆（Short-Term Memory）：体现为神经元的活动，更新较快，维持时间为几秒至几分钟 类比隐状态 工作记忆（Working Memory）：人脑的缓存，短期记忆一般指输入信息在人脑中的表示和短期存储，工作记忆是和任务相关的“容器”。容量较小，一般可以容纳4组项目。 演化（Evolution）过程：．短期记忆、长期记忆的动态更新过程 联想记忆 大脑主要通过联想进行检索 联想记忆（Associative Memory）：学习和记住不同对象之间关系的能力 基于内容寻址的存储（Content-Addressable Memory，CAM）：联想记忆，通过内容匹配方法进行寻址的信息存储方式 随机访问存储（Random Access Memory, RAM)：现代计算机根据地址存储 类比： LSTM记忆单元 \u003c-\u003e 计算机的寄存器 外部记忆 \u003c-\u003e 计算机的内存 神经网络引入外部记忆途径： 结构化记忆，类似于计算机存储信息 基于神经动力学的联想记忆，有更好的生物学解释性 ","date":"2021-02-26","objectID":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/:4:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第8章 - 注意力机制和外部记忆","uri":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/"},{"categories":["Deep Learning"],"content":"8.5 记忆增强神经网络 记忆增强神经网络（Memory Augmented Neural Network，MANN）：简称为记忆网络（Memory Network，MN），装备外部记忆的神经网络。 外部记忆将参数与记忆容量分离，在少量增加参数的条件下可以大幅增加网络容量。因此可以将注意力机制看作一个接口，将信息的存储与计算分离。 ","date":"2021-02-26","objectID":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/:5:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第8章 - 注意力机制和外部记忆","uri":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/"},{"categories":["Deep Learning"],"content":"8.5.1 端到端记忆网络 端到端记忆网络（End-To-End Memory Network，MemN2N）[Sukhbaatar et al., 2015] ：可微网络结构，可以多次从外部记忆中读取信息（只读）。 主网络根据输入 𝒙 生成 𝒒，并使用键值对注意力机制来从外部记忆中读取相关信息 𝒓， $$ \\boldsymbol{r}=\\sum_{n=1}^{N} \\operatorname{softmax}\\left(\\boldsymbol{a}_{n}^{\\top} \\boldsymbol{q}\\right) \\boldsymbol{c}_{n} $$ 并产生输出： $$ y=f(q+r) $$ 多跳操作：主网络与外部记忆进行多轮交互，根据上次读取信息继续查询读取。 ","date":"2021-02-26","objectID":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/:5:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第8章 - 注意力机制和外部记忆","uri":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/"},{"categories":["Deep Learning"],"content":"8.5.2 神经图灵机 神经图灵机（Neural Turing Machine，NTM）：由控制器和外部记忆构成。 外部记忆：矩阵$M \\in \\mathbb{R}^{D \\times N}$ 控制器：前馈或循环神经网络 寻址：基于位置、基于内容 基于内容： 读向量（read vector）$r_t$ 删除向量（erase vector）$e_t$ 增加向量（add vector）$a_t$ $$ \\boldsymbol{m}_{t+1, n}=\\boldsymbol{m}_{t, n}\\left(1-\\alpha_{t, n} \\boldsymbol{e}_{t}\\right)+\\alpha_{t, n} \\boldsymbol{a}_{t}, \\quad \\forall n \\in[1, N] $$ ","date":"2021-02-26","objectID":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/:5:2","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第8章 - 注意力机制和外部记忆","uri":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/"},{"categories":["Deep Learning"],"content":"8.6 基于神经动力学的联想记忆 将基于神经动力学（Neurodynamics）的联想记忆模型引入到神经网络以增加网络容量。联想记忆模型可以利用神经动力学原理实现按内容寻址的信息存储和检索。 联想记忆模型（Associative Memory Model）主要是通过神经网络的动态演化来进行联想，有两种应用场景： 自联想模型（Auto-Associative Model）/自编码器（Auto-Encoder，AE）：输入和输出模式在同一空间。 异联想模型（Hetero-Associative Model）：输入输出模式不在同一空间 ","date":"2021-02-26","objectID":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/:6:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第8章 - 注意力机制和外部记忆","uri":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/"},{"categories":["Deep Learning"],"content":"8.6.1 Hopfield 网络 Hopfield 网络（Hopfield Network）：一种RNN模型，由一组相互连接的神经元构成。所有神经元连接不分层。 下面讨论离散 Hopfield 网络，神经元状态为 {+1, −1} 两种，还有连续 Hopfield 网络，即神经元状态为连续值。 第i个神经元更新规则： $$ S_{i}=\\left{\\begin{array}{ll} +1 \u0026 \\text { if } \\sum_{j=1}^{M} w_{i j} S_{j}+b_{i} \\geq 0 \\\\ -1 \u0026 \\text { otherwise } \\end{array}\\right. $$ 连接权重 $w_{ij}$ 有以下性质： $$ \\begin{array}{ll} w_{i i} \u0026 =0 \\quad \\forall i \\in[1, M] \\\\ w_{i j} \u0026 =w_{j i} \\quad \\forall i, j \\in[1, M] \\end{array} $$ 更新方式： 异步：每次随机或者按顺序更新一个神经元 同步：一次更新所有神经元，需要同步时钟 能量函数 Hopfield 网络的能量函数（Energy Function）： $$ \\begin{aligned} E \u0026=-\\frac{1}{2} \\sum_{i, j} w_{i j} s_{i} s_{j}-\\sum_{i} b_{i} s_{i} \\\\ \u0026=-\\frac{1}{2} \\boldsymbol{s}^{\\top} \\boldsymbol{W} \\boldsymbol{s}-\\boldsymbol{b}^{\\top} \\boldsymbol{s} \\end{aligned} $$ 吸引点（Attractor）：稳态，能量的局部最低点 联想记忆 Hopfield 网络会收敛到所处管辖区域内的吸引点，将吸引点看作网络存储中的模式（Pattern），Hopfield的检索是基于内容寻址的检索。 信息存储 信息存储是指将一组向量$x_{1}, \\cdots, x_{N}$存储在网络中的过程，存储过程主要是调整神经元之间的连接权重，因此可以看作一种学习过程。 学习规则可以是简单的平均点积： $$ w_{i j}=\\frac{1}{N} \\sum_{n=1}^{N} x_{i}^{(n)} x_{j}^{(n)} $$ 赫布规则（Hebbian Rule，或 Hebb’s Rule）：常同时激活的神经元连接加强，反之连接消失。 存储容量 对于数量为 $M$ 的互相连接的二值神经元网络，其总状态数为 $2^M$ Hopfield 网络的最大容量为 0.14𝑀，玻尔兹曼机的容量为 0.6𝑀 改进学习算法、网络结构或者引入更复杂的运算，可以有效改进联想记忆网络的容量。 ","date":"2021-02-26","objectID":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/:6:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第8章 - 注意力机制和外部记忆","uri":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/"},{"categories":["Deep Learning"],"content":"8.6.2 使用联想记忆增加网络容量 将联想记忆作为更大网络的组件，用来增加短期记忆的容量。参数可以使用Hebbian来学习，或者作为整个网络参数的一部分来学习。 ","date":"2021-02-26","objectID":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/:6:2","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第8章 - 注意力机制和外部记忆","uri":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/"},{"categories":["Deep Learning"],"content":"习题 习题 8-1 分析 LSTM 模型中隐藏层神经元数量与参数数量之间的关系． 假设输入x维度为n，隐层神经元数为m，参数数量： 三个门+cell更新：$(m+n+1)\\times m\\times 4$ 习题 8-2 分析缩放点积模型可以缓解 Softmax 函数梯度消失的原因． $$ \\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V $$ 向量点积往往很大，Softmax函数在输入值都很大大的区域会将元素差距拉得非常大，$\\hat{y}_{k}$接近1，梯度也就接近0了 Scale： $$ E\\left(q_{i} k_{i}\\right)=E q_{i} E k_{i}=0 $$ $$ \\begin{aligned} D\\left(q_{i} k_{i}\\right) \u0026=E\\left(q_{i}^{2} k_{i}^{2}\\right)-\\left(E\\left(q_{i} k_{i}\\right)\\right)^{2} \\\\ \u0026=E q_{i}^{2} E k_{i}^{2} \\\\ \u0026=\\left(D\\left(q_{i}\\right)+\\left(E q_{i}\\right)^{2}\\right)\\left(D\\left(k_{i}\\right)+\\left(E k_{i}\\right)^{2}\\right) \\\\ \u0026=\\sigma^{4}=1 \\end{aligned} $$ $$ E\\left(Q K^{T}\\right)=\\sum_{i=1}^{d_{k}} E\\left(q_{i} k_{i}\\right)=0 $$ $$ D\\left(Q K^{T}\\right)=\\sum_{i=1}^{d_{k}} D\\left(q_{i} k_{i}\\right)=d_{k} \\sigma^{4}=d_{k} $$ 点积期望为0，通过除以标准差缩放，相当于进行了标准化Standardization，控制softmax输入的方差为1，有效解决了梯度消失问题。 习题 8-3 当将自注意力模型作为一个神经层使用时，分析它和卷积层以及循环层在建模长距离依赖关系的效率和计算复杂度方面的差异． 图片来源：Why self-attention? [Tang, Gongbo, et al., 2018] 近似时间复杂度： 自注意力：$O(1)$ CNN：$O(\\log_k(n))$ RNN：$O(n)$ 来源：cs224n, 2019, lec14 习题 8-4 试设计用集合、树、栈或队列来组织外部记忆，并分析它们的差异． TODO 习题 8-5 分析端到端记忆网络和神经图灵机对外部记忆操作的异同点． 区别： 端到端记忆网络：使用键值对注意力从外部记忆读取 神经图灵机：同时对所有记忆进行不同程度的读写 习题 8-6 证明 Hopfield 网络的能量函数随时间单调递减 TODO ","date":"2021-02-26","objectID":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/:7:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第8章 - 注意力机制和外部记忆","uri":"/blog/nndl-book-ch8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%92%8C%E5%A4%96%E9%83%A8%E8%AE%B0%E5%BF%86/"},{"categories":["Deep Learning"],"content":"卷积神经网络（Convolutional Neural Network，CNN 或 ConvNet）：一种具有局部连接、权重共享等特性的深层前馈神经网络 FC处理图像问题： 参数过多 局部不变性 启发：感受野（Receptive Filed），视觉神经元只接收视网膜特定区域的信号。 CNN一般组成：卷积层 + 汇聚层 + 全连接层，交叉堆叠 CNN结构特性：局部连接、权重共享、汇聚 一定程度的平移、缩放和旋转不变性（？） ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:0:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.1 卷积 ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:1:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.1.1 卷积的定义 一维卷积 长度K的滤波器与信号序列$x_1,x_2,…$的卷积： $$ y_{t}=\\sum_{k=1}^{K} w_{k} x_{t-k+1} $$ 滤波器（Filter）或卷积核（Convolution Kernel）：$w_{1}, w_{2}, \\cdots$ 简化：$y=w * x$ $\\boldsymbol{w}=[1 / K, \\cdots, 1 / K]$时，卷积相当于简单移动平均 $\\boldsymbol{w}=[1,-2,1]$时，近似信号序列的二阶微分 $$ x^{\\prime \\prime}(t)=x(t+1)+x(t-1)-2 x(t) $$ 上图卷积分别检测信号序列的低频和高频信息 二维卷积 给定图像 $\\boldsymbol{X} \\in \\mathbb{R}^{M \\times N}$ 和一个滤波器 $\\boldsymbol{W} \\in \\mathbb{R}^{U \\times V}$，卷积为： $$ y_{i j}=\\sum_{u=1}^{U} \\sum_{v=1}^{V} w_{u v} x_{i-u+1, j-v+1} $$ 简化：$\\boldsymbol{Y}=\\boldsymbol{W} * \\boldsymbol{X}$ 均值滤波（Mean Filter）即是一种二维卷积 第一个为高斯滤波器，后俩用来提取边缘特征 ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:1:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.1.2 互相关 互相关（Cross-Correlation），衡量两个序列相关性的函数，也称为不翻转卷积（相比卷积，卷积核不同翻转）： $$ y_{i j}=\\sum_{u=1}^{U} \\sum_{v=1}^{V} w_{u v} x_{i+u-1, j+v-1} $$ 即： $$ \\begin{aligned} \\boldsymbol{Y} \u0026=\\boldsymbol{W} \\otimes \\boldsymbol{X} \\\\ \u0026=\\operatorname{rot} 180(\\boldsymbol{W}) * \\boldsymbol{X} \\end{aligned} $$ 卷积核可学习时，卷积和互相关能力等价，为了实现方便，一般用互相关代替卷积。 ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:1:2","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.1.3 卷积的变种 步长（Stride）：卷积核滑动的时间间隔 零填充（Zero Padding）：输入向量两端补零 设卷积大小为K，常用卷积（步长S均为1）： 窄卷积（Narrow Convolution）：P=1 宽卷积（Wide Convolution）：P=K-1 等宽卷积（Equal-Width Convolution）：P=(K-1)/2 ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:1:3","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.1.4 卷积的数学性质 交换性 不限制长度的两个卷积信号的卷积具有交换性： $$y=y * x$$ 固定长度信息和卷积核的宽卷积也具有交换性（？证明）： $$\\operatorname{rot} 180(\\boldsymbol{W}) \\tilde{\\otimes} \\boldsymbol{X}=\\operatorname{rot} 180(\\boldsymbol{X}) \\tilde{\\otimes} \\boldsymbol{W}$$ 导数 假设 $\\boldsymbol{Y}=\\boldsymbol{W} \\otimes \\boldsymbol{X}$，其中 $\\boldsymbol{X} \\in \\mathbb{R}^{M \\times N}, \\boldsymbol{W} \\in \\mathbb{R}^{U \\times V}, \\boldsymbol{Y} \\in \\mathbb{R}^{(M-U+1) \\times(N-V+1)}$，函数 $f(\\boldsymbol{Y}) \\in \\mathbb{R}$ 为一个标量函数, 则： $$ \\begin{aligned} \\frac{\\partial f(\\boldsymbol{Y})}{\\partial w_{u v}} \u0026=\\sum_{i=1}^{M-U+1 N-V+1} \\frac{\\partial y_{i j}}{\\partial w_{u v}} \\frac{\\partial f(\\boldsymbol{Y})}{\\partial y_{i j}} \\\\ \u0026=\\sum_{i=1}^{M-U+1} \\sum_{j=1}^{N-V+1} x_{i+u-1, j+v-1} \\frac{\\partial f(\\boldsymbol{Y})}{\\partial y_{i j}} \\\\ \u0026=\\sum_{i=1}^{M-U+1} \\sum_{j=1}^{N-V+1} \\frac{\\partial f(\\boldsymbol{Y})}{\\partial y_{i j}} x_{u+i-1, v+j-1} \\end{aligned} $$ 即$f(Y)$关于$W$的偏导数为$X$和$\\frac{\\partial f(\\boldsymbol{Y})}{\\partial \\boldsymbol{Y}}$的卷积（互相关）： $$ \\frac{\\partial f(\\boldsymbol{Y})}{\\partial \\boldsymbol{W}}=\\frac{\\partial f(\\boldsymbol{Y})}{\\partial \\boldsymbol{Y}} \\otimes \\boldsymbol{X} $$ 同理： $$ \\begin{aligned} \\frac{\\partial f(\\boldsymbol{Y})}{\\partial x_{s t}} \u0026=\\sum_{i=1}^{M-U+1} \\sum_{j=1}^{N-V+1} \\frac{\\partial y_{i j}}{\\partial x_{s t}} \\frac{\\partial f(\\boldsymbol{Y})}{\\partial y_{i j}} \\\\ \u0026=\\sum_{i=1}^{M-U+1 N-V+1} \\sum_{j=1}^{M} w_{S-i+1, t-j+1} \\frac{\\partial f(\\boldsymbol{Y})}{\\partial y_{i j}} \\end{aligned} $$ 即$f(Y)$关于$X$的偏导数为$W$和$\\frac{\\partial f(\\boldsymbol{Y})}{\\partial \\boldsymbol{Y}}$的宽卷积（互相关）： $$ \\begin{aligned} \\frac{\\partial f(\\boldsymbol{Y})}{\\partial \\boldsymbol{X}} \u0026=\\operatorname{rot} 180\\left(\\frac{\\partial f(\\boldsymbol{Y})}{\\partial \\boldsymbol{Y}}\\right) \\tilde{\\otimes} \\boldsymbol{W} \\\\ \u0026=\\operatorname{rot} 180(\\boldsymbol{W}) \\tilde{\\otimes} \\frac{\\partial f(\\boldsymbol{Y})}{\\partial \\boldsymbol{Y}} \\end{aligned} $$ ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:1:4","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.2 卷积神经网络 ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:2:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.2.1 卷积代替全连接 $$ \\boldsymbol{z}^{(l)}=\\boldsymbol{w}^{(l)} \\otimes \\boldsymbol{a}^{(l-1)}+b^{(l)} $$ 卷积层性质： 局部连接 权重共享：可以理解为一个卷积核捕捉一种局部特征 卷积层参数只有K维权重$w^{(l)}$和一维偏置$b^{(l)}$ 神经元数量满足（默认步长1，无零填充）： $$ \\text { } M_{l}=M_{l-1}-K+1 $$ ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:2:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.2.2 卷积层 特征映射（Feature Map）：一幅图像（或其他特征映射）在经过卷积提取到的特征，每个特征映射可以作为一类抽取的图像特征 $$ X \\in \\mathbb{R}^{M \\times N \\times D} $$ $$ y \\in \\mathbb{R}^{M^{\\prime} \\times N^{\\prime} \\times P} $$ $$ \\mathcal{W} \\in \\mathbb{R}^{U \\times V \\times P \\times D} $$ $$ \\begin{array}{l} \\boldsymbol{Z}^{p}=\\boldsymbol{W}^{p} \\otimes \\boldsymbol{X}+b^{p}=\\sum_{d=1}^{D} \\boldsymbol{W}^{p, d} \\otimes \\boldsymbol{X}^{d}+b^{p} \\\\ \\boldsymbol{Y}^{p}=f\\left(\\boldsymbol{Z}^{p}\\right) \\end{array} $$ 参数个数：$P \\times D \\times(U \\times V)+P$ ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:2:2","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.2.3 汇聚层 汇聚层（Pooling Layer）也叫子采样层（Subsampling Layer）：促进特征选择，降低特征/参数数量，避免过拟合 汇聚（Pooling）是指对每个区域进行下采样（Down Sampling）得到一个值，作为这个区域的概括 常用汇聚函数： 最大汇聚（Maximum Pooling/ Max Pooling）：$y_{m, n}^{d}=\\max _{i \\in R_{m, n}^{d}} x_{i}$ 平均汇聚（Mean Pooling）：$y_{m, n}^{d}=\\frac{1}{\\left|R_{m, n}^{d}\\right|} \\sum_{i \\in R_{m, n}^{d}} x_{i}$ 汇聚层可以看作特殊的卷积层：卷积核大小为$K\\times K$，步长为$S\\times S$，卷积核为max函数或mean函数 ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:2:3","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.2.4 卷积网络的整体结构 一个典型的卷积网络是由卷积层、汇聚层、全连接层交叉堆叠而成 （N为1~100或更大，K一般为0~2） 目前，趋向于使用小卷积核、深结构、少汇聚层的全卷积网络 ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:2:4","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.3 参数学习 CNN参数只有卷积核和偏置，对于： $$ \\boldsymbol{Z}^{(l, p)}=\\sum_{d=1}^{D} \\boldsymbol{W}^{(l, p, d)} \\otimes \\boldsymbol{X}^{(l-1, d)}+b^{(l, p)} $$ 偏导： $$ \\begin{aligned} \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{W}^{(l, p, d)}} \u0026=\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{Z}^{(l, p)}} \\otimes \\boldsymbol{X}^{(l-1, d)} \\\\ \u0026=\\delta^{(l, p)} \\otimes \\boldsymbol{X}^{(l-1, d)} \\end{aligned} $$ $$ \\frac{\\partial \\mathcal{L}}{\\partial b^{(l, p)}}=\\sum_{i, j}\\left[\\delta^{(l, p)}\\right]_{i, j} $$ ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.3.1 卷积神经网络的反向传播算法 汇聚层 上采样与l层特征映射的激活值偏导数逐元素相乘： $$ \\begin{aligned} \\delta^{(l, p)} \u0026 \\triangleq \\frac{\\partial \\mathcal{L}}{\\partial Z^{(l, p)}} \\\\ \u0026=\\frac{\\partial X^{(l, p)}}{\\partial Z^{(l, p)}} \\frac{\\partial Z^{(l+1, p)}}{\\partial \\boldsymbol{X}^{(l, p)}} \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{Z}^{(l+1, p)}} \\\\ \u0026=f_{l}^{\\prime}\\left(\\boldsymbol{Z}^{(l, p)}\\right) \\odot \\operatorname{up}\\left(\\delta^{(l+1, p)}\\right) \\end{aligned} $$ 卷积层 $$ \\begin{aligned} \\delta^{(l, p)} \u0026 \\triangleq \\frac{\\partial \\mathcal{L}}{\\partial Z^{(l, p)}} \\\\ \u0026=\\frac{\\partial X^{(l, p)}}{\\partial Z^{(l, p)}} \\frac{\\partial Z^{(l+1, p)}}{\\partial \\boldsymbol{X}^{(l, p)}} \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{Z}^{(l+1, p)}} \\\\ \u0026=f_{l}^{\\prime}\\left(\\boldsymbol{Z}^{(l, p)}\\right) \\odot \\operatorname{up}\\left(\\delta^{(l+1, p)}\\right) \\end{aligned} $$ ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.4 几种典型的CNN ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:4:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.4.1 LeNet-5 LeNet-5[LeCun et al., 1998] ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:4:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.4.2 AlexNet AlexNet[Krizhevsky et al., 2012]是第一个现代深度的CNN 2012 年ImageNet 图像分类冠军 ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:4:2","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.4.3 Inception网络 Inception 网络是由有多个 Inception 模块和少量的汇聚层堆叠而成 Inception v1：即GoogLeNet[Szegedy et al., 2015] 2014 年 ImageNet 图像分类冠军 Inception v3 网络：用多层的小卷积核来替换大的卷积核 ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:4:3","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"残差网络 残差网络（Residual Network，ResNet）：通过给非线性的卷积层增加直连边（Shortcut Connection）（也称为残差连接 Residual Connection））的方式来提高信息的传播效率 将目标函数拆分为两部分：恒等函数（Identity Function）和残差函数（Residue Function）： $$ h(\\boldsymbol{x})=\\underbrace{\\boldsymbol{x}}_{\\text {恒等函数 }}+\\underbrace{(h(\\boldsymbol{x})-\\boldsymbol{x})}_{\\text {残差函数 }} $$ ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:4:4","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.5 其他卷积方式 ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:5:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.5.1 转置卷积 $$ \\begin{aligned} z \u0026=w \\otimes x \\\\ \u0026=\\left[\\begin{array}{lllll} w_{1} \u0026 w_{2} \u0026 w_{3} \u0026 0 \u0026 0 \\\\ 0 \u0026 w_{1} \u0026 w_{2} \u0026 w_{3} \u0026 0 \\\\ 0 \u0026 0 \u0026 w_{1} \u0026 w_{2} \u0026 w_{3} \\end{array}\\right] \\boldsymbol{x} \\\\ \u0026=\\boldsymbol{C} \\boldsymbol{x} \\end{aligned} $$ 反过来，仿射矩阵的转置： $$ \\begin{aligned} \\boldsymbol{x} \u0026=\\boldsymbol{C}^{\\top} \\boldsymbol{z} \\\\ \u0026=\\left[\\begin{array}{lll} w_{1} \u0026 0 \u0026 0 \\\\ w_{2} \u0026 w_{1} \u0026 0 \\\\ w_{3} \u0026 w_{2} \u0026 w_{1} \\\\ 0 \u0026 w_{3} \u0026 w_{2} \\\\ 0 \u0026 0 \u0026 w_{3} \\end{array}\\right] \\boldsymbol{z} \\\\ \u0026=\\operatorname{rot} 180(\\boldsymbol{w}) \\tilde{\\otimes} z \\end{aligned} $$ 我们将这种低维特征映射到高维特征的卷积操作成为转置卷积（Transposed Convolution），也称为反卷积（Deconvolution） 卷积层的前向计算和后向传播也是一种转置关系 微步卷积：步长 $S\\lt 1$ 的转置卷积 ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:5:1","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"5.5.2 空洞卷积 空洞卷积（Atrous Convolution）：不增加参数，增加了输出单元感受野，也成为膨胀（Dilated Convolution） 空洞卷积中选择D为半径的圆上的点，感受野是不是更加合理？考虑图像旋转后，方形的点位捕捉特征能力可能受到影响。 进而，用圆形的卷积效果是否会更好？运算速度呢？ ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:5:2","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"习题 习题 5-1 1）证明公式 (5.6) 可以近似为离散信号序列 𝑥(𝑡) 关于 𝑡 的二阶微分；2）对于二维卷积，设计一种滤波器来近似实现对二维输入信号的二阶微分． 由导数定义：$x’(t)=x(t)-x(t-1)$，进一步求二阶导数即可 Laplace operator $$ \\nabla^{2} f=\\frac{\\partial^{2} f}{\\partial x^{2}}+\\frac{\\partial^{2} f}{\\partial y^{2}} $$ $$ \\frac{\\partial^{2} f}{\\partial x^{2}}=f(x+1, y)+f(x-1, y)-2 f(x, y) $$ $$ \\frac{\\partial^{2} f}{\\partial y^{2}}=f(x, y+1)+f(x, y-1)-2 f(x, y) $$ $$ \\nabla^{2} f(x, y)=f(x+1, y)+f(x-1, y)+f(x, y+1)+f(x, y-1)-4 f(x, y) $$ 所以： $$ \\mathrm{L} 0=\\left[\\begin{array}{ccc} 0 \u0026 -1 \u0026 0 \\\\ -1 \u0026 4 \u0026 -1 \\\\ 0 \u0026 -1 \u0026 0 \\end{array}\\right] $$ 习题 5-2 证明宽卷积具有交换性，即公式 (5.13)． 抛开padding，只看有效的重叠部分，因为深度D相同，只考虑平面上观察：本质都是就是遍历了两个方形的所有重合摆放方式，将等式一边的图像均翻转180度，可以想象其遍历顺序完全相同。 习题 5-3 分析卷积神经网络中用 1 × 1 的卷积核的作用． 1x1卷积核，又称为网中网（Network in Network），对于三维输入时是一个正方形长条，可以用来升维/降维、接一个ReLU增加非线性、channal变换/通道信息交互 习题5-4 对于一个输入为100 × 100 × 256的特征映射组，使用3 × 3的卷积核，输出为 100 × 100 × 256 的特征映射组的卷积层，求其时间和空间复杂度．如果引入一个 1 × 1 卷积核，先得到 100 × 100 × 64 的特征映射，再进行 3 × 3 的卷积，得到100 × 100 × 256 的特征映射组，求其时间和空间复杂度． 直接卷积 时间：$100\\times 100\\times 256\\times 256\\times 3\\times 3$ 空间：$100\\times 100\\times 256$ 参数：$(3\\times 3 + 1)\\times 256 \\times 256$ 先1x1卷积： 时间：$100\\times 100\\times 256\\times 64\\times 1\\times 1+100\\times 64\\times 256\\times 3 \\times 3$ 空间：$100\\times 100\\times 256+100\\times 100\\times 64$ 参数：$(1\\times 1+1)\\times 256\\times 64+(3\\times 3 + 1)\\times 64 \\times 256$ 习题 5-5 对于一个二维卷积，输入为 3 × 3，卷积核大小为 2 × 2，试将卷积操作重写为仿射变换的形式． 参见公式(5.45)． $$ \\left[\\begin{array}{lcccccccc} w_{11} \u0026 w_{12} \u0026 0 \u0026 w_{21} \u0026 w_{22} \u0026 0 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 w_{11} \u0026 w_{12} \u0026 0 \u0026 w_{21} \u0026 w_{22} \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 w_{11} \u0026 w_{12} \u0026 0 \u0026 w_{21} \u0026 w_{22} \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 0 \u0026 w_{11} \u0026 w_{12} \u0026 0 \u0026 w_{21} \u0026 w_{22} \\end{array}\\right] $$ 习题 5-6 计算函数 𝑦 = max(𝑥1, ⋯ , 𝑥𝐷) 和函数 𝑦 = arg max(𝑥1, ⋯ , 𝑥𝐷) 的梯度． TODO 习题 5-7 忽略激活函数，分析卷积网络中卷积层的前向计算和反向传播（公式(5.39)）是一种转置关系． 前向： $$ z^{(l+1)}=W^{(l+1)} z^{(l)} $$ 反向： $$ \\delta^{(l)}=\\left(W^{(l+1)}\\right)^{\\top} \\delta^{(l+1)} $$ 习题5-8 在空洞卷积中，当卷积核大小为𝐾，膨胀率为𝐷 时，如何设置零填充𝑃 的值以使得卷积为等宽卷积． $$P=(K-1)/2\\times(1+D)$$ ","date":"2021-02-15","objectID":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:6:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第5章 - 卷积神经网络","uri":"/blog/nndl-book-ch5-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["NLP"],"content":"一篇非常简洁的神经网络应用于NLP的综述（2016）。 ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:0:0","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"1. Introduction purpose 过去近十年: NLP通常使用线性机器学习模型（SVM、LR）在高维稀疏特征向量上训练 recent：神经网络（非线性）运用在稠密向量 primer for beginners advanced: Deep Learning (Bengio, Goodfellow, and Courville) ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:1:0","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"1.1 Scope OUT of Scope: vast literature of language modeling acoustic modeling neural networks for machine translation multi-modal applications combining language other signals such as images and videos Caching methods methods for efficient training with large output vocabularies attention models autoencoders and recursive autoencoders etc. ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:1:1","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"1.2 A Note on Terminology feature: a concrete, linguistic input such as a word, a suffix, or a part-of-speech tag input vector: actual input that is fed to the neural-network classifier input vector entry: a specific value of the input ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:1:2","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"1.3 Mathematical Notation matrices: bold upper $(\\mathbf{X}, \\mathbf{Y}, \\mathbf{Z})$ vectors: bold lower $(\\mathbf{b})$, row vec default thus, right multiplied by mat: $(\\mathbf{x} \\mathbf{W}+\\mathbf{b})$ series of mat/vec: superscript indices $\\left(\\mathbf{W}^{1}, \\mathbf{W}^{2}\\right)$ exponentiated: with brackets $(\\mathbf{W})^{2},\\left(\\mathbf{W}^{3}\\right)^{2}$ vector concatenation: $\\left[\\mathbf{v}_{\\mathbf{1}} ; \\mathbf{v}_{\\mathbf{2}}\\right]$ ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:1:3","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"2. Neural Netword Architectures discuss two kinds feed-forward networks recurrent / recursive networks feed-forward networks include: fully connected layers, eg. MLP nn with convolutional and pooling layers All of the networks act as classifiers Fully connected feed-forward neural networks (Section 4) non-linear learners 大多可以直接替代线性分类器 很容易加入预训练词嵌入模型 Networks with convolutional and pooling layers (Section 9) expect to find strong local clues regarding class membership(ie. local indicators regardless of their position) promising results on many tasks… facing structured data of arbitrary sizes, such as sequences and trees: can encode by sacrificing most of the structural information :( Recurrent and recursive architectures work well with sequences and trees while preserving a lot of the structural information. recurrent networks: model sequence recursive networks: generalizations of recurrent networks, model trees an extension of recurrent networks: model stacks ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:2:0","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"3. Feature Representation think of a feed-forward nn as function $\\mathrm{NN}(\\mathbf{x})$，input $d_{in}$ vec $\\mathbf{x}$，output $d_{out}$ vec. the input $\\mathbf{x}$ encodes features such as words, part-of-speech tags or other linguistic information each core feature is embedded as a vector into a $d$ dimensional space embeddings: the vector representation of each core feature, can be trained like the other parameter general structure for NLP classification system based on a feed-forward NN: Extract core features $f_{1}, \\ldots, f_{k}$ $f_i \\rightarrow v(f_i)$ Different feature types may be embedded into different spaces. $v(f_1), \\ldots, v(f_k) \\rightarrow \\mathbf{x}$ by concatenation, summation or a combination of both Feed $\\mathbf{x}$ to NN ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:3:0","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"3.1 Dense Vectors vs. One-Hot Representations One Hot: Each feature is its own dimension. Dimensionality of one-hot vector is same as number of distinct features. Features are completely independent from one another. Dense: Each feature is a d-dimensional vector. Dimensionality of vector is $d$. Model training will cause similar features to have similar vectors – information is shared between similar features. Benifit of Dense: generalization power share statistical strength between similar features 如果特征比较少而训练数据大、特征相互没有联系，或者不希望不同特征共享统计信息，可以采用简单的one-hot。而大多数论文主张对所有特征采用稠密、可以训练的特征向量 使用one-hot和dense表示在NN最终结果上差异不大，使用one-hot输入总是会使得first layer学习dense的嵌入表示 ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:3:1","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"3.2 Variable Number of Features: Continuous Bag of Words continuous bag of words (CBOW) : represent an unbounded number of features using a fixed size vector $$\\operatorname{CBOW}\\left(f_{1}, \\ldots, f_{k}\\right)=\\frac{1}{k} \\sum_{i=1}^{k} v\\left(f_{i}\\right)$$ weighted CBOW: $$\\operatorname{WCBOW}\\left(f_{1}, \\ldots, f_{k}\\right)=\\frac{1}{\\sum_{i=1}^{k} a_{i}} \\sum_{i=1}^{k} a_{i} v\\left(f_{i}\\right)$$ eg. document classification: $f_i$ may correspond to a word, associated weight $a_i$ counld be the word’s TF-IDF score. if $v(f_i)$ were one-hot, CBOW and WCBOW become traditional(weighted) bag-of-words. ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:3:2","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"3.3 Distance and Position Features distance features are encoded similarily to the other feature types (?) ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:3:3","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"3.4 Feature Combinations combination features are crucial in linear models, 因为提供了更多维度以线性区分 feature designer’s dirty work :( non-linear neural network only needs the core features network will find feature combinations complexity scales linearly with network size Kernel methods, in particulr polynomial kernels also allow only core features vs. NN, kernel methods are convex, admiting exact solutions high complexity, which scales linearly with data size, too slow for practice and large datasets. ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:3:4","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"3.5 Dimensionality How many dimensions should we allocate for each feature? no theoretical bounds or even best-practices In current research, the dimensionality of word-embedding vectors range between about 50 to a few hundreds, and, in some extreme cases, thousands. experiment and trade-off between speed and accuracy ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:3:5","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"3.6 Vector Sharing Should the vector for “dog:previous-word” be the same as the vector of “dog:next-word”? empirical question ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:3:6","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"3.7 Network’s Output scalar scores to items in a discrete set, same as traditional linear models there is $d \\times k$ matrix associated with the output layer, 列是每一类的嵌入表示，列向量的相似度代表类相似度 ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:3:7","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"3.8 Historical Node dense word vector for NN: Bengio et al. (2003) introduced to NLP: Collobert, Weston and colleagues (2008, 2011) embeddings for represent not only words but arbitrary features: Chen and Manning (2014) ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:3:8","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"4. Feed-Forward Neural Networks ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:4:0","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"4.1 A Brain-Inspired Metaphor sigmoid shape: non-linear function fully-connected layer / affine layer: each neuron is connected to all neurons in the next layer each row -\u003e a vector $\\mathbf{y}=\\left(g\\left(\\mathbf{x} \\mathbf{W}^{\\mathbf{1}}\\right)\\right) \\mathbf{W}^{\\mathbf{2}}$ ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:4:1","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"4.2 In Mathematical Notation perceptron: linear function of input $$\\text { NN }_{\\text {Perceptron }}(\\mathbf{x})=\\mathbf{x} \\mathbf{W}+\\mathbf{b} $$ $$ \\mathbf{x} \\in \\mathbb{R}^{d_{\\text {in }}}, \\mathbf{W} \\in \\mathbb{R}^{d_{\\text {in }} \\times d_{\\text {out }}}, \\quad \\mathbf{b} \\in \\mathbb{R}^{d_{\\text {out }}}$$ Multi Layer Perceptron with one hidden-layer (MLP1) $$\\mathrm{NN}_{\\mathrm{MLP} 1}(\\mathrm{x})=g\\left(\\mathrm{x} \\mathbf{W}^{1}+\\mathrm{b}^{1}\\right) \\mathbf{W}^{2}+\\mathbf{b}^{2}$$ $$\\mathbf{x} \\in \\mathbb{R}^{d_{\\text {in }}}, \\quad \\mathbf{W}^{1} \\in \\mathbb{R}^{d_{\\text {in }} \\times d_{1}}, \\quad \\mathbf{b}^{\\mathbf{1}} \\in \\mathbb{R}^{d_{1}}, \\quad \\mathbf{W}^{2} \\in \\mathbb{R}^{d_{1} \\times d_{2}}, \\quad \\mathbf{b}^{2} \\in \\mathbb{R}^{d_{2}}$$ $g$: non-linearity / activation function, applied element-wise MLP with two hidden-layers $$\\mathrm{NN}_{\\mathrm{MLP} 2}(\\mathrm{x})=\\left(g^{2}\\left(g^{1}\\left(\\mathrm{x} \\mathbf{W}^{1}+\\mathrm{b}^{1}\\right) \\mathbf{W}^{2}+\\mathrm{b}^{2}\\right)\\right) \\mathbf{W}^{3}$$ or: $$\\begin{aligned} \\mathrm{NN}_{\\mathrm{MLP} 2}(\\mathrm{x}) \u0026=\\mathrm{y} \\\\ \\mathrm{h}^{1} \u0026=g^{1}\\left(\\mathrm{x} \\mathbf{W}^{1}+\\mathrm{b}^{1}\\right) \\\\ \\mathrm{h}^{2} \u0026=g^{2}\\left(\\mathbf{h}^{1} \\mathbf{W}^{2}+\\mathbf{b}^{2}\\right) \\\\ \\mathbf{y} \u0026=\\mathbf{h}^{2} \\mathbf{W}^{3} \\end{aligned}$$ deep networks: Networks with several hidden layers $\\theta$: collection of all parameters ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:4:2","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"4.3 Representation Power MLP1 is universal approximator: approximate all continuous functions (on a closed and bounded subset of $\\mathbb{R}^{n}$) and discrete function (mapping from finite dimensional discrete space to another) However, DEEP is needed for learnability MLP1 do not guarantee for finding correct function further discussion: Bengio et al. (2015, Section 6.5) ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:4:3","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"4.4 Common Non-linearities currently no good theory as to which non-linearity to apply in which condition 4.4.1 Sigmoid $$ \\sigma(x)=1 /\\left(1+e^{-x}\\right) $$ also called logistic function currently considered to be deprecated for use in internal layers of neural networks 4.4.2 Hyperbolic Tangent (tanh) $$ \\tanh (x)=\\frac{e^{2 x}-1}{e^{2 x}+1} $$ 4.4.3 Hard tanh $$ \\operatorname{hardtanh}(x)=\\left{\\begin{array}{ll} -1 \u0026 x\u003c-1 \\\\ 1 \u0026 x\u003e1 \\\\ x \u0026 \\text { otherwise } \\end{array}\\right. $$ an approximation of the tanh function faster to compute and take derivatives of 4.4.4 Rectifier (ReLU) The Rectifier activation function $$ \\operatorname{ReLU}(x)=\\max (0, x)=\\left{\\begin{array}{ll} 0 \u0026 x\u003c0 \\\\ x \u0026 \\text { otherwise } \\end{array}\\right. $$ performs well for many tasks, especially when combined with the dropout regularization technique As a rule of thumb, ReLU units work better than tanh, and tanh works better than sigmoid. ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:4:4","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"4.5 Output Transformations softmax: $$ \\begin{aligned} \\mathbf{x} \u0026=x_{1}, \\ldots, x_{k} \\\\ \\operatorname{softmax}\\left(x_{i}\\right) \u0026=\\frac{e^{x_{i}}}{\\sum_{j=1}^{k} e^{x_{j}}} \\end{aligned} $$ used when modeling a probability distribution over the possible output classes To be effective, it should be used in conjunction with a probabilistic training objective such as cross-entropy 如在不含hidden layer的network上使用softmax，即multinomial logistic regression model，也即maximum-entropy classifier ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:4:5","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"4.6 Embedding Layers $c(\\cdot)$: core features -\u003e input vector concatenate: $$ \\begin{aligned} \\mathbf{x}=c\\left(f_{1}, f_{2}, f_{3}\\right) \u0026=\\left[v\\left(f_{1}\\right) ; v\\left(f_{2}\\right) ; v\\left(f_{3}\\right)\\right] \\\\ \\mathrm{NN}_{\\mathrm{MLP} 1}(\\mathbf{x}) \u0026=\\mathrm{NN}_{\\mathrm{MLP} 1}\\left(c\\left(f_{1}, f_{2}, f_{3}\\right)\\right) \\\\ \u0026=\\mathrm{N} \\mathrm{N}_{\\mathrm{MLP} 1}\\left(\\left[v\\left(f_{1}\\right) ; v\\left(f_{2}\\right) ; v\\left(f_{3}\\right)\\right]\\right) \\\\ \u0026=\\left(g\\left(\\left[v\\left(f_{1}\\right) ; v\\left(f_{2}\\right) ; v\\left(f_{3}\\right)\\right] \\mathbf{W}^{1}+\\mathbf{b}^{1}\\right)\\right) \\mathbf{W}^{2}+\\mathbf{b}^{2} \\end{aligned} $$ sum: $$ \\begin{aligned} \\mathbf{x}=c\\left(f_{1}, f_{2}, f_{3}\\right) \u0026=v\\left(f_{1}\\right)+v\\left(f_{2}\\right)+v\\left(f_{3}\\right) \\\\ \\mathrm{NN}_{\\mathrm{MLP} 1}(\\mathbf{x}) \u0026=\\mathrm{NN}_{\\mathrm{MLP} 1}\\left(c\\left(f_{1}, f_{2}, f_{3}\\right)\\right) \\\\ \u0026=\\mathrm{NN}_{\\mathrm{MLP} 1}\\left(v\\left(f_{1}\\right)+v\\left(f_{2}\\right)+v\\left(f_{3}\\right)\\right) \\\\ \u0026=\\left(g\\left(\\left(v\\left(f_{1}\\right)+v\\left(f_{2}\\right)+v\\left(f_{3}\\right)\\right) \\mathbf{W}^{1}+\\mathbf{b}^{1}\\right)\\right) \\mathbf{W}^{2}+\\mathbf{b}^{2} \\end{aligned} $$ embedding layer / lookup layer: $c$ $$ v\\left(f_{i}\\right)=\\mathbf{f}_{\\mathbf{i}} \\mathbf{E} $$ $$ \\operatorname{CBOW}\\left(f_{1}, \\ldots, f_{k}\\right)=\\sum_{i=1}^{k}\\left(\\mathbf{f}_{\\mathbf{i}} \\mathbf{E}\\right)=\\left(\\sum_{i=1}^{k} \\mathbf{f}_{\\mathbf{i}}\\right) \\mathbf{E} $$ 4.6.1 A Note on Notation $$ ([\\mathbf{x} ; \\mathbf{y} ; \\mathbf{z}] \\mathbf{W}+\\mathbf{b}) $$ is same as affine transformation: $$ (\\mathrm{x} \\mathbf{U}+\\mathbf{y} \\mathbf{V}+\\mathbf{z} \\mathbf{W}+\\mathbf{b}) $$ 4.6.2 A Note on Sparse vs. Dense Features “traditional” sparse representation for its input vectors, input: $$ \\mathbf{x}=\\sum_{i=1}^{k} \\mathbf{f}_{\\mathbf{i}} \\quad \\mathbf{x} \\in \\mathbb{N}_{+}^{|V|} $$ first layers: $$ \\begin{array}{l} \\mathrm{xW}+\\mathrm{b}=\\left(\\sum_{i=1}^{k} \\mathrm{f}_{\\mathrm{i}}\\right) \\mathbf{W} \\\\ \\mathbf{W} \\in \\mathbb{R}^{|V| \\times d}, \\quad \\mathbf{b} \\in \\mathbb{R}^{d} \\end{array} $$ similar to embedding layer that produces CBOW difference: the introduction of the bias vector b non-linear activation each feature receive a separate vector (row in $\\mathbf{W}$) while embedding layer can share these differences are small and subtle ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:4:6","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"4.7 Loss Functions parameters of the network: the matrices $\\mathbf{W^i}$, the biases $\\mathbf{b^i}$ and commonly the embeddings $\\mathbf{E}$ 4.7.1 Hinge (binary) / margin loss / SVM loss 合页损失（长得像合页） $$ L_{\\text {hinge(binary) }}(\\hat{y}, y)=\\max (0,1-y \\cdot \\hat{y}) $$ 4.7.2 Hinge (multiclass) $$ \\text { prediction }=\\arg \\max _{i} \\hat{y}_{i} $$ $t=\\arg \\max _{i} y_{i}$ the correct class, $k=\\arg \\max _{i \\neq t} \\hat{y}_{i}$ the highest scoring class such that $k \\neq t$, multiclass hinge loss: $$ L_{\\text {hinge(multiclass) }}(\\hat{\\mathbf{y}}, \\mathbf{y})=\\max \\left(0,1-\\left(\\hat{y}_{t}-\\hat{y}_{k}\\right)\\right) $$ Both the binary and multiclass hinge losses are intended to be used with a linear output layer The hinge losses are useful whenever we require a hard decision rule, and do not attempt to model class membership probability. 4.7.3 Log Loss “soft” version of the hinge loss with an infinite margin $$ L_{l o g}(\\hat{\\mathbf{y}}, \\mathbf{y})=\\log \\left(1+\\exp \\left(-\\left(\\hat{y}_{t}-\\hat{y}_{k}\\right)\\right)\\right. $$ 4.7.4 Categorical Cross-Entropy Loss also referred to as negative log likelihood used when a probabilistic interpretation of the scores is desired $$ \\hat{y}_{i}=P(y=i \\mid \\mathbf{x}) $$ $$ L_{\\text {cross-entropy }}(\\hat{\\mathbf{y}}, \\mathbf{y})=-\\sum_{i} y_{i} \\log \\left(\\hat{y}_{i}\\right) $$ hard classification problems: each training example has a single correct class assignment, $\\mathbf{y}$ is one-hot: $$ L_{\\text {cross-entropy }(\\text { hard classification })}(\\hat{\\mathbf{y}}, \\mathbf{y})=-\\log \\left(\\hat{y}_{t}\\right) $$ When using the cross-entropy loss, it is assumed that the network’s output is transformed using the softmax transformation 4.7.5 Ranking Losses With pairs of correct and incorrect (by corrupting a positive example) items x and x′, and our goal is to score correct items above incorrect ones. $$ L_{\\text {ranking }(\\operatorname{margin})}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)=\\max \\left(0,1-\\left(\\mathrm{NN}(\\mathbf{x})-\\mathrm{NN}\\left(\\mathbf{x}^{\\prime}\\right)\\right)\\right) $$ $$ L_{\\text {ranking }(\\log )}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)=\\log \\left(1+\\exp \\left(-\\left(\\mathrm{NN}(\\mathbf{x})-\\mathrm{NN}\\left(\\mathbf{x}^{\\prime}\\right)\\right)\\right)\\right) $$ ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:4:7","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"5. Word Embeddings embeddings: representing each feature as a vector in a low dimensional space ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:5:0","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"5.1 Random Initialization Random Initialization [Mikolov et al., 2013]: $\\left[-\\frac{1}{2 d}, \\frac{1}{2 d}\\right]$ xavier initialization (Section 6.3.1): $\\left[-\\frac{\\sqrt{6}}{\\sqrt{d}}, \\frac{\\sqrt{6}}{\\sqrt{d}}\\right]$ in practice random initialization: for commonly occurring features, eg. part-of-speech tags or individual letters supervised or unsupervised pre-training: for potentially rare features, eg. features for individual words treate pre-trained vectors as fixed or further tuned. ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:5:1","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"5.2 Supervised Task-Specific Pre-training use auxiliary task B (say, part-of-speech tagging) to pre-train word vectors for task A (eg. syntactic parsing) or train jointly (Section 7) ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:5:2","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"5.3 Unsupervised Pre-training distributional hypothesis (Harris, 1954): words are similar if they appear in similar contexts It is thus desired that the similarity between word vectors learned by the unsupervised algorithm captures the same aspects of similarity that are useful for performing the intended task of the network. unsupervised word-embedding algorithms word2vec(Mikolov et al., 2013) GloVe (Pennington, Socher, \u0026 Manning, 2014) the Collobert and Weston (2008, 2011) embeddings algorithm all above based on stochastic gradient training algorithms based on matrix factorization the choice of auxiliary problem matters much more than the learning method software packages word2vec, Gensim: word2vec model with word-windows based contexts word2vecf: allowing the use of arbitrary contexts GloVe: GloVe model have many other applications in NLP beyond initializing word-embeddings layer of NN ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:5:3","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"5.4 Training Objectives Language-modeling inspired approaches: model the conditional probability P(w|c) Mikolov et al. (2013) Mnih and Kavukcuoglu (2013) GloVe (Pennington et al., 2014) binary classification Collobert and Weston (2008, 2011) Mikolov et al. (2013, 2014) ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:5:4","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"5.5 The Choice of Contexts symmetric window rather than preivous words in original language model 5.5.1 Window Approach a sequence of 2k +1 words tow approach one task: predict the focus word based on context words (represented using CBOW [Mikolov et al., 2013] or vector concatenation [Collobert \u0026 Weston, 2008]) 2k tasks (skip-gram, often SOTA) [Mikolov et al., 2013] Effect of Window Size larger: topical similarities smaller: functional and syntatic similarities Positional Windows CBOW和skip-gram丢失了位置信息 positional contexts, eg. “the:+2” Variants 5.5.2 Sentences, Paragraphs or Documents skip-grams (or CBOW) is equivalent to using very large window sizes. capture topical similarity 5.5.3 Syntactic Window replace the linear context within a sentence with a syntactic one produce highly functional similarities 5.5.4 Multilingual using multilingual, translation based contexts 5.5.5 Character-Based and Sub-word Representations derive the vector representation of a word from the characters that compose it motivation: unknown words problem challenging, as the relationship between form (characters) and function (syntax, semantics)in language is quite loose middle-ground: vector for word + vectors of sub-word ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:5:5","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"6. Neural Network Training ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:6:0","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"6.1 Stochastic Gradient Training SGD: minibatch SGD: others: SGD + Momentum Nesterov Momentum Adaptive learning rate: AdaGrad, AdaDelta, Adam ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:6:1","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"6.2 The Computation Graph Abstraction computation graph: directed acyclic graph(DAG) example of compution: 6.2.1 Forward Computation topological order: 6.2.2 Backward Computation (Derivatives, Backprop) 误差乘以偏导再求和 6.2.3 Software implement computation-graph model: Theano, Chainer, penne, CNN/pyCNN Theano has optimizing compiler pros: run efficiently on either the CPU or a GPU cons: compilation can be costly other packages dynamic CG “on the fly”: cons: speed may suffer especially convenient with recurrnt and recursive networds 6.2.4 Implementation Recipe 6.2.5 Network Composition when networks’s output is a vector, easy to compose ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:6:2","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"6.3 Optimization Issues further: Bengio et al. (2015, ch. 8) 6.3.1 Initialization tips: When debugging, and for reproducibility of results, it is advised to used a fixed random seed. xavier initialization: $$ \\mathbf{W} \\sim U\\left[-\\frac{\\sqrt{6}}{\\sqrt{d_{\\mathrm{in}}+d_{\\mathrm{out}}}},+\\frac{\\sqrt{6}}{\\sqrt{d_{\\mathrm{in}}+d_{\\mathrm{out}}}}\\right] $$ too large or too small cause saturated internal covariate shift: use batch normalization, adjust inputs to fix the activation function (vice versa) image source He et al. (2015): ReLU initialized by sampling from a zero-mean Gaussian distribution whose standard deviation is $\\sqrt{\\frac{2}{d_{\\text {in }}}}$, works better than xavier in image classification 6.3.2 Vanishing and Exploding Gradients especially in deeper networks, recursive and recurrent networks solutions for vanishing (still open research): shallower step-wise training batch-normalization(for every minibatch, normalizing the inputs to each of the network layers to have zero mean and unit variance) specialized architectures for gradient flow (e.g. LSTM and GRU) solution for exploding: clipping the gradients if their norm exceeds a given threshold 6.3.3 Saturation and Dead Neurons Saturated: layers with tanh and sigmoid output values all close to one Dead: layers with ReLU most or all values are negative and thus clipped at zero solutions: avoid large gradient (reduce learning rate) for saturated: normalize after activation, $g(\\mathbf{h})=\\frac{\\tanh (\\mathbf{h})}{|\\tanh (\\mathbf{h})|}$ batch normalization (important in CV) 6.3.4 Shuffling shuffle the training examples before each pass 6.3.5 Learning Rate rule of thumb: try $[0,1]$, e.g. 0.001, 0.01, 0.1, 1 decrease rate once the loss stops improving Learning rate scheduling: learning rate / iter L´eon Bottou (2012): $\\eta_{t}=\\eta_{0}\\left(1+\\eta_{0} \\lambda t\\right)^{-1}$ 6.3.6 Minibatches benefit from GPUs ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:6:3","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"6.4 Regularization to alleviate overfitting add to objective function: $$ \\frac{\\lambda}{2}|\\theta|^{2} $$ dropout: 每批训练中，忽略一半（或某层中一半）特征检测器（设为0），减少特征检测器（隐层结点）之间的相互作用。 Srivastava, Nitish, et al. ”Dropout: a simple way to prevent neural networks from overfitting”, JMLR 2014 key factors in image classification, especially with ReLU also matters in NLP ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:6:4","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"7. Cascading and Multi-task Learning ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:7:0","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"7.1 Model Cascading build large networks with smaller component networks to cambat vanishing gradient and make the most of training material, bootstrap component networks’s parameters by training separately ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:7:1","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"7.2 Multi-task Learning e.g. chunking, named entity recognition (NER) and language modeling are examples of synergistic tasks ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:7:2","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"8. Structured Output Prediction structured ouput: sequence, tree, graph e.g. sequence tagging, sequence segmentation(chunking, NER) and syntactic parsing. ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:8:0","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"8.1 Greedy Structured Prediction decompose into a sequence of local prediction problems (classifier) e.g. -left-to-right tagging models (Gimenez \u0026 Marquez, 2004) greedy transition-based parsing (Nivre, 2008) cons: error propagation nonlinear NN classifier helps the easy-first approach in Goldberg \u0026 Elhadad, 2010 making training conditions more similar to testing conditions by exposing the training procedure to inputs that result from likely mistakes ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:8:1","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"8.2 Search Based Structured Prediction also: energy based learning (LeCun et al., 2006, Section 7) Search-based structured prediction is formulated as a search problem over possible structures: $$ \\operatorname{predict}(x)=\\underset{y \\in \\mathcal{Y}(x)}{\\arg \\max } \\operatorname{score}(x, y) $$ scoring function: $$ \\operatorname{score}(x, y)=\\mathbf{w} \\cdot \\Phi(x, y) $$ Φ is a feature extraction function and w is a weight vector decomposed y: $$ \\Phi(x, y)=\\sum_{p \\in \\operatorname{parts}(x, y)} \\phi(p) $$ $$ \\operatorname{score}(x, y)=\\mathbf{w} \\cdot \\Phi(x, y)=\\mathbf{w} \\cdot \\sum_{p \\in y} \\phi(p)=\\sum_{p \\in y} \\mathbf{w} \\cdot \\phi(p)=\\sum_{p \\in y} \\operatorname{score}(p) $$ replace linear scoring function with NN: $$ \\operatorname{score}(x, y)=\\sum_{p \\in y} \\operatorname{score}(p)=\\sum_{p \\in y} \\mathrm{NN}(c(p)) $$ c(p) maps the part p into a $d_{in}$ dimensional vector eg. one-hidden-layer MLP: $$ \\operatorname{score}(x, y)=\\sum_{p \\in y} \\mathrm{NN}_{\\mathrm{MLP1}}(c(p))=\\sum_{p \\in y}\\left(g\\left(c(p) \\mathbf{W}^{1}+\\mathbf{b}^{1}\\right)\\right) \\mathbf{w} $$ find the best scoring structure $y’$, generalized perceptron loss: $$ \\max _{y^{\\prime}} \\operatorname{score}\\left(x, y^{\\prime}\\right)-\\operatorname{score}(x, y) $$ LeCun et al. (2006, Section 5), margin-based hinge loss: $$ \\max \\left(0, m+\\max _{y^{\\prime} \\neq y} \\operatorname{score}\\left(x, y^{\\prime}\\right)-\\operatorname{score}(x, y)\\right) $$ 8.2.1 Probabilistic Objective (CRF) conditional random fields, “CRF”: treat each parts scores as a clique potential and define score: $$ \\begin{aligned} \\operatorname{score}_{\\mathrm{CRF}}(x, y)=P(y \\mid x) \u0026=\\frac{\\exp \\left(\\sum_{p \\in y} \\operatorname{score}(p)\\right)}{\\sum_{y^{\\prime} \\in \\mathcal{Y}(x)} \\exp \\left(\\sum_{p \\in y^{\\prime}} \\operatorname{score}(p)\\right)} \\\\ \u0026=\\frac{\\exp \\left(\\sum_{p \\in y} \\operatorname{NN}(\\phi(p))\\right)}{\\sum_{y^{\\prime} \\in \\mathcal{Y}(x)} \\exp \\left(\\sum_{p \\in y^{\\prime}} \\operatorname{NN}(\\phi(p))\\right)} \\end{aligned} $$ loss for training example (x,y): $$ -\\log \\operatorname{score}_{\\mathrm{CRF}}(x, y) $$ 分母需要计算所有指数多的可能结构，可以用DP, e.g. the forward-backward viterbi recurrences for sequences the CKY insideoutside recurrences for tree structures approximate methods for computing the partition function: eg. beam search (波束搜索) for inference 8.2.2 Reranking reranking framework: base model produce k-best scoring structures, complex model scores the candidates in the k-best list 8.2.3 MEMM and Hybrid Approaches 可以将MEMM中的logistic regression（Maximum Entropy）替换为MLP Hybrid between NN and linear models Weiss et al. (2015): transition-based dependency parsing in a two-stage (MLP2 and NN) model ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:8:2","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"9. Convolutional Layers CBOW忽略了位置信息(“good, not bad” == “bad, not good”) embedding word-pairs (bi-grams) -\u003e huge embedding matrices, sparsity convolution-and-pooling (CNNs) CNN: evolved in CV (LeCun \u0026 Bengio, 1995) bject detectors (Krizhevsky et al., 2012) images using 2-d, text using 1-d evolved in NLP, semantic-role labeling (Collobert, Weston and colleagues, 2011) sentiment and question-type classification (Kalchbrenner et al. (2014) and Kim (2014)) ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:9:0","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"9.1 Basic Convolution + Pooling convolution: each k-word window -\u003e d-dimensional vector (filter) pooling: all d-dimensional vector -\u003e single d-dimensional vector (max or average) 即滑窗学习有价值的k-grams 是否在两端填充： narrow convolution: m = n − k + 1 windows wide convolution: m = n + k + 1 windows convolution layer: m vectors p1, . . . , pm, $\\mathbf{p}_{\\mathbf{i}} \\in \\mathbb{R}^{d_{\\text {conv }}}$ $$ \\mathbf{p}_{\\mathbf{i}}=g\\left(\\mathbf{w}_{\\mathbf{i}} \\mathbf{W}+\\mathbf{b}\\right) $$ 其中 $\\mathbf{W} \\in \\mathbb{R}^{k \\cdot d_{\\mathrm{emb}} \\times d_{\\mathrm{conv}}}$ max-pooling: $$ c_{j}=\\max _{1\u003ci \\leq m} \\mathbf{p}_{\\mathbf{i}}[j] $$ ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:9:1","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"9.2 Dynamic, Hierarchical and k-max Pooling 根据领域知识将每个窗口对应的 $\\mathbf{p}_i$ 划分为l组，每组分别pooling，再全部拼接 e.g. 关系抽取任务，给定两个词判断其关系，可以将窗口划分为两词前、两词后、两词之间三个部分，分别提取特征 hierarchy of convolutional layers k-max pooling: $$ \\left[\\begin{array}{lll} 1 \u0026 2 \u0026 3 \\\\ 9 \u0026 6 \u0026 5 \\\\ 2 \u0026 3 \u0026 1 \\\\ 7 \u0026 8 \u0026 1 \\\\ 3 \u0026 4 \u0026 1 \\end{array}\\right] $$ 1-max: $$ \\left[\\begin{array}{lll} 9 \u0026 8 \u0026 5 \\end{array}\\right] $$ 2-max: $$ \\left[\\begin{array}{lll} 9 \u0026 6 \u0026 3 \\\\ 7 \u0026 8 \u0026 5 \\end{array}\\right] $$ 拼接为： $$ \\left[\\begin{array}{llllll} 9 \u0026 6 \u0026 3 \u0026 7 \u0026 8 \u0026 5 \\end{array}\\right] $$ ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:9:2","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"9.3 Variations parallel convolutional layers: e.g. 4个平行convolutional layers分别采用用2-5的窗口，分别pooling再拼接 Ma et al. (2015) convolution on syntactic dependency trees Liu et al. (2015) convolutional on top of dependency paths extracted from dependency trees Le and Zuidema (2015) perform max pooling over vectors representing the different derivations leading to the same chart item in a chart parser ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:9:3","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"10. Recurrent Neural Networks – Modeling Sequences and Stacks 对比： CBOW: 没有order of features CNNs: 局部order RNNs（Elman, 1990）: 全局order ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:10:0","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"10.1 The RNN Abstraction input: sequence of vectors $\\mathbf{x}_{\\mathbf{i}}, \\ldots, \\mathbf{x}_{\\mathbf{j}}$ initial state vector $\\mathbf{s_o}$ output: state vectors: $\\mathbf{s}_{1}, \\ldots, \\mathbf{s}_{\\mathbf{n}}$ output vectors: $\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{\\mathbf{n}}$ $\\mathbf{y_i}$ for further prediction: $$ p\\left(e=j \\mid \\mathbf{x}_{\\mathbf{1}: \\mathbf{i}}\\right)=\\operatorname{softmax}\\left(\\mathbf{y}_{\\mathbf{i}} \\mathbf{W}+\\mathbf{b}\\right)[j] $$ RNN don’t need Markov assumption, and works better than n-gram. $$ \\begin{array}{c} \\mathrm{RNN}\\left(\\mathrm{s}_{0}, \\mathrm{x}_{1: \\mathrm{n}}\\right)=\\mathrm{s}_{1: \\mathrm{n}}, \\mathrm{y}_{1: \\mathrm{n}} \\\\ \\mathrm{s}_{\\mathrm{i}}=R\\left(\\mathrm{~s}_{\\mathrm{i}-1}, \\mathrm{x}_{\\mathrm{i}}\\right) \\\\ \\mathrm{y}_{\\mathrm{i}}=O\\left(\\mathrm{~s}_{\\mathrm{i}}\\right) \\\\ \\mathrm{x}_{\\mathrm{i}} \\in \\mathbb{R}^{d_{i n}}, \\quad \\mathbf{y}_{\\mathrm{i}} \\in \\mathbb{R}^{d_{\\text {out }}}, \\quad \\mathrm{s}_{\\mathrm{i}} \\in \\mathbb{R}^{f\\left(d_{\\text {out }}\\right)} \\end{array} $$ unroll: $O$ function: Simple RNN (Elman Rnn) / GRU: identity mapping LSTM: fixed subset of state ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:10:1","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"10.2 RNN Training backpropagation through time (BPTT): with unrolled computation graph. variants of BPTT: k-size unrolling forward entire, backward k 10.2.1 Acceptor 接收器 接收整个句子，从final output决策，loss根据$\\mathbf{y}_{\\mathbf{n}}=O\\left(\\mathbf{s}_{\\mathbf{n}}\\right)$定义，再BP回序列其他部分 eg. part-of-speech, sentiment analysis, non-phrase classfication 序列过长时，由于梯度消失，很难训练 输入没有指明重点（？），很难训练 10.2.2 Encoder 编码器 也是只用final output，但将$\\mathbf{y_n}$作为整个序列的信息，与其他信息一起使用。 10.2.3 Transducer 转换器 an output for each input e.g. sequence tagger (SOTA CCG super-tagger: (Xu et al., 2015)) language modeling loss (or average / weighted average): $$ L\\left(\\mathbf{y} \\hat{\\mathbf{1}: \\mathbf{n}}, \\mathbf{y}_{\\mathbf{1}: \\mathbf{n}}\\right)=\\sum_{i=1}^{n} L_{\\text {local }}\\left(\\hat{\\mathbf{y}}_{\\mathbf{i}}, \\mathbf{y}_{\\mathbf{i}}\\right) $$ RNN transduces relax the Markov assumption and condition on the entire history. (powerful!) generative character-level RNN models (Sutskever, Martens, \u0026 Hinton, 2011) 生成的文本相比n-gram捕捉了更多性质，e.g. 句子长度、括号匹配 Karpathy, (Johnson, and Li, 2015) 10.2.4 Encoder - Decoder encoder output as auxiliary input to decoder (transducer-like) eg. machine-translation, great using LSTM (Sutskever et al., 2014) 输入句子倒置，使得$\\mathbf{X_n}$对应开头，翻译时一一对应，效果更好 sequence transduction ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:10:2","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"10.3 Multi-layer (Stacked) RNNs also called deep RNNs deep RNNs works better than shallower ones on some tasks: Sutskever et al. (2014) 4-layers RNN for machine-translation Irsoy and Cardie (2014) biRNN with several layers ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:10:3","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"10.4 Bidirectional RNNs (biRNN) (Schuster \u0026 Paliwal, 1997; Graves, 2008) biRNN relaxes the fixed window size assumption, allowing to look arbitrarily far at both the past and the future two independent RNN: $\\mathrm{RNN}\\left(R^{f}, O^{f}\\right)$ input $\\mathbf{X}_{\\mathbf{1}: \\mathbf{n}}$ $\\mathrm{RNN}\\left(R^{b}, O^{b}\\right)$ input $\\mathbf{X}_{\\mathbf{n}: \\mathbf{1}}$ output: $$ \\mathbf{y}_{\\mathbf{i}}=\\left[\\mathbf{y}_{\\mathbf{i}}^{\\mathbf{f}} ; \\mathbf{y}_{\\mathbf{i}}^{\\mathbf{b}}\\right]=\\left[O^{f}\\left(\\mathbf{s}_{\\mathbf{i}}^{\\mathbf{f}}\\right) ; O^{b}\\left(\\mathbf{s}_{\\mathbf{i}}^{\\mathbf{b}}\\right)\\right] $$ biRNNs for sequence tagging (Irsoy and Cardie, 2014) ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:10:4","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"10.5 RNNs for Representing Stacks main intuition: Encode the stack sequence push: $\\mathrm{s}_{\\mathrm{i+1}}=R(\\mathrm{s}_{\\mathrm{i}}, \\mathrm{x_{i+1}})$ pop: persistent-stack (immutable) data-structure ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:10:5","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"10.6 A Note on Reading the Literature Many aspects of the models are not yet standardized, be careful with ambiguous. ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:10:6","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"11. Concrete RNN Architectures ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:11:0","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"11.1 Simple RNN (SRNN) also: Elman Network (Elman, 1990) $$ \\begin{array}{c} \\mathbf{S}_{\\mathbf{i}}=R_{\\mathrm{SRNN}}\\left(\\mathbf{s}_{\\mathbf{i}-\\mathbf{1}}, \\mathbf{x}_{\\mathbf{i}}\\right)=g\\left(\\mathbf{x}_{\\mathbf{i}} \\mathbf{W}^{\\mathbf{x}}+\\mathbf{s}_{\\mathbf{i}-\\mathbf{1}} \\mathbf{W}^{\\mathbf{s}}+\\mathbf{b}\\right) \\\\ \\mathbf{y}_{\\mathbf{i}}=O_{\\mathrm{SRNN}}\\left(\\mathbf{s}_{\\mathbf{i}}\\right)=\\mathbf{s}_{\\mathbf{i}} \\\\ \\mathbf{s}_{\\mathbf{i}}, \\mathbf{y}_{\\mathbf{i}} \\in \\mathbb{R}^{d_{s}}, \\mathbf{x}_{\\mathbf{i}} \\in \\mathbb{R}^{d_{x}}, \\mathbf{W}^{\\mathbf{x}} \\in \\mathbb{R}^{d_{x} \\times d_{s}}, \\mathbf{W}^{\\mathbf{s}} \\in \\mathbb{R}^{d_{s} \\times d_{s}}, \\mathbf{b} \\in \\mathbb{R}^{d_{s}} \\end{array} $$ strong result in sequence tagging (Xu et al., 2015) and language modeling hard to train for vanishing gradients more discussion: PhD thesis by Mikolov (2012) ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:11:1","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"11.2 Long Short-Term Memory (LSTM) (Hochreiter \u0026 Schmidhuber, 1997) S-RNN hard to capture long-range dependencies, LSTM can solve this vanishing gradients problem. memory cells: preserve gradients across time gating components: smooth mathematical functions that simulate logical gates, control access to the memory cells gete: (after sigmoid) vector $\\mathrm{g} \\in[0,1]^{n}$ $$\\begin{aligned} \\mathbf{s}_{\\mathbf{j}}=R_{\\mathrm{LSTM}}\\left(\\mathbf{s}_{\\mathbf{j}-\\mathbf{1}}, \\mathbf{x}_{\\mathbf{j}}\\right) \u0026=\\left[\\mathbf{c}_{\\mathbf{j}} ; \\mathbf{h}_{\\mathbf{j}}\\right] \\\\ \\mathbf{c}_{\\mathbf{j}} \u0026=\\mathbf{c}_{\\mathbf{j}-\\mathbf{1}} \\odot \\mathbf{f}+\\mathbf{g} \\odot \\mathbf{i} \\\\ \\mathbf{h}_{\\mathbf{j}} \u0026=\\tanh \\left(\\mathbf{c}_{\\mathbf{j}}\\right) \\odot \\mathbf{o} \\\\ \\mathbf{i} \u0026=\\sigma\\left(\\mathbf{x}_{\\mathbf{j}} \\mathbf{W}^{\\mathbf{x} \\mathbf{i}}+\\mathbf{h}_{\\mathbf{j}-\\mathbf{1}} \\mathbf{W}^{\\mathbf{h i}}\\right) \\\\ \\mathbf{f} \u0026=\\sigma\\left(\\mathbf{x}_{\\mathbf{j}} \\mathbf{W}^{\\mathbf{x f}}+\\mathbf{h}_{\\mathbf{j}-\\mathbf{1}} \\mathbf{W}^{\\mathbf{h f}}\\right) \\\\ \\mathbf{o} \u0026=\\sigma\\left(\\mathbf{x}_{\\mathbf{j}} \\mathbf{W}^{\\mathbf{x} \\mathbf{o}}+\\mathbf{h}_{\\mathbf{j}-\\mathbf{1}} \\mathbf{W}^{\\mathbf{h o}}\\right) \\\\ \\mathbf{g} \u0026=\\tanh \\left(\\mathbf{x}_{\\mathbf{j}} \\mathbf{W}^{\\mathbf{x g}}+\\mathbf{h}_{\\mathbf{j}-\\mathbf{1}} \\mathbf{W}^{\\mathbf{h g}}\\right) \\\\ \\mathbf{y}_{\\mathbf{j}}=O_{\\mathrm{LSTM}}\\left(\\mathbf{s}_{\\mathbf{j}}\\right) \u0026=\\mathbf{h}_{\\mathbf{j}} \\end{aligned}$$ $$\\mathbf{j} \\in \\mathbb{R}^{2 \\cdot d_{h}}, \\mathbf{x}_{\\mathbf{i}} \\in \\mathbb{R}^{d_{x}}, \\mathbf{c}_{\\mathbf{j}}, \\mathbf{h}_{\\mathbf{j}}, \\mathbf{i}, \\mathbf{f}, \\mathbf{o}, \\mathbf{g} \\in \\mathbb{R}^{d_{h}}, \\mathbf{W}^{\\mathbf{x} \\circ} \\in \\mathbb{R}^{d_{x} \\times d_{h}}, \\mathbf{W}^{\\mathbf{h} \\circ} \\in \\mathbb{R}^{d_{h} \\times d_{h}}$$ 结构详解：https://colah.github.io/posts/2015-08-Understanding-LSTMs/ f：遗忘门 i：记忆/输入门 更新Cell： o：输出门 further: PhD thesis by Alex Graves (2008) LSTM character-level language model: Karpathy et al. (2015) motivation: Sections 4.2 and 4.3 in the detailed course notes of Cho (2015) variants: Greff, Srivastava, Koutn´ık, Steunebrink, and Schmidhuber (2015) Practical Considerations: initialize the bias term of the forget gate to be close to one when training (Jozefowicz et al., 2015) dropout only on the non-recurrent connection (Zaremba et al., 2014) ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:11:2","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"11.3 Gated Recurrent Unit (GRU) a simpler alternative to the LSTM, also based on a gating mechanism $$\\begin{aligned} \\mathbf{s}_{\\mathbf{j}}=R_{\\mathrm{GRU}}\\left(\\mathbf{s}_{\\mathbf{j}-\\mathbf{1}}, \\mathbf{x}_{\\mathbf{j}}\\right) \u0026=(\\mathbf{1}-\\mathbf{z}) \\odot \\mathbf{s}_{\\mathbf{j}-\\mathbf{1}}+\\mathbf{z} \\odot \\tilde{\\mathbf{s}_{\\mathbf{j}}} \\\\ \\mathbf{z} \u0026=\\sigma\\left(\\mathbf{x}_{\\mathbf{j}} \\mathbf{W}^{\\mathbf{x} \\mathbf{z}}+\\mathbf{s}_{\\mathbf{j}-\\mathbf{1}} \\mathbf{W}^{\\mathbf{s z}}\\right) \\\\ \\mathbf{r} \u0026=\\sigma\\left(\\mathbf{x}_{\\mathbf{j}} \\mathbf{W}^{\\mathbf{x r}}+\\mathbf{s}_{\\mathbf{j}-\\mathbf{1}} \\mathbf{W}^{\\mathbf{s r}}\\right) \\\\ \\tilde{\\mathbf{s}}_{\\mathbf{j}} \u0026=\\tanh \\left(\\mathbf{x}_{\\mathbf{j}} \\mathbf{W}^{\\mathbf{x s}}+\\left(\\mathbf{s}_{\\mathbf{j}-\\mathbf{1}} \\odot \\mathbf{r}\\right) \\mathbf{W}^{\\mathrm{sg}}\\right) \\\\ \\mathbf{y}_{\\mathbf{j}}=O_{\\mathrm{GRU}}\\left(\\mathbf{s}_{\\mathbf{j}}\\right) \u0026=\\mathbf{s}_{\\mathbf{j}} \\end{aligned}$$ $$\\mathbf{s}_{\\mathbf{j}}, \\tilde{\\mathbf{s}}_{\\mathbf{j}} \\in \\mathbb{R}^{d_{s}}, \\mathbf{x}_{\\mathbf{i}} \\in \\mathbb{R}^{d_{x}}, \\mathbf{z}, \\mathbf{r} \\in \\mathbb{R}^{d_{s}}, \\mathbf{W}^{\\mathbf{x} \\circ} \\in \\mathbb{R}^{d_{x} \\times d_{s}}, \\mathbf{W}^{\\mathbf{s} \\circ} \\in \\mathbb{R}^{d_{s} \\times d_{s}}$$ effective in language modeling and machine translation ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:11:3","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"11.4 Other Variants Mikolov et al. (2014): split the state vector si into a slow changing component $c_i$(“context units”) and a fast changing component $h_i$ Le, Jaitly, and Hinton (2015): set the activation function of the S-RNN to a ReLU, and initialize the biases b as zeroes and the matrix Ws as the identify matrix ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:11:4","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"12. Modeling Trees – Recursive Neural Networks recursive neural network (RecNN): RNN from sequences to (binary) trees tree node $p$ encodes the entrie subtree rooted at $p$ $$ \\operatorname{vec}(p)=f\\left(\\operatorname{vec}\\left(c_{1}\\right), \\operatorname{vec}\\left(c_{2}\\right)\\right) $$ ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:12:0","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"12.1 Formal Definition 解析树： unlabeled: 三元组集合 $(i, k, j)$ labeled：六元组集合 $(A \\rightarrow B, C, i, k, j)$ $$ \\begin{aligned} \\operatorname{RecNN}\\left(x_{1}, \\ldots, x_{n}, \\mathcal{T}\\right) \u0026=\\left{\\mathbf{s}_{\\mathbf{i}: \\mathbf{j}}^{\\mathbf{A}} \\in \\mathbb{R}^{d} \\mid q_{i: j}^{A} \\in \\mathcal{T}\\right} \\\\ \\mathbf{s}_{\\mathbf{i}: \\mathbf{i}}^{\\mathbf{A}} \u0026=v\\left(x_{i}\\right) \\\\ \\mathbf{s}_{\\mathbf{i}: \\mathbf{j}}^{\\mathbf{A}} \u0026=R\\left(A, B, C, \\mathbf{s}_{\\mathbf{i}: \\mathbf{k}}^{\\mathbf{B}}, \\mathbf{s}_{\\mathbf{k}+\\mathbf{1} :\\mathbf{j}}^{\\mathbf{C}}\\right) \\quad q_{i: k}^{B} \\in \\mathcal{T}, \\quad q_{k+1: j}^{C} \\in \\mathcal{T} \\end{aligned} $$ 组合函数$R$: $$ R\\left(A, B, C, \\mathbf{s}_{\\mathbf{i}: \\mathbf{k}}^{\\mathbf{B}}, \\mathbf{s}_{\\mathbf{k}+\\mathbf{1}:\\mathbf{j}}^{\\mathbf{C}}\\right)=g\\left(\\left[\\mathbf{s}_{\\mathbf{i}: \\mathbf{k}}^{\\mathbf{B}} ; \\mathbf{s}_{\\mathbf{k}+\\mathbf{1}: \\mathbf{j}}^{\\mathbf{C}}\\right] \\mathbf{W}\\right) $$ 带有标签（label embeddings）的组合函数$R$: $$ R\\left(A, B, C, \\mathbf{s}_{\\mathbf{i}: \\mathbf{k}}^{\\mathbf{B}}, \\mathbf{s}_{\\mathbf{k}+\\mathbf{1} : \\mathbf{j}} \\mathbf{C}\\right)=g\\left(\\left[\\mathbf{s}_{\\mathbf{i}: \\mathbf{k}}^{\\mathbf{B}} ; \\mathbf{s}_{\\mathbf{k}+\\mathbf{1} : \\mathbf{j}}^{\\mathbf{C}} ; v(A) ; v(B)\\right] \\mathbf{W}\\right) $$ 也可以对每一对B、C采用不同的W: $$ R\\left(A, B, C, \\mathbf{s}_{\\mathbf{i}: \\mathbf{k}}^{\\mathbf{B}}, \\mathbf{s}_{\\mathbf{k}+\\mathbf{1}: \\mathbf{j}}^{\\mathbf{C}}\\right)=g\\left(\\left[\\mathbf{s}_{\\mathbf{i}: \\mathbf{k}}^{\\mathbf{B}} ; \\mathbf{s}_{\\mathbf{k}+\\mathbf{1} : \\mathbf{j}}^{\\mathbf{C}}\\right] \\mathbf{W}^{\\mathbf{B C}}\\right) $$ ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:12:1","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"12.2 Extensions and Variations Tree-shaped LSTMs recursive matrix-vector model recursive neural tensor network ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:12:2","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["NLP"],"content":"12.3 Training Recursive Neural Networks Loss: loss on root or any node or a set of nodes one can treat the RecNN as an Encoder ","date":"2021-02-02","objectID":"/blog/nnlp-notes/:12:3","tags":["NNLP","NLP","Neural Network","notes"],"title":"NNLP: A Primer on Neural Network Models for Natural Language Processing","uri":"/blog/nnlp-notes/"},{"categories":["Deep Learning"],"content":"ch6 循环神经网络 学习：随时间反向传播算法 长程依赖：长序列时梯度爆炸和消失 -\u003e 门控机制（Gating Mechanism） 广义记忆网络：递归神经网络、图网络 ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:0:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.1 给网络增强记忆能力 ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:1:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.1.1 延时神经网络 延时神经网络（Time Delay Neural Network，TDNN）：在FNN非输出层都添加一个延时器，记录神经元历史活性值。 l层神经元活性值依赖于l-1层神经元的最近K个时刻的活性值： $$ \\boldsymbol{h}_{t}^{(l)}=f\\left(\\boldsymbol{h}_{t}^{(l-1)}, \\boldsymbol{h}_{t-1}^{(l-1)}, \\cdots, \\boldsymbol{h}_{t-K}^{(l-1)}\\right) $$ ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:1:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.1.2 有外部输入的非线性自回归模型 自回归模型（AutoRegressive Model，AR）：用变量历史信息预测自己。 $$ \\boldsymbol{y}_{t}=w_{0}+\\sum_{k=1}^{K} w_{k} \\boldsymbol{y}_{t-k}+\\epsilon_{t} $$ 有外部输入的非线性自回归模型（Nonlinear AutoRegressive with Exogenous Inputs Model，NARX）： 每个时候都有输入输出，通过延时器记录最近$K_x$次输入和$K_y$次输出，则t时刻输出$y_t$为 $$ \\boldsymbol{y}_{t}=f\\left(\\boldsymbol{x}_{t}, \\boldsymbol{x}_{t-1}, \\cdots, \\boldsymbol{x}_{t-K_{x}}, \\boldsymbol{y}_{t-1}, \\boldsymbol{y}_{t-2}, \\cdots, \\boldsymbol{y}_{t-K_{y}}\\right) $$ ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:1:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.1.3 循环神经网络 活性值/状态（State）/隐状态（Hidden State）更新： $$ \\boldsymbol{h}_{t}=f\\left(\\boldsymbol{h}_{t-1}, \\boldsymbol{x}_{t}\\right) $$ RNN可以近似任意非线性动力系统 FNN模拟任何连续函数，RNN模拟任何程序 ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:1:3","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.2 简单循环网络 简单循环网络（Simple Recurrent Network，SRN） $$ z_{t}=U \\boldsymbol{h}_{t-1}+W x_{t}+\\boldsymbol{b} $$ $$ \\boldsymbol{h}_{t}=f\\left(\\boldsymbol{z}_{t}\\right) $$ ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:2:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.2.1 循环神经网络的计算能力 对于： $$ \\begin{array}{l} \\boldsymbol{h}_{t}=f\\left(\\boldsymbol{U} \\boldsymbol{h}_{t-1}+\\boldsymbol{W} \\boldsymbol{x}_{t}+\\boldsymbol{b}\\right) \\\\ \\boldsymbol{y}_{t}=\\boldsymbol{V} \\boldsymbol{h}_{t} \\end{array} $$ RNN的通用近似定理：全连接RNN在足够多sigmoid隐藏神经元的情况下，可以以任意准确度近似任何一个非线性动力系统： $$ \\begin{array}{l} \\boldsymbol{s}_{t}=g\\left(\\boldsymbol{s}_{t-1}, \\boldsymbol{x}_{t}\\right) \\\\ \\boldsymbol{y}_{t}=o\\left(\\boldsymbol{s}_{t}\\right) \\end{array} $$ RNN是图灵完备的：所有图灵机可以被一个由Sigmoid型激活函数的神经元构成的全连接循环网络来进行模拟，可以近似解决所有可计算问题 ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:2:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.3 应用到机器学习 ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.3.1 序列到类别模式 主要用于序列分类 用最终状态表征序列：$\\hat{y}=g\\left(\\boldsymbol{h}_{T}\\right)$ 用状态平均表征序列：$\\hat{y}=g\\left(\\frac{1}{T} \\sum_{t=1}^{T} \\boldsymbol{h}_{t}\\right)$ ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.3.2 同步的序列到序列模式 主要用于序列标注（Sequence Labeling） $$ \\hat{y}_{t}=g\\left(\\boldsymbol{h}_{t}\\right), \\quad \\forall t \\in[1, T] $$ ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.3.3 异步的序列到序列模式 也称编码器-解码器（Encoder-Decoder）模型 $$ \\begin{aligned} \\boldsymbol{h}_{t} \u0026=f_{1}\\left(\\boldsymbol{h}_{t-1}, \\boldsymbol{x}_{t}\\right), \u0026 \u0026 \\forall t \\in[1, T] \\\\ \\boldsymbol{h}_{T+t} \u0026=f_{2}\\left(\\boldsymbol{h}_{T+t-1}, \\hat{\\boldsymbol{y}}_{t-1}\\right), \u0026 \u0026 \\forall t \\in[1, M] \\\\ \\hat{y}_{t} \u0026=g\\left(\\boldsymbol{h}_{T+t}\\right), \u0026 \u0026 \\forall t \\in[1, M] \\end{aligned} $$ ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:3","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.4 参数学习 以同步的序列到序列为例： $$ \\mathcal{L}_{t}=\\mathcal{L}\\left(y_{t}, g\\left(\\boldsymbol{h}_{t}\\right)\\right) $$ $$ \\mathcal{L}=\\sum_{t=1}^{T} \\mathcal{L}_{t} $$ $$ \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{U}}=\\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}_{t}}{\\partial \\boldsymbol{U}} $$ ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:4:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.4.1 随时间反向传播算法 随时间反向传播（BackPropagation Through Time，BPTT） $$ \\frac{\\partial \\mathcal{L}_{t}}{\\partial u_{i j}}=\\sum_{k=1}^{t} \\frac{\\partial^{+} z_{k}}{\\partial u_{i j}} \\frac{\\partial \\mathcal{L}_{t}}{\\partial z_{k}} $$ t时刻的损失对k时刻的隐藏层净输入的导数： $$ \\begin{aligned} \\delta_{t, k} \u0026=\\frac{\\partial \\mathcal{L}_{t}}{\\partial z_{k}} \\\\ \u0026=\\frac{\\partial \\boldsymbol{h}_{k}}{\\partial \\boldsymbol{z}_{k}} \\frac{\\partial \\boldsymbol{z}_{k+1}}{\\partial \\boldsymbol{h}_{k}} \\frac{\\partial \\mathcal{L}_{t}}{\\partial \\boldsymbol{z}_{k+1}} \\\\ \u0026=\\operatorname{diag}\\left(f^{\\prime}\\left(\\boldsymbol{z}_{k}\\right)\\right) \\boldsymbol{U}^{\\top} \\delta_{t, k+1} \\end{aligned} $$ 参数梯度： $$ \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{U}}=\\sum_{t=1}^{T} \\sum_{k=1}^{t} \\delta_{t, k} \\boldsymbol{h}_{k-1}^{\\top} $$ $$ \\begin{aligned} \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{W}} \u0026=\\sum_{t=1}^{T} \\sum_{k=1}^{t} \\delta_{t, k} \\boldsymbol{x}_{k}^{\\top}, \\\\ \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{b}} \u0026=\\sum_{t=1}^{T} \\sum_{k=1}^{t} \\delta_{t, k} \\end{aligned} $$ ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:4:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.4.2 实时循环学习算法 实时循环学习（Real-Time Recurrent Learning，RTRL）：前向传播计算梯度 $$ \\frac{\\partial \\mathcal{L}_{t}}{\\partial u_{i j}}=\\frac{\\partial \\boldsymbol{h}_{t}}{\\partial u_{i j}} \\frac{\\partial \\mathcal{L}_{t}}{\\partial \\boldsymbol{h}_{t}} $$ 两种算法比较： 一般为网络输出维度远地域输入，BPTT计算量更小 BPTT保持所有时刻的中间维度，空间复杂度较高 RTRL不需要梯度回传，适合在线学习或无限序列任务 ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:4:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.5 长程依赖问题 由于梯度消失或爆炸问题，很难建模长时间间隔（Long Range）的状态之间的依赖关系 ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:5:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.5.1 改进方案 梯度爆炸：权重衰减（正则项）、梯度截断 梯度消失：优化技巧、改变模型 ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:5:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.6 基于门控的RNN 基于门控的循环神经网络（Gated RNN） ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:6:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.6.1 长短期记忆网络 长短期记忆网络（Long Short-Term Memory Network，LSTM） 引入新的内部状态（internal state）$\\boldsymbol{c}_{t} \\in \\mathbb{R}^{D}$ 专门进行线性的循环信息传递，同时非线性地输出信息给隐藏层的外部状态 $\\boldsymbol{h}_{t} \\in \\mathbb{R}^{D}$ $$ \\begin{aligned} \\boldsymbol{c}_{t} \u0026=\\boldsymbol{f}_{t} \\odot \\boldsymbol{c}_{t-1}+\\boldsymbol{i}_{t} \\odot \\tilde{\\boldsymbol{c}}_{t} \\\\ \\boldsymbol{h}_{t} \u0026=\\boldsymbol{o}_{t} \\odot \\tanh \\left(\\boldsymbol{c}_{t}\\right) \\end{aligned} $$ 候选状态： $$ \\tilde{\\boldsymbol{c}}_{t}=\\tanh \\left(\\boldsymbol{W}_{c} \\boldsymbol{x}_{t}+\\boldsymbol{U}_{c} \\boldsymbol{h}_{t-1}+\\boldsymbol{b}_{c}\\right) $$ 门控机制： $$ \\begin{aligned} \\boldsymbol{i}_{t} \u0026=\\sigma\\left(\\boldsymbol{W}_{i} \\boldsymbol{x}_{t}+\\boldsymbol{U}_{i} \\boldsymbol{h}_{t-1}+\\boldsymbol{b}_{i}\\right) \\\\ \\boldsymbol{f}_{t} \u0026=\\sigma\\left(\\boldsymbol{W}_{f} \\boldsymbol{x}_{t}+\\boldsymbol{U}_{f} \\boldsymbol{h}_{t-1}+\\boldsymbol{b}_{f}\\right) \\\\ \\boldsymbol{o}_{t} \u0026=\\sigma\\left(\\boldsymbol{W}_{o} \\boldsymbol{x}_{t}+\\boldsymbol{U}_{o} \\boldsymbol{h}_{t-1}+\\boldsymbol{b}_{o}\\right) \\end{aligned} $$ 简洁描述： $$ \\begin{aligned} \\left[\\begin{array}{c} \\tilde{\\boldsymbol{c}}_{t} \\\\ \\boldsymbol{o}_{t} \\\\ \\boldsymbol{i}_{t} \\\\ \\boldsymbol{f}_{t} \\end{array}\\right] \u0026=\\left[\\begin{array}{c} \\tanh \\\\ \\sigma \\\\ \\sigma \\\\ \\sigma \\end{array}\\right]\\left(\\boldsymbol{W}\\left[\\begin{array}{c} \\boldsymbol{x}_{t} \\\\ \\boldsymbol{h}_{t-1} \\end{array}\\right]+\\boldsymbol{b}\\right), \\\\ \\boldsymbol{c}_{t} \u0026=\\boldsymbol{f}_{t} \\odot \\boldsymbol{c}_{t-1}+\\boldsymbol{i}_{t} \\odot \\tilde{\\boldsymbol{c}}_{t} \\\\ \\boldsymbol{h}_{t} \u0026=\\boldsymbol{o}_{t} \\odot \\tanh \\left(\\boldsymbol{c}_{t}\\right) \\end{aligned} $$ 记忆： 短期记忆：S-RNN中隐状态，每个时刻都会被重写 长期记忆：网络参数 长短期记忆LSTM：记忆单元 c 保存的信息生命周期长于短期记忆h，短于长期记忆 参数设置： 一般深度学习初始参数比较小，但是遗忘的参数初始值一般设得比较大，偏置向量 $b_f$ 设为1或2，防止大量遗忘导致难以捕捉长距离依赖信息（梯度弥散） ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:6:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.6.2 LSTM网络的各种变体 改进门控机制： 无遗忘门的 LSTM 网络（[Hochreiter et al., 1997] 最早提出的 LSTM 网络）： $$ \\boldsymbol{c}_{t}=\\boldsymbol{c}_{t-1}+\\boldsymbol{i}_{t} \\odot \\tilde{\\boldsymbol{c}}_{t} $$ peephole连接：也依赖与上一时刻记忆单元： $$ \\begin{aligned} \\boldsymbol{i}_{t} \u0026=\\sigma\\left(\\boldsymbol{W}_{i} \\boldsymbol{x}_{t}+\\boldsymbol{U}_{i} \\boldsymbol{h}_{t-1}+\\boldsymbol{V}_{i} \\boldsymbol{c}_{t-1}+\\boldsymbol{b}_{i}\\right) \\\\ \\boldsymbol{f}_{t} \u0026=\\sigma\\left(\\boldsymbol{W}_{f} \\boldsymbol{x}_{t}+\\boldsymbol{U}_{f} \\boldsymbol{h}_{t-1}+\\boldsymbol{V}_{f} \\boldsymbol{c}_{t-1}+\\boldsymbol{b}_{f}\\right) \\\\ \\boldsymbol{o}_{t} \u0026=\\sigma\\left(\\boldsymbol{W}_{o} \\boldsymbol{x}_{t}+\\boldsymbol{U}_{o} \\boldsymbol{h}_{t-1}+\\boldsymbol{V}_{o} \\boldsymbol{c}_{t}+\\boldsymbol{b}_{o}\\right) \\end{aligned} $$ 耦合输入门和遗忘门（$\\boldsymbol{f}_{t}=1-\\boldsymbol{i}_{t}$）： $$ \\boldsymbol{c}_{t}=\\left(1-\\boldsymbol{i}_{t}\\right) \\odot \\boldsymbol{c}_{t-1}+\\boldsymbol{i}_{t} \\odot \\tilde{\\boldsymbol{c}}_{t} $$ ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:6:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"门控循环单元网络 门控循环单元（Gated Recurrent Unit，GRU）网络 不引入额外记忆单元 更新门（Update Gate） 重置门（Reset Gate） $$ z_{t}=\\sigma\\left(W_{z} x_{t}+U_{z} h_{t-1}+b_{z}\\right) $$ $$ \\boldsymbol{r}_{t}=\\sigma\\left(\\boldsymbol{W}_{r} \\boldsymbol{x}_{t}+\\boldsymbol{U}_{r} \\boldsymbol{h}_{t-1}+\\boldsymbol{b}_{r}\\right) $$ $$ \\tilde{\\boldsymbol{h}}_{t}=\\tanh \\left(\\boldsymbol{W}_{h} \\boldsymbol{x}_{t}+\\boldsymbol{U}_{h}\\left(\\boldsymbol{r}_{t} \\odot \\boldsymbol{h}_{t-1}\\right)+\\boldsymbol{b}_{h}\\right) $$ $$ \\boldsymbol{h}_{t}=\\boldsymbol{z}_{t} \\odot \\boldsymbol{h}_{t-1}+\\left(1-\\boldsymbol{z}_{t}\\right) \\odot \\tilde{\\boldsymbol{h}}_{t} $$ 当 $z_t$ = 0, r = 1 时，GRU 网络退化为简单循环网络 ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:6:3","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.7 深层RNN 加深x到y的路径 ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:7:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.7.1 堆叠循环神经网络 堆叠循环神经网络（Stacked Recurrent Neural Network，SRNN） 其中，堆叠的简单循环网络（Stacked SRN）也成为循环多层感知机（Recurrent MultiLayer Perceptron，RMLP） $$ \\boldsymbol{h}_{t}^{(l)}=f\\left(\\boldsymbol{U}^{(l)} \\boldsymbol{h}_{t-1}^{(l)}+\\boldsymbol{W}^{(l)} \\boldsymbol{h}_{t}^{(l-1)}+\\boldsymbol{b}^{(l)}\\right) $$ ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:7:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.7.2 双向循环神经网络 双向循环神经网络（Bidirectional Recurrent Neural Network，Bi-RNN） $$ \\begin{aligned} \\boldsymbol{h}_{t}^{(1)} \u0026=f\\left(\\boldsymbol{U}^{(1)} \\boldsymbol{h}_{t-1}^{(1)}+\\boldsymbol{W}^{(1)} \\boldsymbol{x}_{t}+\\boldsymbol{b}^{(1)}\\right) \\\\ \\boldsymbol{h}_{t}^{(2)} \u0026=f\\left(\\boldsymbol{U}^{(2)} \\boldsymbol{h}_{t+1}^{(2)}+\\boldsymbol{W}^{(2)} \\boldsymbol{x}_{t}+\\boldsymbol{b}^{(2)}\\right) \\\\ \\boldsymbol{h}_{t} \u0026=\\boldsymbol{h}_{t}^{(1)} \\oplus \\boldsymbol{h}_{t}^{(2)} \\end{aligned} $$ ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:7:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.8 扩展到图结构 ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:8:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.8.1 递归神经网络 递归神经网络（Recursive Neural Network，RecNN）：RNN在有向无环图上的扩展 RecNN退化为线性序列结构时，等价于简单循环网络 RecNN主要用于建模自然语言句子的语义 树结构的长短期记忆模型（Tree-Structured LSTM） ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:8:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"6.8.2 图神经网络 图神经网络（Graph Neural Network，GNN） 每个结点 v 用一组神经元表示其状态$\\boldsymbol{h}^{(v)}$，初始状态为节点 v 的输入特征$\\boldsymbol{x}^{(v)}$。每个节点接收相邻节点的消息，并更新自己的状态 $$ \\begin{aligned} \\boldsymbol{m}_{t}^{(v)} \u0026=\\sum_{u \\in \\mathcal{N}(v)} f\\left(\\boldsymbol{h}_{t-1}^{(v)}, \\boldsymbol{h}_{t-1}^{(u)}, \\boldsymbol{e}^{(u, v)}\\right), \\\\ \\boldsymbol{h}_{t}^{(v)} \u0026=g\\left(\\boldsymbol{h}_{t-1}^{(v)}, \\boldsymbol{m}_{t}^{(v)}\\right) \\end{aligned} $$ 上式为同步更新，对于有向图采用异步更新会更有效率，比如RNN和RecNN。 读出函数（Readout Function）得到整个网络的表示： $$ \\boldsymbol{o}_{t}=g\\left(\\{\\boldsymbol{h}_{T}^{(v)} \\mid v \\in \\mathcal{V}\\}\\right) $$ ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:8:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"习题 习题 6-1 分析延时神经网络、卷积神经网络和循环神经网络的异同点． 同：共享权重 异： 延时神经网络依赖最近K个状态（活性值），RNN依赖之前所有状态 RNN在时间维度共享权重，CNN在空间维度共享权重 习题 6-2 推导公式 (6.40) 和公式 (6.41) 中的梯度． 分别替换 $\\boldsymbol{h}_{k-1}^{\\top}$ 为 $\\boldsymbol{x}_{k}^{\\top}$ 和 $1$ 即可 习题6-3 当使用公式(6.50) 作为循环神经网络的状态更新公式时，分析其可能存在梯度爆炸的原因并给出解决方法． 公式（6.50）： $$ \\boldsymbol{h}_{t}=\\boldsymbol{h}_{t-1}+g\\left(\\boldsymbol{x}_{t}, \\boldsymbol{h}_{t-1} ; \\theta\\right), $$ 计算误差项时梯度可能过大，不断反向累积导致梯度爆炸： $$ \\begin{aligned} \\delta_{t, k} \u0026=\\frac{\\partial \\mathcal{L}_{t}}{\\partial \\boldsymbol{z}_{k}} \\\\ \u0026=\\frac{\\partial \\boldsymbol{h}_{k}}{\\partial \\boldsymbol{z}_{k}} \\frac{\\partial \\boldsymbol{z}_{k+1}}{\\partial \\boldsymbol{h}_{k}} \\frac{\\partial \\mathcal{L}_{t}}{\\partial \\boldsymbol{z}_{k+1}} \\\\ \u0026=\\operatorname{diag}\\left(f^{\\prime}\\left(\\boldsymbol{z}_{k}\\right)\\right) \\boldsymbol{U}^{\\top} \\delta_{t, k+1} \\end{aligned} $$ 解决方法：引入门控机制等。 习题 6-4 推导 LSTM 网络中参数的梯度，并分析其避免梯度消失的效果． 习题 6-5 推导 GRU 网络中参数的梯度，并分析其避免梯度消失的效果． 习题 6-6 除了堆叠循环神经网络外，还有什么结构可以增加循环神经网络深度？ 增加神经网络深度主要方法：增加同一时刻网络输入到输出之间的路径。 如：堆叠神经网络、双向循环网络等。 习题 6-7 证明当递归神经网络的结构退化为线性序列结构时，递归神经网络就等价于简单循环神经网络． RecNN 退化为线性序列结构时： $$ \\boldsymbol{h}_{t}=\\sigma\\left(\\boldsymbol{W}\\left[\\begin{array}{l} \\boldsymbol{h}_{t-1} \\\\ \\boldsymbol{x}_{t} \\end{array}\\right]+\\boldsymbol{b}\\right) $$ 显而易见，即 SRN. ","date":"2021-02-01","objectID":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:9:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第6章 - 循环神经网络","uri":"/blog/nndl-book-ch6-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["NLP"],"content":"Distributed Representations of Words and Phrases and their Compositionality [Mikolov 2013] negative sampling with Skip-gram ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec-improvement/:0:0","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec improvement","uri":"/blog/nlp-papersword2vec-improvement/"},{"categories":["NLP"],"content":"1 Abstract \u0026 Introduction several extensions of continuous skip-gram that improve quality and speed: subsampling of the frequent words speedup (around 2x - 10x) improve accuracy of less frequent words Noise Contrastive Estimation (NCE) replace hierarchical softmax nagative sampling (alternative to hierarchical softmax) treat word pairs / phase as one word interesting property of Skip-gram: simple vector addition can often produce meaningful results vec(“Russia”) + vec(“river”) is close to vec(“Volga River”) vec(“Germany”) + vec(“capital”) is close to vec(“Berlin”) ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec-improvement/:1:0","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec improvement","uri":"/blog/nlp-papersword2vec-improvement/"},{"categories":["NLP"],"content":"2 The Skip-gram Model objective is to maximize: $$ \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p\\left(w_{t+j} \\mid w_{t}\\right) $$ defines $p(w_{t+j}\\mid w_t)$ using softmax: $$ p\\left(w_{O} \\mid w_{I}\\right)=\\frac{\\exp \\left(v_{w_{O}}^{\\prime}{ }^{\\top} v_{w_{I}}\\right)}{\\sum_{w=1}^{W} \\exp \\left(v_{w}^{\\prime}{ }^{\\top} v_{w_{I}}\\right)} $$ impractical because the cost of computing $\\nabla \\log p\\left(w_{O} \\mid w_{I}\\right)$ is proportional to W ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec-improvement/:2:0","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec improvement","uri":"/blog/nlp-papersword2vec-improvement/"},{"categories":["NLP"],"content":"2.1 Hierarchical Softmax CBOW 输出层为以词在语料出现次数为权值构造的 Huffman 树，每个词为叶子结点，每个分支视为二分类（假设左负右正），路径上的概率之积： $$ p(w \\mid \\operatorname{Context}(w))=\\prod_{j=2}^{l^{w}} p\\left(d_{j}^{w} \\mid \\mathbf{x}_{w}, \\theta_{j-1}^{w}\\right) $$ 其中： $$ p\\left(d_{j}^{w} \\mid \\mathbf{x}_{w}, \\theta_{j-1}^{w}\\right)=\\left{\\begin{array}{ll} \\sigma\\left(\\mathbf{x}_{w}^{\\top} \\theta_{j-1}^{w}\\right), \u0026 d_{j}^{w}=0 \\\\ 1-\\sigma\\left(\\mathbf{x}_{w}^{\\top} \\theta_{j-1}^{w}\\right), \u0026 d_{j}^{w}=1 \\end{array}\\right. $$ 或者写成整体表达式： $$ p\\left(d_{j}^{w} \\mid \\mathbf{x}_{w}, \\theta_{j-1}^{w}\\right)=\\left[\\sigma\\left(\\mathbf{x}_{w}^{\\top} \\theta_{j-1}^{w}\\right)\\right]^{1-d_{j}^{w}} \\cdot\\left[1-\\sigma\\left(\\mathbf{x}_{w}^{\\top} \\theta_{j-1}^{w}\\right)\\right]^{d_{j}^{w}} $$ 带入对数似然函数，采用随机梯度上升即可 SG 同理： $$ p(u \\mid w)=\\prod_{j=2}^{l^{u}} p\\left(d_{j}^{u} \\mid \\mathbf{v}(w), \\theta_{j-1}^{u}\\right) $$ 其中： $$ p\\left(d_{j}^{u} \\mid \\mathbf{v}(w), \\theta_{j-1}^{u}\\right)=\\left[\\sigma\\left(\\mathbf{v}(w)^{\\top} \\theta_{j-1}^{u}\\right)\\right]^{1-d_{j}^{u}} \\cdot\\left[1-\\sigma\\left(\\mathbf{v}(w)^{\\top} \\theta_{j-1}^{u}\\right)\\right]^{d_{j}^{u}} $$ ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec-improvement/:2:1","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec improvement","uri":"/blog/nlp-papersword2vec-improvement/"},{"categories":["NLP"],"content":"2.2 Negative Sampling 增大正样本概率，降低一部分负样本概率： $$ \\log \\sigma\\left(v_{w_{O}}^{\\prime}{ }^{\\top} v_{w_{I}}\\right)+\\sum_{i=1}^{k} \\mathbb{E}_{w_{i} \\sim P_{n}(w)}\\left[\\log \\sigma\\left(-v_{w_{i}}^{\\prime}{ }^{\\top} v_{w_{I}}\\right)\\right] $$ 选取概率，noise distribution： $$ P_n(w)=U(w)^{3 / 4} / Z $$ ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec-improvement/:2:2","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec improvement","uri":"/blog/nlp-papersword2vec-improvement/"},{"categories":["NLP"],"content":"2.3 Subsampling of Frequent Words 每个词汇以一定概率丢弃： $$ P\\left(w_{i}\\right)=1-\\sqrt{\\frac{t}{f\\left(w_{i}\\right)}} $$ ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec-improvement/:2:3","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec improvement","uri":"/blog/nlp-papersword2vec-improvement/"},{"categories":["NLP"],"content":"Efficient Estimation of Word Representations in Vector Space [Mikolov 2013] original word2vec paper images from The Pre-LSTM Ice-Age References https://machinelearningmedium.com/2017/07/11/word-to-vector-word-representations/ Y. Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137-1155, 2003. T. Mikolov, M. Karafi´at, L. Burget, J. ˇCernock´y, S. Khudanpur. Recurrent neural network based language model, In: Proceedings of Interspeech, 2010. ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec/:0:0","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec","uri":"/blog/nlp-papersword2vec/"},{"categories":["NLP"],"content":"Abstract continuous word2vec: skip-gram, CBOW measure: word similarity task (syntactic and semantic) SOTA! ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec/:1:0","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec","uri":"/blog/nlp-papersword2vec/"},{"categories":["NLP"],"content":"1 Introduction ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec/:2:0","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec","uri":"/blog/nlp-papersword2vec/"},{"categories":["NLP"],"content":"1.1 Goals of the Paper Introduce techniques that can be used for learning high-quality wordvectors from huge data sets with billions of words, and with millions of words in the vocabulary. ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec/:2:1","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec","uri":"/blog/nlp-papersword2vec/"},{"categories":["NLP"],"content":"1.2 Previous Work neural network language model (NNLM) learn word vector with single hidden layer, then train the NNLM (this work is the extension of this architecture) ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec/:2:2","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec","uri":"/blog/nlp-papersword2vec/"},{"categories":["NLP"],"content":"2 Model Architectures representations of words continuous: Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA) distributed: word2vec focus on 分布式与连续不互斥, 分布式表示相对的是one-hot表示。 training complexity (all the following models): $O=E \\times T \\times Q$ E=epochs(3-50), T=words in trainning set, Q depends on model trainning method: SGD, BP ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec/:3:0","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec","uri":"/blog/nlp-papersword2vec/"},{"categories":["NLP"],"content":"2.1 Feedforward Neural Net Language Model (NNLM) [Bengio, 2003] 3 layers: input: $N$ previous words (1-of-V coding, V is vovabulary size) projection: $D$ hidden: $H$ output: $V$，对于输入词，每个词是 complexity: $$ Q=N \\times D+N \\times D \\times H+H \\times V $$ dominating term is $H \\times V$, reduce to $H * \\log _{2}(V)$: Hierarchical softmax Avoiding normalized models for training Binary tree representations of the vocabulary (Huffman Trees) dominating term becomes $N \\times D \\times H$ ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec/:3:1","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec","uri":"/blog/nlp-papersword2vec/"},{"categories":["NLP"],"content":"2.2 Recurrent Neural Net Language Model (RNNLM) [Mikolov, 2010] 2 layers: input: $D=H$ hidden: $H$ (with recurrent matrix connects itself) ouput: $V$ complexity: $$ Q=H \\times H+H \\times V $$ $H \\times V$ can be efficiently reduced to $H \\times \\log_2(V)$ by using hierarchical softmax Most of the complexity then comes from $H \\times H$ ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec/:3:2","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec","uri":"/blog/nlp-papersword2vec/"},{"categories":["NLP"],"content":"2.3 Parallel Training of Neural Networks mini-batch asynchronous gradient descent with Adagrad ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec/:3:3","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec","uri":"/blog/nlp-papersword2vec/"},{"categories":["NLP"],"content":"3 New Log-linear Models simplify hidden layer ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec/:4:0","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec","uri":"/blog/nlp-papersword2vec/"},{"categories":["NLP"],"content":"3.1 Continuous Bag-of-Words Model（CBOW） predicts the current word based on the context bag-of-words: order not matter the weight matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM complexity: $$ Q=N \\times D+D \\times \\log _{2}(V) $$ ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec/:4:1","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec","uri":"/blog/nlp-papersword2vec/"},{"categories":["NLP"],"content":"3.2 Continuous Skip-gram Model（SG） predicts surrounding words given the current word increase range (window size) improves quality and computational complexity give less weight to distant words by sampling less complexity: $$ Q=C \\times\\left(D+D \\times \\log _{2}(V)\\right) $$ C = max distance ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec/:4:2","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec","uri":"/blog/nlp-papersword2vec/"},{"categories":["NLP"],"content":"4 Results Algebraic operations on the vector representations X = vector(”biggest”)−vector(”big”)+vector(”small”) search for word closest to X measured by cosine distance (answer is smallest) ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec/:5:0","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec","uri":"/blog/nlp-papersword2vec/"},{"categories":["NLP"],"content":"4.1 Task Description five types of semantic questions nine types of syntacitic questions ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec/:5:1","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec","uri":"/blog/nlp-papersword2vec/"},{"categories":["NLP"],"content":"4.2 Maximization of Accuracy corpus: Google News ","date":"2021-01-25","objectID":"/blog/nlp-papersword2vec/:5:2","tags":["NLP","word2vec","notes"],"title":"【NLP Papers】word2vec","uri":"/blog/nlp-papersword2vec/"},{"categories":["Deep Learning"],"content":"ch4 前馈神经网络 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:0:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.1 神经元 $$ \\begin{aligned} z \u0026=\\sum_{d=1}^{D} w_{d} x_{d}+b \\\\ \u0026=\\boldsymbol{w}^{\\top} \\boldsymbol{x}+b \\end{aligned} $$ 净输入z在经过非线性函数 𝑓(⋅) 后，得到神经元的活性值（Activation）$a=f(z)$ 非线性函数 𝑓(⋅) 称为激活函数（Activation Function） 激活函数： 连续可导（允许少数点上不可导）的非线性函数 可导则可以直接利用数值优化方法学习参数 导函数尽可能简单，提高计算效率 导函数值域区间合适，太大太小影响训练效率和稳定性 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:1:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.1.1 Sigmoid 型函数 指一类S型函数，两端饱和（正负无穷导函数趋近0），常用的有Logistic、Tanh Logistic 函数： $$ \\sigma(x)=\\frac{1}{1+\\exp (-x)} $$ $$ \\sigma^{\\prime}(x)=\\sigma(x)(1-\\sigma(x)) $$ 其输出直接可以看作概率分布，使得神经网络可以更好地和统计学习模型进行结合． 其可以看作一个软性门（Soft Gate），用来控制其他神经元输出信息的数量 Tanh 函数： $$ \\tanh (x)=\\frac{\\exp (x)-\\exp (-x)}{\\exp (x)+\\exp (-x)} $$ $$ \\tanh (x)=2 \\sigma(2 x)-1 $$ Tanh函数输出是零中心化的（Zero-Centered），非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下降的收敛速度变慢． 4.1.1.1 Hard-Logistic 函数和 Hard-Tanh 函数 分段函数近似 $$ \\begin{aligned} \\text { hard-logistic }(x)=\u0026\\left{\\begin{array}{ll} 1 \u0026 g_{l}(x) \\geq 1 \\\\ g_{l} \u0026 0\u003cg_{l}(x)\u003c1 \\\\ 0 \u0026 g_{l}(x) \\leq 0 \\end{array}\\right.\\ \u0026=\\max \\left(\\min \\left(g_{l}(x), 1\\right), 0\\right) \\\\ \u0026=\\max (\\min (0.25 x+0.5,1), 0) . \\end{aligned} $$ $$ \\begin{aligned} \\operatorname{hard}-\\tanh (x) \u0026=\\max \\left(\\min \\left(g_{t}(x), 1\\right),-1\\right) \\\\ \u0026=\\max (\\min (x, 1),-1) \\end{aligned} $$ ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:1:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.1.2 ReLU 函数 ReLU（Rectified Linear Unit，修正线性单元），也叫Rectifier函数 $$ \\begin{aligned} \\operatorname{ReLU}(x) \u0026=\\left{\\begin{array}{ll} x \u0026 x \\geq 0 \\\\ 0 \u0026 x\u003c0 \\end{array}\\right.\\ \u0026=\\max (0, x) \\end{aligned} $$ 优点： 生物学合理性（Biological Plausibility） ReLU 具有很好的稀疏性，大约 50% 的神经元会处于激活状态 相比于 Sigmoid 型函数的两端饱和，ReLU 函数为左饱和函数，一定程度缓解了梯度消失问题，加速收敛 缺点： 非零中心化，给后一层引入偏置偏移 死亡 ReLU 问题（Dying ReLU Problem）：一次不当更新引起永世不得激活 4.1.2.1 带泄露的 ReLU $$ \\text { LeakyReLU( } x)=\\max (x, \\gamma x) $$ 避免永不激活 4.1.2.2 带参数的 ReLU $$ \\begin{aligned} \\operatorname{PReLU}_{i}(x) \u0026=\\left{\\begin{array}{ll} x \u0026 \\text { if } x\u003e0 \\\\ \\gamma_{i} x \u0026 \\text { if } x \\leq 0 \\end{array}\\right.\\ \u0026=\\max (0, x)+\\gamma_{i} \\min (0, x) \\end{aligned} $$ 引入可学习参数 4.1.2.3 ELU 函数 ELU（Exponential Linear Unit，指数线性单元） $$ \\begin{aligned} \\operatorname{ELU}(x) \u0026=\\left{\\begin{array}{ll} x \u0026 \\text { if } x\u003e0 \\\\ \\gamma(\\exp (x)-1) \u0026 \\text { if } x \\leq 0 \\end{array}\\right.\\ \u0026=\\max (0, x)+\\min (0, \\gamma(\\exp (x)-1)), \\end{aligned} $$ 近似的零中心化的非线性函数 4.1.2.4 Softplus 函数 $$ \\text { Softplus }(x)=\\log (1+\\exp (x)) $$ ReLU平滑版本 Softplus 函数其导数刚好是 Logistic 函数 单侧抑制、宽兴奋边界的特性，却没有稀疏激活性 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:1:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.1.3 Swish 函数 一种自门控（Self-Gated）激活函数 $$ \\operatorname{swish}(x)=x \\sigma(\\beta x) $$ 线性函数和 ReLU 函数之间的非线性插值函数，其程度由参数 𝛽 控制 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:1:3","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.1.4 GELU 函数 GELU（Gaussian Error Linear Unit，高斯误差线性单元），也是门控激活函数 $$ \\operatorname{GELU}(x)=x P(X \\leq x) $$ ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:1:4","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.1.5 Maxout 单元 一种分段线性函数 输入是上一层的全部原始输出$\\boldsymbol{x}=\\left[x_{1} ; x_{2} ; \\cdots ; x_{D}\\right]$ 采用 Maxout 单元的神经网络也叫作Maxout网络 $$ z_{k}=\\boldsymbol{w}_{k}^{\\top} \\boldsymbol{x}+b_{k} $$ $$ \\operatorname{maxout}(\\boldsymbol{x})=\\max _{k \\in[1, K]}\\left(z_{k}\\right) $$ ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:1:5","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.2 网络结构 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:2:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.2.1 前馈网络 包括 全连接前馈网络 卷积神经网络 可以看作一个函数 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:2:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.2.2 记忆网络 也称反馈网络 具有记忆功能，可以接受历史信息 信息传播可以单向或双向，可以用有向循环图或者无向图表示 包括 循环神经网络 Hopfield网络 玻尔兹曼机 受限玻尔兹曼机 记忆增强神经网络（Memory Augmented Neural Network，MANN）：为了增强记忆容量，引入外部记忆单元和读写机制，eg. 神经图灵机、记忆网络 可以看作一个程序 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:2:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.2.3 图网络 前馈网络和记忆网络很难处理图结构的数据，如知识图谱、社交网络、分子（Molecular ）网络 是前馈网络和记忆网络的泛化，实现方式很多 图卷积网络（Graph Convolutional Network，GCN） 图注意力网络（Graph Attention Network，GAT） 消息传递神经网络（Message Passing Neural Network，MPNN） ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:2:3","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.3 前馈神经网络 前馈神经网络（Feedforward Neural Network，FNN），也称多层感知器（Multi-Layer Perceptron，MLP） MLP叫法不合理，FNN由多层Logistic回归模型（连续）组成，而非多层感知器（非连续） $$ \\begin{array}{l} z^{(l)}=W^{(l)} a^{(l-1)}+b^{(l)} \\\\ a^{(l)}=f_{l}\\left(z^{(l)}\\right) \\end{array} $$ 仿射变换（Affine Transformation，线性变化 + 平移） + 非线性变换 整个网络可以看作一个复合函数𝜙(𝒙; 𝑾, 𝒃) ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.3.1 通用近似定理 FNN可以近似任何连续非线性函数 通用近似定理（Universal Approximation Theorem） XML ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.3.2 应用到机器学习 多层前馈神经网络也可以看成是一种特征转换方法，其输出 𝜙(𝒙) 作为分类器的输入进行分类 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.3.3 参数学习 梯度下降法需要计算损失函数对参数的偏导数，链式法则逐一求偏导比较低效，常用方向传播算法。 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:3:3","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.4 反向传播算法 第 𝑙 层神经元的误差项： $$ \\delta^{(l)} \\triangleq \\frac{\\partial \\mathcal{L}(\\boldsymbol{y}, \\hat{\\boldsymbol{y}})}{\\partial \\boldsymbol{z}^{(l)}} \\in \\mathbb{R}^{M_{l}} $$ 误差项𝛿(𝑙) 也间接反映了不同神经元对网络能力的贡献程度，从而比较好地解决了贡献度分配问题（Credit Assignment Problem，CAP） $$ \\begin{aligned} \\delta^{(l)} \u0026 \\triangleq \\frac{\\partial \\mathcal{L}(\\boldsymbol{y}, \\hat{\\boldsymbol{y}})}{\\partial \\boldsymbol{z}^{(l)}} \\\\ \u0026=\\frac{\\partial \\boldsymbol{a}^{(l)}}{\\partial \\boldsymbol{z}^{(l)}} \\cdot \\cdot \\cdot \\frac{\\partial \\boldsymbol{z}^{(l+1)}}{\\partial \\boldsymbol{a}^{(l)}} \\cdot {\\frac{\\partial \\mathcal{L}(\\boldsymbol{y}, \\hat{\\boldsymbol{y}})}{\\partial \\boldsymbol{z}^{(l+1)}}} \\\\ \u0026={\\operatorname{diag}\\left(f_{l}^{\\prime}\\left(\\boldsymbol{z}^{(l)}\\right)\\right)}\\left(\\boldsymbol{W}^{(l+1)}\\right)^{\\mathrm{T}} \\cdot {\\delta}^{(l+1)} \\\\ \u0026=f_{l}^{\\prime}\\left(\\boldsymbol{z}^{(l)}\\right) \\odot\\left(\\left(\\boldsymbol{W}^{(l+1)}\\right)^{\\top} \\delta^{(l+1)}\\right) \\quad \\in \\mathbb{R}^{M} \\end{aligned} $$ 误差的反向传播（BackPropagation，BP）：l层误差通过l+1层误差计算得到 BP算法内涵：l层一个神经元误差项是所有与该神经元相连的l+1层神经元的误差项的权重和，再乘上该神经元激活函数的梯度 $$ \\begin{aligned} \\frac{\\partial \\mathcal{L}(\\boldsymbol{y}, \\hat{\\boldsymbol{y}})}{\\partial w_{i j}^{(l)}} \u0026=\\llbracket_{i}\\left(a_{j}^{(l-1)}\\right) \\delta^{(l)} \\\\ \u0026=\\left[0, \\cdots, a_{j}^{(l-1)}, \\cdots, 0\\right]\\left[\\delta_{1}^{(l)}, \\cdots, \\delta_{i}^{(l)}, \\cdots, \\delta_{M_{l}}^{(l)}\\right]^{\\top} \\\\ \u0026=\\delta_{i}^{(l)} a_{j}^{(l-1)} \\end{aligned} $$ 进一步： $$ \\left[\\frac{\\partial \\mathcal{L}(\\boldsymbol{y}, \\hat{\\boldsymbol{y}})}{\\partial \\boldsymbol{W}^{(l)}}\\right]_{i j}=\\left[\\delta^{(l)}\\left(\\boldsymbol{a}^{(l-1)}\\right)^{\\top}\\right]_{i j} $$ 权重梯度： $$ \\frac{\\partial \\mathcal{L}(\\boldsymbol{y}, \\hat{\\boldsymbol{y}})}{\\partial \\boldsymbol{W}^{(l)}}=\\delta^{(l)}\\left(\\boldsymbol{a}^{(l-1)}\\right)^{\\top} \\quad \\in \\mathbb{R}^{M_{l} \\times M_{l-1}} $$ 偏置梯度： $$ \\frac{\\partial \\mathcal{L}(\\boldsymbol{y}, \\hat{\\boldsymbol{y}})}{\\partial \\boldsymbol{b}^{(l)}}=\\delta^{(l)} \\in \\mathbb{R}^{M_{l}} $$ BP训练FNN过程： 前馈计算每一层的净输入 𝒛(𝑙) 和激活值 𝒂(𝑙)，直到最后一层； 反向传播计算每一层的误差项 𝛿(𝑙)； 计算每一层参数的偏导数，并更新参数． ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:4:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.5 自动梯度计算 自动计算梯度的方法三类： 数值微分 符号微分 自动微分 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:5:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.5.1 数值微分（Numerical Differentiation） $$ f^{\\prime}(x)=\\lim _{\\Delta x \\rightarrow 0} \\frac{f(x+\\Delta x)-f(x)}{\\Delta x} $$ 对x加上扰动 Δ𝑥，通过上述定义直接求解。 找到一个合适的扰动 Δ𝑥 十分困难 Δ𝑥 过小，会引起数值计算问题，比如舍入误差 Δ𝑥 过大，会增加截断误差，使得导数计算不准确 实用性差 实际常用： $$ f^{\\prime}(x)=\\lim _{\\Delta x \\rightarrow 0} \\frac{f(x+\\Delta x)-f(x-\\Delta x)}{2 \\Delta x} $$ 数值微分计算复杂度较高 $O\\left(N^{2}\\right)$ ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:5:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.5.2 符号微分（Symbolic Differentiation） 一种基于符号计算的自动求导方法 符号计算也叫代数计算（对应数值计算），是指用计算机来处理带有变量的数学表达式 优点： 可以在编译时计算梯度的属性表示，并利用符号计算方法优化 与平台无关，可以在CPU、GPU上运行 缺点： 编译时间长，尤其是循环 符号微分需要专门语言表示数学表达式，并预先声明变量（符号） 调试困难 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:5:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.5.3 自动微分（Automatic Differentiation，AD） 一种可以对一个（程序）函数进行计算导数的方法 符号微分的处理对象是数学表达式，而自动微分的处理对象是一个函数或一段程序 思想：链式法则计算复合函数梯度 计算图（Computational Graph）：数学运算的图形化表示 $$ \\begin{aligned} \\frac{\\partial f(x ; w, b)}{\\partial w} \u0026=\\frac{\\partial f(x ; w, b)}{\\partial h_{6}} \\frac{\\partial h_{6}}{\\partial h_{5}} \\frac{\\partial h_{5}}{\\partial h_{4}} \\frac{\\partial h_{4}}{\\partial h_{3}} \\frac{\\partial h_{3}}{\\partial h_{2}} \\frac{\\partial h_{2}}{\\partial h_{1}} \\frac{\\partial h_{1}}{\\partial w} \\\\ \\frac{\\partial f(x ; w, b)}{\\partial b} \u0026=\\frac{\\partial f(x ; w, b)}{\\partial h_{6}} \\frac{\\partial h_{6}}{\\partial h_{5}} \\frac{\\partial h_{5}}{\\partial h_{4}} \\frac{\\partial h_{4}}{\\partial h_{3}} \\frac{\\partial h_{3}}{\\partial h_{2}} \\frac{\\partial h_{2}}{\\partial b} \\end{aligned} $$ 分为前向模式和反向模式，反向模式和反向传播的计算梯度的方式相同 自下而上，反向模式遍历每个输出，每次自动微分都求出所有相关节点的一个自变量分量 $x_{i}$ 的导数 $\\frac{d \\cdot}{d x_{i}}$ 自上而下，前向模式遍历每个输入，每次自动求导都是求出函数 $y_{i}(x)$ 关于所有相关节点的导数 $\\frac{d y_{i}}{d \\cdot}$ 计算图构建方式 静态计算图（Static Computational Graph）：编译时构建计算图，构建时可以优化，并行能力强 动态计算图（Dynamic Computational Graph）：运行时构建集散图，灵活性高 符号微分和自动微分 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:5:3","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.6 优化问题 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:6:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.6.1 非凸优化问题 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:6:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.6.2 梯度消失问题 误差反向传播的迭代公式为 $$ \\delta^{(l)}=f_{l}^{\\prime}\\left(z^{(l)}\\right) \\odot\\left(W^{(l+1)}\\right)^{\\top} \\delta^{(l+1)} $$ 每一层都乘以激活函数导数，使用Sigmoid型函数时，其导数： 梯度消失问题（Vanishing Gradient Problem）/梯度弥散问题：Sigmoid型函数导数值域小，两端饱和区导数更是接近于0。这样，误差在传递中不断衰减，网络很深时，梯度过小甚至消失，使得网络很难训练。 解决：使用导数比较大的激活函数，比如 ReLU 等 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:6:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"4.7 总结 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:7:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"习题 习题 4-1 对于一个神经元 𝜎(𝒘T𝒙 + 𝑏)，并使用梯度下降优化参数 𝒘 时，如果输入𝒙 恒大于 0，其收敛速度会比零均值化的输入更慢 Sigmoid型函数在零点处导数最大，收敛最快。 习题 4-2 试设计一个前馈神经网络来解决 XOR 问题，要求该前馈神经网络具有两个隐藏神经元和一个输出神经元，并使用 ReLU 作为激活函数． from keras.optimizers import SGD from keras.layers.core import Dense, Dropout, Activation import numpy as np import tensorflow as tf from keras.layers import Dense from keras.models import Sequential X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) y = np.array([[0], [1], [1], [0]]) model = Sequential() model.add(Dense(units=2, activation='relu', input_dim=2)) model.add(Dense(units=1, activation='sigmoid')) sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9) model.compile(loss='binary_crossentropy', optimizer=sgd) print(model.summary()) print(model.get_weights()) model.fit(X, y, epochs=1000, batch_size=1) print(model.get_weights()) print(model.predict(X, batch_size=1)) 随缘，经常遇到死亡ReLU，换成softplus就好了 习题 4-3 试举例说明“死亡 ReLU 问题”，并提出解决方法． BP中，学习率比较大，对较大的梯度，ReLU神经元更新后的权重和偏置为负，下一轮正向传播时Z为负，ReLU输出a为0，后续反向传播时参数永远不再更新。 解决：Leaky ReLU、PReLU、ELU、Softplus 习题 4-5 如果限制一个神经网络的总神经元数量（不考虑输入层）为 𝑁 + 1，输入层大小为 𝑀，输出层大小为 1，隐藏层的层数为 𝐿，每个隐藏层的神经元数量为𝑁/𝐿 ，试分析参数数量和隐藏层层数 𝐿 的关系 b数量：$N+1$ w数量：$M \\times \\frac{N}{L}+(L-1)(\\frac{N}{L})^2+\\frac{N}{L}$ 习题 4-7 为什么在神经网络模型的结构化风险函数中不对偏置 𝒃 进行正则化？ 正则化降低模型空间/复杂性，防止过拟合。w过大对输入数据敏感，b偏置与特征无关，对所有数据都相同。 习题 4-8 为什么在用反向传播算法进行参数学习时要采用随机参数初始化的方式而不是直接令 𝑾 = 0, 𝒃 = 0？ 对称权重现象：会使每层中的参数相同，不同结点无法学习不同特征 习题 4-9 梯度消失问题是否可以通过增加学习率来缓解？ 可以缓解，不能解决，反倒可能使得深层梯度爆炸 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/:8:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第4章 - 前馈神经网络","uri":"/blog/nndl-book-ch4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"categories":["Deep Learning"],"content":"ch3 线性模型 线性模型：通过样本特征的线性组合来进行预测。其线性组合函数为： $$ \\begin{aligned} f(\\boldsymbol{x} ; \\boldsymbol{w}) \u0026=w_{1} x_{1}+w_{2} x_{2}+\\cdots+w_{D} x_{D}+b \\\\ \u0026=\\boldsymbol{w}^{\\top} \\boldsymbol{x}+b \\end{aligned} $$ 线性回归：直接使用 $y=f(\\boldsymbol{x} ; \\boldsymbol{w})$ 来预测输出目标 分类问题：离散便签，需要引入非线性决策函数（Decision Function） $g(\\cdot)$ 预测输出目标： $y=g(f(\\boldsymbol{x} ; \\boldsymbol{w}))$ $f(\\boldsymbol{x} ; \\boldsymbol{w})$ 也称判别函数（Discriminant Function） 四种线性分类模型（主要区别：不同损失函数） Logistic 回归 Softmax 回归 感知器 支持向量机 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:0:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"3.1 线性判别函数和决策边界 线性分类模型（Linear Classification Model）或线性分类器（Linear Classifier）：一个或多个线性判别函数 + 非线性决策函数 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:1:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"3.1.1 二分类 分割超平面（Hyperplane） / 决策边界（Decision Boundary） / 决策平面（Decision Surface）： $f(\\boldsymbol{x} ; \\boldsymbol{w})=0$ 的点组成的平面 特征空间中每个样本点到决策平面的有向距离（Signed Distance）： $$ \\gamma=\\frac{f(\\boldsymbol{x} ; \\boldsymbol{w})}{|\\boldsymbol{w}|} $$ 线性模型学习目标是尽量满足： $$ y^{(n)} f\\left(\\boldsymbol{x}^{(n)} ; \\boldsymbol{w}^{*}\\right)\u003e0, \\quad \\forall n \\in[1, N] $$ 两类线性可分：训练集的所有样本都满足上式。 学习参数 $\\boldsymbol{w}$，需要定义合适的损失函数和优化方法。 直接采用0-1损失函数： $$ y^{(n)} f\\left(\\boldsymbol{x}^{(n)} ; \\boldsymbol{w}^{*}\\right)\u003e0, \\quad \\forall n \\in[1, N] $$ 存在问题：$\\boldsymbol{w}$ 导数为0，无法优化。 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:1:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"3.1.2 多分类 判别函数： 一对其余： $C$ 个二分类函数 一对一：$C(C-1)/2$ 个二分类函数 argmax：改进的“一对其余”，$C$ 个判别函数 $$ y=\\underset{c=1}{\\arg \\max } f_{c}\\left(\\boldsymbol{x} ; \\boldsymbol{w}_{c}\\right) . $$ “一对其余”和“一对一”存在难以确定区域： 多类线性可分：对训练集，每一类均存在判别函数使得该类下所有样本的当前类判别函数最大。 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:1:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"3.2 Logistic 回归 Logistic 回归（Logistic Regression，LR）：二分类 引入非线性函数 g 预测类别后验概率： $$ p(y=1 \\mid \\boldsymbol{x})=g(f(\\boldsymbol{x} ; \\boldsymbol{w})) $$ $g(\\cdot)$ 称为激活函数（Activation Funtion）：将线性函数值域挤压到 $(0, 1)$ 之间，表示概率。 $g(\\cdot)$ 的逆函数 $g^{-1}(\\cdot)$ 称为联系函数（Link Function） Logistic 回归使用 Logistic 函数作为激活函数。标签y=1的后验概率： $$ \\begin{aligned} p(y=1 \\mid \\boldsymbol{x}) \u0026=\\sigma\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}\\right) \\\\ \u0026 \\triangleq \\frac{1}{1+\\exp \\left(-\\boldsymbol{w}^{\\top} \\boldsymbol{x}\\right)}, \\end{aligned} $$ 变换得到： $$ \\begin{aligned} \\boldsymbol{w}^{\\top} \\boldsymbol{x} \u0026=\\log \\frac{p(y=1 \\mid \\boldsymbol{x})}{1-p(y=1 \\mid \\boldsymbol{x})} \\\\ \u0026=\\log \\frac{p(y=1 \\mid \\boldsymbol{x})}{p(y=0 \\mid \\boldsymbol{x})}, \\end{aligned} $$ 其中 $\\frac{p(y=1 \\mid \\boldsymbol{x})}{p(y=0 \\mid \\boldsymbol{x})}$ 称为几率（Odds），几率的对数称为对数几率（Log Odds，或Logit） 参数学习 损失函数：交叉熵 风险函数： $$ \\begin{aligned} \\mathcal{R}(\\boldsymbol{w})=\u0026-\\frac{1}{N} \\sum_{n=1}^{1 \\mathrm{~N}}\\left(p_{r}\\left(y^{(n)}=1 \\mid \\boldsymbol{x}^{(n)}\\right) \\log \\hat{y}^{(n)}+p_{r}\\left(y^{(n)}=0 \\mid \\boldsymbol{x}^{(n)}\\right) \\log \\left(1-\\hat{y}^{(n)}\\right)\\right) \\\\ \u0026=-\\frac{1}{N} \\sum_{n=1}^{N}\\left(y^{(n)} \\log \\hat{y}^{(n)}+\\left(1-y^{(n)}\\right) \\log \\left(1-\\hat{y}^{(n)}\\right)\\right) . \\end{aligned} $$ 求导： $$ \\begin{aligned} \\frac{\\partial \\mathcal{R}(\\boldsymbol{w})}{\\partial \\boldsymbol{w}} \u0026=-\\frac{1}{N} \\sum_{n=1}^{N}\\left(y^{(n)} \\frac{\\hat{y}^{(n)}\\left(1-\\hat{y}^{(n)}\\right)}{\\hat{y}^{(n)}} \\boldsymbol{x}^{(n)}-\\left(1-y^{(n)}\\right) \\frac{\\hat{y}^{(n)}\\left(1-\\hat{y}^{(n)}\\right)}{1-\\hat{y}^{(n)}} \\boldsymbol{x}^{(n)}\\right) \\\\ \u0026=-\\frac{1}{N} \\sum_{n=1}^{N}\\left(y^{(n)}\\left(1-\\hat{y}^{(n)}\\right) \\boldsymbol{x}^{(n)}-\\left(1-y^{(n)}\\right) \\hat{y}^{(n)} \\boldsymbol{x}^{(n)}\\right) \\\\ \u0026=-\\frac{1}{N} \\sum_{n=1}^{N} \\boldsymbol{x}^{(n)}\\left(y^{(n)}-\\hat{y}^{(n)}\\right) \\end{aligned} $$ 梯度下降法参数更新： $$ \\boldsymbol{w}{t+1} \\leftarrow \\boldsymbol{w}{t}+\\alpha \\frac{1}{N} \\sum_{n=1}^{N} \\boldsymbol{x}^{(n)}\\left(y^{(n)}-\\hat{y}{\\boldsymbol{w}{t}}^{(n)}\\right) $$ 因为风险函数 $\\mathcal{R}(\\boldsymbol{w})$ 是关于参数的连续可导凸函数，所以还可以用更高阶的优化方法（如牛顿法）来优化。 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:2:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"3.3 Softmax 回归 Sotfmax回归（Softmax Regression）：即多项或多类的 Logistic 回归。 预测类别： $$ \\begin{aligned} p(y=c \\mid \\boldsymbol{x}) \u0026=\\operatorname{softmax} (\\boldsymbol{w}_{c}^{\\top} \\boldsymbol{x}) \\\\ \u0026=\\frac{\\exp \\left(\\boldsymbol{w}_{c}^{\\top} \\boldsymbol{x}\\right)}{\\sum_{c^{\\prime}=1}^{C} \\exp \\left(\\boldsymbol{w}_{c^{\\prime}}^{\\top} \\boldsymbol{x}\\right)} \\end{aligned} $$ 向量化表示： $$ \\begin{aligned} \\hat{\\boldsymbol{y}} \u0026=\\operatorname{softmax}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{x}\\right) \\\\ \u0026=\\frac{\\exp \\left(\\boldsymbol{W}^{\\top} \\boldsymbol{x}\\right)}{\\mathbf{1}_{C}^{\\mathrm{T}} \\exp \\left(\\boldsymbol{W}^{\\top} \\boldsymbol{x}\\right)} \\end{aligned} $$ 决策函数： $$ \\begin{aligned} \\hat{y} \u0026=\\underset{c=1}{\\arg \\max } p(y=c \\mid \\boldsymbol{x}) \\\\ \u0026=\\underset{c=1}{\\arg \\max } \\boldsymbol{w}_{c}^{\\top} \\boldsymbol{x} . \\end{aligned} $$ 风险函数： $$ \\begin{aligned} \\mathcal{R}(\\boldsymbol{W}) \u0026=-\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{c=1}^{C} \\boldsymbol{y}{c}^{(n)} \\log \\hat{\\boldsymbol{y}}{c}^{(n)} \\\\ \u0026=-\\frac{1}{N} \\sum_{n=1}^{N}\\left(\\boldsymbol{y}^{(n)}\\right)^{\\mathrm{T}} \\log \\hat{\\boldsymbol{y}}^{(n)} \\end{aligned} $$ Softmax 函数求导： $$ \\begin{aligned} \u0026\\frac{\\partial \\operatorname{softmax}(\\boldsymbol{x})}{\\partial \\boldsymbol{x}}=\\frac{\\partial\\left(\\frac{\\exp (x)}{1_{K}^{\\top} \\exp (x)}\\right)}{\\partial \\boldsymbol{x}} \\\\ \u0026=\\frac{1}{\\mathbf{1}{K}^{\\top} \\exp (\\boldsymbol{x})} \\frac{\\partial \\exp (\\boldsymbol{x})}{\\partial \\boldsymbol{x}}+\\frac{\\partial\\left(\\frac{1}{1{K}^{\\mathrm{T} \\exp (x)}}\\right)}{\\partial \\boldsymbol{x}}(\\exp (\\boldsymbol{x}))^{\\top} \\\\ \u0026=\\frac{\\operatorname{diag}(\\exp (\\boldsymbol{x}))}{\\mathbf{1}{K}^{\\top} \\exp (\\boldsymbol{x})}-\\left(\\frac{1}{\\left(\\mathbf{1}{K}^{\\mathrm{T}} \\exp (\\boldsymbol{x})\\right)^{2}}\\right) \\frac{\\partial\\left(\\mathbf{1}{K}^{\\top} \\exp (\\boldsymbol{x})\\right)}{\\partial \\boldsymbol{x}}(\\exp (\\boldsymbol{x}))^{\\top} \\\\ \u0026=\\frac{\\operatorname{diag}(\\exp (\\boldsymbol{x}))}{\\mathbf{1}{K}^{\\top} \\exp (\\boldsymbol{x})}-\\left(\\frac{1}{\\left(\\mathbf{1}{K}^{\\mathrm{T}} \\exp (\\boldsymbol{x})\\right)^{2}}\\right) \\operatorname{diag}(\\exp (\\boldsymbol{x})) \\mathbf{1}{K}(\\exp (\\boldsymbol{x}))^{\\top} \\\\ \u0026=\\frac{\\operatorname{diag}(\\exp (\\boldsymbol{x}))}{\\mathbf{1}{K}^{\\top} \\exp (\\boldsymbol{x})}-\\left(\\frac{1}{\\left(\\mathbf{1}{K}^{\\top} \\exp (\\boldsymbol{x})\\right)^{2}}\\right) \\exp (\\boldsymbol{x})(\\exp (\\boldsymbol{x}))^{\\top} \\\\ \u0026=\\operatorname{diag}\\left(\\frac{\\exp (\\boldsymbol{x})}{\\mathbf{1}{K}^{\\mathrm{T}} \\exp (\\boldsymbol{x})}\\right)-\\frac{\\exp (\\boldsymbol{x})}{\\mathbf{1}{K}^{\\top} \\exp (\\boldsymbol{x})} \\frac{(\\exp (\\boldsymbol{x}))^{\\mathrm{T}}}{\\mathbf{1}_{K}^{\\mathrm{T}} \\exp (\\boldsymbol{x})} \\\\ \u0026=\\operatorname{diag}(\\operatorname{softmax}(\\boldsymbol{x}))-\\operatorname{softmax}(\\boldsymbol{x}) \\operatorname{softmax}(\\boldsymbol{x})^{\\top} . \\end{aligned} $$ 即若 $y=\\operatorname{softmax}(z)$，则 $\\frac{\\partial y}{\\partial z}=\\operatorname{diag}(y)-y y^{\\top}$ 所以风险函数求梯度为： $$ \\frac{\\partial \\mathcal{R}(\\boldsymbol{W})}{\\partial \\boldsymbol{W}}=-\\frac{1}{N} \\sum_{n=1}^{N} \\boldsymbol{x}^{(n)}\\left(\\boldsymbol{y}^{(n)}-\\hat{\\boldsymbol{y}}^{(n)}\\right)^{\\top} $$ 梯度下降法更新： $$ \\boldsymbol{W}_{t+1} \\leftarrow \\boldsymbol{W}_{t}+\\alpha\\left(\\frac{1}{N} \\sum_{n=1}^{N} \\boldsymbol{x}^{(n)}\\left(\\boldsymbol{y}^{(n)}-\\hat{\\boldsymbol{y}}_{W_{t}}^{(n)}\\right)^{\\top}\\right) $$ ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:3:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"3.4 感知器 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:4:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"3.4.1 参数学习 感知器（Perceptron） 分类准则： $$ \\hat{y}=\\operatorname{sgn}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}\\right) $$ 学习目标，找到参数使得： $$ y^{(n)} \\boldsymbol{w}^{* \\top} \\boldsymbol{x}^{(n)}\u003e0, \\quad \\forall n \\in{1, \\cdots, N} $$ 感知器的学习算法：错误驱动的在线学习算法 [Rosenblatt, 1958]，每错分一个样本，就用该样本更新权重： $$ \\boldsymbol{w} \\leftarrow \\boldsymbol{w}+y \\boldsymbol{x} $$ 损失函数： $$ \\mathcal{L}(\\boldsymbol{w} ; \\boldsymbol{x}, y)=\\max \\left(0,-y \\boldsymbol{w}^{\\top} \\boldsymbol{x}\\right) $$ 梯度更新： $$ \\frac{\\partial \\mathcal{L}(\\boldsymbol{w} ; \\boldsymbol{x}, y)}{\\partial \\boldsymbol{w}}=\\left{\\begin{array}{lll} 0 \u0026 \\text { if } \u0026 y \\boldsymbol{w}^{\\top} \\boldsymbol{x}\u003e0 \\\\ -y \\boldsymbol{x} \u0026 \\text { if } \u0026 y \\boldsymbol{w}^{\\top} \\boldsymbol{x}\u003c0 \\end{array}\\right. $$ 感知器参数学习过程： （黑色：当前权重向量，红色虚线：更新方向） ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:4:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"3.4.2 感知器的收敛性 在数据集线性可分时，感知器可以找到一个超平面把两类数据分开，但并不能保证其泛化能力． 感知器对样本顺序比较敏感．每次迭代的顺序不一致时，找到的分割超平面也往往不一致． 如果训练集不是线性可分的，就永远不会收敛． ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:4:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"3.4.3 参数平均感知器 投票感知器（Voted Perceptron）：感知器收单个样本影响大，为提高鲁棒性和泛化能力，将所有 K 个权重用置信系数加权平均起来，投票决定结果。 置信系数 $c_{k}$ 设置为当前更新权重后直到下一次更新的迭代次数。则投票感知器为： $$ \\hat{y}=\\operatorname{sgn}\\left(\\sum_{k=1}^{K} c_{k} \\operatorname{sgn}\\left(\\boldsymbol{w}_{k}^{\\top} \\boldsymbol{x}\\right)\\right) $$ 平均感知器（Averaged Perceptron）[Collins, 2002]： $$ \\begin{aligned} \\hat{y} \u0026=\\operatorname{sgn}\\left(\\frac{1}{T} \\sum_{k=1}^{K} c_{k}\\left(\\boldsymbol{w}{k}^{\\top} \\boldsymbol{x}\\right)\\right) \\\\ \u0026=\\operatorname{sgn}\\left(\\frac{1}{T}\\left(\\sum{k=1}^{K} c_{k} \\boldsymbol{w}{k}\\right)^{\\top} \\boldsymbol{x}\\right) \\\\ \u0026=\\operatorname{sgn}\\left(\\left(\\frac{1}{T} \\sum{t=1}^{T} \\boldsymbol{w}_{t}\\right)^{\\top} \\boldsymbol{x}\\right) \\\\ \u0026=\\operatorname{sgn}\\left(\\overline{\\boldsymbol{w}}^{\\top} \\boldsymbol{x}\\right) \\end{aligned} $$ ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:4:3","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"3.4.4 扩展到多分类 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:4:4","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"3.5 支持向量机 支持向量机（Support Vector Machine，SVM）：经典二分类算法，找到的超平面具有更好的鲁棒性。 $$ y_{n} \\in{+1,-1} $$ 超平面： $$ \\boldsymbol{w}^{\\top} \\boldsymbol{x}+b=0 $$ 每个样本到分割超平面的距离： $$ \\gamma^{(n)}=\\frac{\\left|\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}+b\\right|}{|\\boldsymbol{w}|}=\\frac{y^{(n)}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}+b\\right)}{|\\boldsymbol{w}|} $$ 间隔（Margin）：数据集中所有样本到分割超平面的最短距离： $$ \\gamma=\\min _{n} \\gamma^{(n)} $$ SVM的目标： $$ \\begin{array}{ll} \\max _{\\boldsymbol{w}, b} \u0026 \\gamma \\\\ \\text { s.t. } \u0026 \\frac{y^{(n)}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}+b\\right)}{|\\boldsymbol{w}|} \\geq \\gamma, \\forall n \\in{1, \\cdots, N} . \\end{array} $$ 由于 $\\boldsymbol{w}$ 和 $b$ 可以同时缩放不改变间隔，可以限制 $|\\boldsymbol{w}| \\cdot \\gamma=1$ ，则上式等价于： $$ \\begin{aligned} \\max _{\\boldsymbol{w}, b} \u0026 \\frac{1}{|\\boldsymbol{w}|^{2}} \\\\ \\text { s.t. } \u0026 y^{(n)}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}+b\\right) \\geq 1, \\forall n \\in{1, \\cdots, N} \\end{aligned} $$ 支持向量（Suport Vector）： 满足 $y^{(n)}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}+b\\right)=1$ 的样本点。 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:5:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"3.5.1 参数学习 将目标函数写成凸优化问题，采用拉格朗日乘数法，并得到拉格朗日对偶函数，采用序列最小优化（Sequential Minimal Optimization, SMO）等高效算法进行优化。 最优参数的SVM决策函数： $$ \\begin{aligned} f(\\boldsymbol{x}) \u0026=\\operatorname{sgn}\\left(\\boldsymbol{w}^{* \\top} \\boldsymbol{x}+b^{}\\right) \\\\ \u0026=\\operatorname{sgn}\\left(\\sum_{n=1}^{N} \\lambda_{n}^{} y^{(n)}\\left(\\boldsymbol{x}^{(n)}\\right)^{\\top} \\boldsymbol{x}+b^{*}\\right) \\end{aligned} $$ ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:5:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"3.5.2 核函数 SVM可以使用核函数（Kernel Function）隐式地将样本从延时特征空间映射到更高维的空间，解决原始特征空间中线性不可分的问题。 则决策函数为： $$ \\begin{aligned} f(\\boldsymbol{x}) \u0026=\\operatorname{sgn}\\left(\\boldsymbol{w}^{*} \\boldsymbol{\\phi}(\\boldsymbol{x})+b^{*}\\right) \\\\ \u0026=\\operatorname{sgn}\\left(\\sum_{n=1}^{N} \\lambda_{n}^{*} y^{(n)} k\\left(\\boldsymbol{x}^{(n)}, \\boldsymbol{x}\\right)+b^{*}\\right) \\end{aligned} $$ $k(\\boldsymbol{x}, \\boldsymbol{z})=\\phi(\\boldsymbol{x})^{\\top} \\phi(\\boldsymbol{z})$ 为核函数，通常不需要显示给出 $\\phi(\\boldsymbol{x})$ 的具体形式，可以通过核技巧（Kernel Trick）来构造，比如构造： $$ k(\\boldsymbol{x}, \\boldsymbol{z})=\\left(1+\\boldsymbol{x}^{\\top} \\boldsymbol{z}\\right)^{2}=\\phi(\\boldsymbol{x})^{\\top} \\phi(\\boldsymbol{z}) $$ 来隐式地计算 $\\boldsymbol{x, z}$ 在特征空间 $\\phi$ 中的内积，其中： $$ \\phi(\\boldsymbol{x})=\\left[1, \\sqrt{2} x_{1}, \\sqrt{2} x_{2}, \\sqrt{2} x_{1} x_{2}, x_{1}^{2}, x_{2}^{2}\\right]^{\\top} $$ ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:5:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"3.5.3 软间隔 当线性不可分时，为了容忍部分不满足约束的样本，引入松弛变量（Slack Variable）$\\xi$，将优化问题变为 $$ \\begin{array}{ll} \\min _{\\boldsymbol{w}, b} \u0026 \\frac{1}{2}|\\boldsymbol{w}|^{2}+C \\sum_{n=1}^{N} \\xi_{n} \\\\ \\text { s.t. } \u0026 1-y^{(n)}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}+b\\right)-\\xi_{n} \\leq 0, \\quad \\forall n \\in{1, \\cdots, N} \\\\ \u0026 \\xi_{n} \\geq 0, \\quad \\forall n \\in{1, \\cdots, N} \\end{array} $$ 参数 $C \u003e 0$ 控制间隔和松弛变量之间和平衡，引入松弛变量的间隔称为软间隔（Soft Margin）。 上式也可以表示为 经验风险 + 正则化项 的形式： $$ \\min _{\\boldsymbol{w}, b} \\quad \\sum_{n=1}^{N} \\max \\left(0,1-y^{(n)}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}+b\\right)\\right)+\\frac{1}{2 C}|\\boldsymbol{w}|^{2} $$ 前面一项可以看作 Hinge损失函数，后一项看作正则项，$\\frac{1}{c}$ 为正则化系数。 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:5:3","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"3.6 损失函数对比 统一定义标签： $$ y \\in{+1,-1} $$ 决策函数： $$ f(\\boldsymbol{x} ; \\boldsymbol{w})=\\boldsymbol{w}^{\\top} \\boldsymbol{x}+b $$ 平方损失函数其实也可以用于分类问题的 loss 函数，但本质上等同于误差服从高斯分布假设下的极大似然估计，而分类问题大部分时候不服从高斯分布。 直观上理解，标签之间的距离没有意义，预测值和标签之间的距离不能反应问题优化程度。 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:6:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"习题 习题 3-1 证明在两类线性分类中，权重向量𝒘 与决策平面正交． 判别函数： $$ f(x)=w^{T} * x+w_{0} $$ 决策平面： $$ f(x)=w^{T} * x+w_{0}=0 $$ $w^T$ 平面法向量，任取平面两点构成线段均垂直于法向量。 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:7:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"习题 3-2 在线性空间中，证明一个点 𝒙 到平面 𝑓(𝒙; 𝒘) = 𝒘T𝒙 + 𝑏 = 0 的距离为 |𝑓(𝒙; 𝒘)|/‖𝒘‖ 点到面距离计算：任取平面一点与该点构成直线 $AB$ ，距离即是 $AB$ 在法向量 $\\boldsymbol{w}$ 上的投影。 $$ |\\mathrm{AC}|=\\left|\\overrightarrow{\\mathrm{AB}} \\cdot \\frac{\\overrightarrow{\\mathrm{n}}}{|\\overrightarrow{\\mathrm{n}}|}\\right| $$ 点积展开计算、消去0项即可。 ","date":"2021-01-22","objectID":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/:7:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第3章 - 线性模型","uri":"/blog/nndl-book-ch3-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"},{"categories":["Deep Learning"],"content":"2 机器学习概述 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:0:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"2.1 基本概念 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:1:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"2.2 机器学习三要素 模型 线性 非线性 学习准则 损失函数 经验风险最小化（Empirical Risk Minimization, ERM） $\\mathcal{R}_{\\mathcal{D}}^{e m p}(\\theta)=\\frac{1}{N} \\sum_{n=1}^{N} \\mathcal{L}\\left(y^{(n)}, f\\left(\\boldsymbol{x}^{(n)} ; \\theta\\right)\\right)$ $\\theta^{*}=\\underset{\\theta}{\\arg \\min } \\mathcal{R}_{\\mathcal{D}}^{e m p}(\\theta)$ 结构风险最小化（Structure Risk Minimization, SRM） 加入正则项，限制模型能力 优化算法 参数和超参数 梯度下降 提前停止 BGD、SGD、mini-batch ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:2:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"2.3 示例：线性回归 线性模型：$f(\\boldsymbol{x} ; \\boldsymbol{w}, b)=\\boldsymbol{w}^{\\top} \\boldsymbol{x}+b$ 增广权重和特征向量后：$f(\\boldsymbol{x} ; \\hat{\\boldsymbol{w}})=\\hat{\\boldsymbol{w}}^{\\top} \\hat{\\boldsymbol{x}}$ 参数估计 经验风险最小化 结构风险最小化 最大似然估计 最大后验估计 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:3:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"（1）经验风险最小化 经验风险： $$\\begin{aligned} \\mathcal{R}(\\boldsymbol{w}) \u0026=\\sum_{n=1}^{N} \\mathcal{L}\\left(y^{(n)}, f\\left(\\boldsymbol{x}^{(n)} ; \\boldsymbol{w}\\right)\\right) \\\\ \u0026=\\frac{1}{2} \\sum_{n=1}^{N}\\left(y^{(n)}-\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}\\right)^{2} \\\\ \u0026=\\frac{1}{2}\\left|\\boldsymbol{y}-\\boldsymbol{X}^{\\top} \\boldsymbol{w}\\right|^{2} \\end{aligned}$$ 偏导数： $$\\begin{aligned} \\frac{\\partial \\mathcal{R}(\\boldsymbol{w})}{\\partial \\boldsymbol{w}} \u0026=\\frac{1}{2} \\frac{\\partial\\left|\\boldsymbol{y}-\\boldsymbol{X}^{\\top} \\boldsymbol{w}\\right|^{2}}{\\partial \\boldsymbol{w}} \\\\ \u0026=-\\boldsymbol{X}\\left(\\boldsymbol{y}-\\boldsymbol{X}^{\\top} \\boldsymbol{w}\\right) \\end{aligned}$$ 令偏导数为0，求得最优参数： $$\\begin{aligned} \\boldsymbol{w}^{*} \u0026=\\left(\\boldsymbol{X} \\boldsymbol{X}^{\\mathrm{T}}\\right)^{-1} \\boldsymbol{X} \\boldsymbol{y} \\\\ \u0026=\\left(\\sum_{n=1}^{N} \\boldsymbol{x}^{(n)}\\left(\\boldsymbol{x}^{(n)}\\right)^{\\top}\\right)^{-1}\\left(\\sum_{n=1}^{N} \\boldsymbol{x}^{(n)} y^{(n)}\\right) . \\end{aligned}$$ 这种求解线性回归参数的方法称为最小二乘法（Least Square Method, LSM） LSM要求$\\boldsymbol{X X}^{\\mathrm{T}} \\in \\mathbb{R}^{(D+1) \\times(D+1)}$可逆，即满秩，即行向量线性无关，即每个特征与其他特征无关。常见的不可逆情况是样本数量N小于特征数量(D+1)，$\\boldsymbol{X X}^{\\mathrm{T}}$秩为N，存在多解。 $\\boldsymbol{X X}^{\\mathrm{T}}$不可逆解决： 先用主成分分析等方法预处理数据，消除不同特征的相关性，再用最小二乘法 用梯度下降法估计参数，初始化$\\boldsymbol{w}=0$，迭代： $$\\boldsymbol{w} \\leftarrow \\boldsymbol{w}+\\alpha \\boldsymbol{X}\\left(\\boldsymbol{y}-\\boldsymbol{X}^{\\top} \\boldsymbol{w}\\right)$$ 也称最小均方算法（Least Mean Squares, LMS) ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:3:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"（2）结构风险最小化 最小二乘法要求特征相互独立，但即使独立，如果特征之间有较大多重共线性（Multicollinearity，即其他特征线性组合预测某一特征），也会使得$\\boldsymbol{X X}^{\\top}$的逆数值无法准确计算。数据集X上的小扰动就会导致逆发生大的改变，结果不稳定。 解决：[Hoerl et al., 1970] 岭回归（Ridge Regression） 给$X X^{\\top}$对角元素加上常数$\\lambda$使得$\\left(\\boldsymbol{X X}^{\\top}+\\lambda I\\right)$满秩（？），最优参数为： $$\\boldsymbol{w}^{*}=\\left(\\boldsymbol{X} \\boldsymbol{X}^{\\top}+\\lambda I\\right)^{-1} \\boldsymbol{X} \\boldsymbol{y}$$ 岭回归的解$\\boldsymbol{w}^*$可以看作结构风险最小化准则下的最小二乘估计，目标函数可以写为： $$\\mathcal{R}(\\boldsymbol{w})=\\frac{1}{2}\\left|\\boldsymbol{y}-\\boldsymbol{X}^{\\top} \\boldsymbol{w}\\right|^{2}+\\frac{1}{2} \\lambda|\\boldsymbol{w}|^{2}$$ ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:3:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"（3）最大似然估计 概率表达了给定$\\theta$下随机变量$\\mathbf{X}=\\mathbf{x}$的可能性，似然表达了给定样本$\\mathbf{X}=\\mathbf{x}$下参数$\\theta_1$（相对于参数$\\theta_2$）为真实值的可能性 似然函数定义：$L(\\theta \\mid \\mathbf{x})=f(\\mathbf{x} \\mid \\theta)$，严格记号竖线|表示条件概率/分布，分号;隔开参数，则该式子严格书写应为：$L(\\theta \\mid \\mathbf{x})=f(\\mathbf{x} ; \\theta)$ y随机变量为函数加噪声： $$y=f(\\boldsymbol{x} ; \\boldsymbol{w})+\\epsilon=\\boldsymbol{w}^{\\top} \\boldsymbol{x}+\\epsilon$$ 其中$\\epsilon$服从高斯分布，则y服从高斯分布： $$\\begin{aligned} p(y \\mid \\boldsymbol{x} ; \\boldsymbol{w}, \\sigma) \u0026=\\mathcal{N}\\left(y ; \\boldsymbol{w}^{\\top} \\boldsymbol{x}, \\sigma^{2}\\right) \\\\ \u0026=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{\\left(y-\\boldsymbol{w}^{\\top} \\boldsymbol{x}\\right)^{2}}{2 \\sigma^{2}}\\right) . \\end{aligned}$$ 参数$\\boldsymbol{w}$在训练集上的似然函数（Likelihood）： $$\\begin{aligned} p(\\boldsymbol{y} \\mid \\boldsymbol{X} ; \\boldsymbol{w}, \\sigma) \u0026=\\prod_{n=1}^{N} p\\left(y^{(n)} \\mid \\boldsymbol{x}^{(n)} ; \\boldsymbol{w}, \\sigma\\right) \\\\ \u0026=\\prod_{n=1}^{N} \\mathcal{N}\\left(y^{(n)} ; \\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}, \\sigma^{2}\\right), \\end{aligned}$$ 方便计算取对数似然函数（Log Likelihood）： $$\\log p(\\boldsymbol{y} \\mid \\boldsymbol{X} ; \\boldsymbol{w}, \\sigma)=\\sum_{n=1}^{N} \\log \\mathcal{N}\\left(y^{(n)} ; \\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}, \\sigma^{2}\\right)$$ 最大似然估计（Maximum Likelihood Estimation，MLE）指找到参数$\\boldsymbol{w}$使得似然函数最大，令偏导数为0得到： $$\\boldsymbol{w}^{M L}=\\left(\\boldsymbol{X} \\boldsymbol{X}^{\\top}\\right)^{-1} \\boldsymbol{X} \\boldsymbol{y}$$ 与最小二乘法解相同。 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:3:3","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"（4）最大后验估计 为了避免最大似然估计因数据较少而过拟合，给参数分布加上先验知识（如符合各向同性高斯分布） 最大后验估计（Maximum A Posteriori Estimation，MAP）是指最优参数为后验分布 𝑝(𝒘|𝑿, 𝒚; 𝜈, 𝜎) 中概率密度最高的参数： $$\\boldsymbol{w}^{M A P}=\\underset{\\boldsymbol{w}}{\\arg \\max } p(\\boldsymbol{y} \\mid \\boldsymbol{X}, \\boldsymbol{w} ; \\sigma) p(\\boldsymbol{w} ; \\nu)$$ $$\\begin{aligned} \\log p(\\boldsymbol{w} \\mid \\boldsymbol{X}, \\boldsymbol{y} ; \\nu, \\sigma) \u0026 \\propto \\log p(\\boldsymbol{y} \\mid \\boldsymbol{X}, \\boldsymbol{w} ; \\sigma)+\\log p(\\boldsymbol{w} ; v) \\\\ \u0026 \\propto-\\frac{1}{2 \\sigma^{2}} \\sum_{n=1}^{N}\\left(y^{(n)}-\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}\\right)^{2}-\\frac{1}{2 v^{2}} \\boldsymbol{w}^{\\top} \\boldsymbol{w} \\\\ \u0026=-\\frac{1}{2 \\sigma^{2}}\\left|\\boldsymbol{y}-\\boldsymbol{X}^{\\top} \\boldsymbol{w}\\right|^{2}-\\frac{1}{2 v^{2}} \\boldsymbol{w}^{\\top} \\boldsymbol{w} \\end{aligned}$$ 等价于平方损失的结构风险最小化。 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:3:4","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"2.4 偏差-方差分解（Bias-Variance Decomposition） $$\\mathbb{E}_{\\mathcal{D}}\\left[\\left(f_{\\mathcal{D}}(\\boldsymbol{x})-f^{*}(\\boldsymbol{x})\\right)^{2}\\right]$$ $$\\quad=\\mathbb{E}_{\\mathcal{D}}\\left[\\left(f_{\\mathcal{D}}(\\boldsymbol{x})-\\mathbb{E}_{\\mathcal{D}}\\left[f_{\\mathcal{D}}(\\boldsymbol{x})\\right]+\\mathbb{E}_{\\mathcal{D}}\\left[f_{\\mathcal{D}}(\\boldsymbol{x})\\right]-f^{*}(\\boldsymbol{x})\\right)^{2}\\right]$$ $$\\quad=\\underbrace{\\left(\\mathbb{E}_{\\mathcal{D}}\\left[f_{\\mathcal{D}}(\\boldsymbol{x})\\right]-f^{*}(\\boldsymbol{x})\\right)^{2}}_{\\text {(bias.} \\mathrm{x})^{2}}+\\underbrace{\\mathbb{E}_{\\mathcal{D}}\\left[\\left(f_{\\mathcal{D}}(\\boldsymbol{x})-\\mathbb{E}_{\\mathcal{D}}\\left[f_{\\mathcal{D}}(\\boldsymbol{x})\\right]\\right)^{2}\\right]}_{\\text {variance.} \\mathrm{x}},$$ ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:4:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"2.5 机器学习算法的类型 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:5:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"2.6 数据的特征表示 图像特征：简单表示为$M\\times N$维向量。为提高准确率，也常常加入额外特征，如直方图、宽高比、纹理特征、边缘特征等。假设总共抽取了D个特征，则这些特征可以表示为一个向量$x \\in \\mathbb{R}^{D}$ 文本特征： BOW N-Gram 直接使用原始特征进行学习，对模型能力要求高，原始特征存在很多不足。 特征工程（Feature Engineering）：人工特征提取 特征学习（Feature Learning）/表示学习（Representation Learning）：机器自动学习有效的特征 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:6:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"2.6.1 传统的特征学习 特征选择（Feature Selection） 选取特征子集 子集搜索： 暴力：搜索全部$2^{D}$个候选子集 贪心： 前向搜索（Forward Search）：空集开始，每一轮加入最优特征 反向搜索（Backward Search）：原始开始，每一轮删除最无用特征 子集搜索方法分类： 过滤式方法（Filter Method）：不依赖于具体机器学习模型，每次增加最有信息量的特征，或删除最没有信息量的特征。通过信息增益（Information Gain）衡量。 包裹式方法（Wrapper Method）：使用后续机器学习模型的准确率作为评价，每次增加最有用特征，或删除最无用特征。将机器学习模型包裹到特征选择过程内部 $\\ell_{1}$ 正则化：也可以实现特征选择，因为$\\ell_{1}$ 正则化导致稀疏特征，间接实现了特征选择 特征抽取（Feature Extraction） 构造新的特征空间，将原始特征投影在新的空间中得到新的表示 特征抽取分类： 监督：抽取对特性任务最有用的特征，eg. 线性判别分析（Linear Discriminant Analysis，LDA） 无监督：与具体任务无关，目的通常是减少冗余信息和噪声，eg. 主成分分析（Principal Component Analysis，PCA）和自编码器（Auto-Encoder，AE） 特征选择和特征抽取的优点：用较少特征表示原始特征大部分信息，去掉噪声信息，并进而调高计算效率和减小维度灾难（Curse of Dimensionality）。因为特征选择或抽取后一般特征数量会减少，也经常称为维数约减或降维（Dimension Reduction）。 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:6:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"2.6.2 深度学习方法 传统的特征抽取一般是和预测模型的学习分离的 深度学习：表示学习和预测学习有机统一，端到端。难点是如何评价表示学习对最终系统输出结果的贡献或影响，即贡献度分配问题。目前比较有效的模型是神经网络，将最后的输出层作为预测学习，其他层作为表示学习 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:6:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"2.7 评价指标 准确率（Accuracy）：$\\mathcal{A}=\\frac{1}{N} \\sum_{n=1}^{N} I\\left(y^{(n)}=\\hat{y}^{(n)}\\right)$ 错误率（Error Rate）：$\\begin{aligned} \\mathcal{E} \u0026=1-\\mathcal{A} \\\\ \u0026=\\frac{1}{N} \\sum_{n=1}^{N} I\\left(y^{(n)} \\neq \\hat{y}^{(n)}\\right) \\end{aligned}$ 精确率/精度/查准率（Precision）：$\\mathcal{P}_{c}=\\frac{T P_{c}}{T P_{c}+F P_{c}}$ 召回率（Recall）/查全率：$\\mathcal{R}_{c}=\\frac{T P_{c}}{T P_{c}+F N_{c}}$ F值（F Measure）：$\\mathcal{F}_{c}=\\frac{\\left(1+\\beta^{2}\\right) \\times \\mathcal{P}_{c} \\times \\mathcal{R}_{c}}{\\beta^{2} \\times \\mathcal{P}_{c}+\\mathcal{R}_{c}}$ 宏平均（Macro Average）：每一类的性能指标的算术平均值 $\\begin{aligned} \\mathcal{P}_{\\text {macro }} \u0026=\\frac{1}{C} \\sum_{c=1}^{C} \\mathcal{P}_{c} \\\\ \\mathcal{R}_{\\text {macro }} \u0026=\\frac{1}{C} \\sum_{c=1}^{C} \\mathcal{R}_{c} \\\\ \\mathcal{F} 1_{\\text {macro }} \u0026=\\frac{2 \\times \\mathcal{P}_{\\text {macro }} \\times R_{\\text {macro }}}{P_{\\text {macro }}+R_{\\text {macro }}} . \\end{aligned}$ 微平均（Micro Average）：每一个样本的性能指标的算术平均值 不同类别的样本数量不均衡时，使用宏平均比微平均更合理，因为宏平均更关注小类别上的评价指标 在实际应用中，我们也可以通过调整分类模型的阈值来进行更全面的评价，比如 AUC（Area Under Curve）、ROC（Receiver Operating Characteristic）曲线、PR（Precision-Recall）曲线等． 此外，很多任务还有自己专门的评价方式，比如TopN 准确率． 交叉验证（Cross-Validation）：K组，轮流一组测试，其他训练，求平均 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:7:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"2.8 理论和定理 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:8:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"2.8.1 PAC 学习理论 计算学习理论（Computational Learning Theory）是机器学习的理论基础：分析问题难度、计算模型能力，为学习算法提供理论保证，并指导机器学习模型和学习算法的设计 其中最基础的理论就是可能近似正确（Probably Approximately Correct，PAC）学习理论． 泛化错误（Generalization Error）：期望错误与经验错误之间的差异，可以衡量模型是否可以很好地泛化到未知数据 $$\\mathcal{G}_{\\mathcal{D}}(f)=\\mathcal{R}(f)-\\mathcal{R}_{\\mathcal{D}}^{e m p}(f)$$ 根据大数定律，训练集趋于无穷大时，经验风险趋近于期望风险，泛化错误趋向于0：$\\lim _{|\\mathcal{D}| \\rightarrow \\infty} \\mathcal{R}(f)-\\mathcal{R}_{\\mathcal{D}}^{e m p}(f)=0$ PAC学习（PAC Learning）：因为不知道真实数据分布、目标函数，需要降低学习算法能力期望，只要求算法以一定概率学习到一个近似正确的假设 近似正确（Approximately Correct）：泛化错误 𝒢𝒟(𝑓) 小于一个界限 𝜖 可能（Probably）：一个学习算法𝒜 有“可能”以 1−𝛿 的概率学习到这样一个“近似正确”的假设 PAC可学习（PAC-Learnable）的算法：该学习算法能够在多项式时间内从合理数量的训练数据中学习到一个近似正确的𝑓(𝒙) PAC学习的公式描述：$P\\left(\\left(\\mathcal{R}(f)-\\mathcal{R}_{\\mathcal{D}}^{e m p}(f)\\right) \\leq \\epsilon\\right) \\geq 1-\\delta$ 其中 𝜖,𝛿 是和样本数量 𝑁 以及假设空间$\\mathcal{F}$相关的变量．如果固定 𝜖,𝛿，可以反过来计算出需要的样本数量： $$N(\\epsilon, \\delta) \\geq \\frac{1}{2 \\epsilon^{2}}\\left(\\log |\\mathcal{F}|+\\log \\frac{2}{\\delta}\\right)$$ 可以看到，模型越复杂，即假设空间$\\mathcal{F}$越大， 模型泛化能力越差为了提高模型的泛化能力，通常需要正则化（Regularization）来限制模型复杂度． ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:8:1","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"2.8.2 没有免费午餐定理（No Free Lunch Theorem，NFL） Wolpert 和 Macerday 在最优化理论提出：对于基于迭代的最优化算法，不存在某种算法对所有问题（有限的搜索空间内）都有效 如果一个算法对某些问题有效，那么它一定在另外一些问题上比纯随机搜索算法更差 也就是说，不能脱离具体问题来谈论算法的优劣，任何算法都有局限性．必须要“具体问题具体分析”． 机器学习中，不存在一种机器学习算法能适合于任何领域或任务 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:8:2","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"2.8.3 奥卡姆剃须刀原则（Occam’s Razor） 如无必要，勿增实体 正则化思想：简单模型泛化能力好 性能相近模型，选择简单的 最小描述长度（Minimum Description Length，MDL）原则 奥卡姆剃刀的一种形式化 即对一个数据集 𝒟，最好的模型$f \\in \\mathcal{F}$会使得数据集的压缩效果最好，即编码长度最小（压缩数据长度 + 模型长度） 贝叶斯学习解释：模型𝑓 在数据集𝒟 上的对数后验概率为$\\begin{aligned} \\max _{f} \\log p(f \\mid \\mathcal{D}) \u0026=\\max _{f} \\log p(\\mathcal{D} \\mid f)+\\log p(f) \\\\ \u0026=\\min _{f}-\\log p(\\mathcal{D} \\mid f)-\\log p(f) \\end{aligned}$ ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:8:3","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"2.8.4 丑小鸭定理（Ugly Duckling Theorem） [Watanable, 1969]：“丑小鸭与白天鹅之间的区别和两只白天鹅之间的区别一样大” 不存在相似性的客观标准，一切相似性标注都是主观的。eg. 外观两只白天鹅更相似，基因丑小鸭与它父母的差别小于它父母与其他白天鹅的差别。 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:8:4","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"2.8.5 归纳偏置（Inductive Bias） 归纳偏置：学习算法对学习问题做的假设。贝叶斯学习称之为先验（Prior） eg. 最近邻分类器中，我们会假设在特征空间中，一个小的局部区域中的大部分样本同属一类．在朴素贝叶斯分类器中，我们会假设每个特征的条件概率是互相独立的 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:8:5","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"习题选做 习题 2-1 分析为什么平方损失函数不适用于分类问题 损失函数用于反应问题的优化程度，分类问题中的标签，没有连续概念，每个标签之间的距离也没有意义，预测值和标签之间的均分误差不能反应问题的优化程度。 最小化平方损失函数本质上等同于在误差服从高斯分布的假设下的极大似然估计，在分类问题下大部分时候误差并不服从高斯分布。 2-2 $w=\\left(X^{T} R X\\right)^{-1}(R X)^{T} Y$ 每个样本重视程度不同 习题 2-11 分别用一元、二元和三元特征的词袋模型表示文本“我打了张三”和“张三打了我”，并分析不同模型的优缺点． 一元：我、打了、张三 x1 = [1,1,1] x2 = [1,1,1] 无法表示语序 二元：$我、$张三、我打了、张三打了、打了张三、打了我、张三#、我# $x_{1}=[1,0,1,0,1,0,1,0]$ $x_{2}=[0,1,0,1,0,1,0,1]$ 可以表示单词间相邻顺序 三元：$我打了、$张三打了、我打了张三、张三打了我、打了张三#、打了我# $x_{1}=[1,0,1,0,1,0]$ $x_{2}=[0,1,0,1,0,1]$ 可以表示单词前后相邻顺序 n-gram：n为1时，无法表示顺序信息，n太大出现一个特征表示一个句子的情况，失去文本元信息 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/:9:0","tags":["神经网络与深度学习","NLP","notes","DL"],"title":"《神经网络与深度学习》第2章 - 机器学习概述","uri":"/blog/nndl-book-ch2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"},{"categories":["Deep Learning"],"content":"1 绪论 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch1-%E7%BB%AA%E8%AE%BA/:0:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第1章 - 绪论","uri":"/blog/nndl-book-ch1-%E7%BB%AA%E8%AE%BA/"},{"categories":["Deep Learning"],"content":"1.1 人工智能 领域 感知：CV、Audio 学习：监督、无监督、强化学习 认知：知识表示、NLU、推理、规划、决策 历史 推理期、知识期、学习期 流派 符号主义 连接主义 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch1-%E7%BB%AA%E8%AE%BA/:1:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第1章 - 绪论","uri":"/blog/nndl-book-ch1-%E7%BB%AA%E8%AE%BA/"},{"categories":["Deep Learning"],"content":"1.2 机器学习 步骤 数据预处理 特征提取 特征转换 升维 降维 特征选择 特征抽取 主成分分析PCA 线性判别分析LDA 传统机器学习主要关注学习预测模型，可以看作浅层学习，不涉及特征学习 特征靠人工经验或特征转换来抽取 很多问题变成了特征工程问题 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch1-%E7%BB%AA%E8%AE%BA/:2:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第1章 - 绪论","uri":"/blog/nndl-book-ch1-%E7%BB%AA%E8%AE%BA/"},{"categories":["Deep Learning"],"content":"1.3 表示学习 Representation：输入 -\u003e 特征 表示学习：自动学习有效特征的算法 关键：语义鸿沟（Semantic Gap）问题，输入的底层特征与高层语义的差异 核心问题 什么是好的表示 如何学习好的表示 表示方式 局部表示/离散表示/符号表示 one-hot、高维稀疏二值向量 分布式表示 一种语义分散到低维基向量、低维稠密向量 分散过程成为嵌入（Embedding） ","date":"2021-01-21","objectID":"/blog/nndl-book-ch1-%E7%BB%AA%E8%AE%BA/:3:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第1章 - 绪论","uri":"/blog/nndl-book-ch1-%E7%BB%AA%E8%AE%BA/"},{"categories":["Deep Learning"],"content":"1.4 深度学习 主要目的：自动学习特征表示，避免特征工程 关键问题：贡献度分配问题（Credit Assignment Problem，CAP） 看作特殊的强化学习 每个内部组件间接、延迟地从最终奖励中得到监督信息 主要模型： 神经网络：误差反向传播，解决贡献度分配问题 端到端学习： 传统切割任务：模块单独优化与总目标不一致、错误传播 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch1-%E7%BB%AA%E8%AE%BA/:4:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第1章 - 绪论","uri":"/blog/nndl-book-ch1-%E7%BB%AA%E8%AE%BA/"},{"categories":["Deep Learning"],"content":"1.5 神经网络 学习能力 赫布网络：基于赫布规则无监督 感知器：无法扩展到多层 反向传播 两层网络逼近任何函数 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch1-%E7%BB%AA%E8%AE%BA/:5:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第1章 - 绪论","uri":"/blog/nndl-book-ch1-%E7%BB%AA%E8%AE%BA/"},{"categories":["Deep Learning"],"content":"1.6 知识体系 机器学习 监督学习 无监督学习 强化学习 神经网络 前馈神经网络 卷积神经网络 循环神经网络 图网络 ","date":"2021-01-21","objectID":"/blog/nndl-book-ch1-%E7%BB%AA%E8%AE%BA/:6:0","tags":["神经网络与深度学习","NLP","notes","ML"],"title":"《神经网络与深度学习》第1章 - 绪论","uri":"/blog/nndl-book-ch1-%E7%BB%AA%E8%AE%BA/"},{"categories":["NLP"],"content":"ch13 文本分类与情感分类 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:0:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.1 文本分类概述 [Sebastiani, 2002]数学模型描述文本分类： 获得函数（分类器）：$\\Phi: {D} \\times {C} \\rightarrow{ {T}, \\quad {F}}$ 文档：$D={d_1, d_2, …,d_{|D|}}$ 类别：${C}=\\left{ {c}_{1}, {c}_{2}, \\ldots, {c}_{|C|}\\right}$ 关键问题 文本表示 分类器设计 文本分类系统 文本预处理：分词，取出停用词，过滤低频词，编码归一化等 文本向量化：如使用向量空间模型VSM或者概率统计模型对文本进行表示，使计算机能够理解计算，用的方法基于集合论模型、基于代数轮模型、基于频率统计模型等 文本特征提取和选择：特征提取对应着特征项的选择和特征权重的计算。是文本分类的核心内容，常用的特征提取方法： 1)用映射或者变换的方法对原始特征降维（word2vec）； 2)从原始的特征中挑选出一些最具代表性的特征； 3)根据专家的知识挑选出最具影响力的特征； 4)基于数学的方法选取出最具分类信息的特征。 分类器选择：回归模型，二元独立概率模型，语言模型建模IR模型 文本分类系统分类 基于知识工程（knowledge learning，KE） 专家人工规则 基于机器学习（machine learning，ML） 用训练样本进行特征选择、分类器参数训练 根据选择的特征对分类输入样本进行形式化 输入到分类器进行类别判定 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:1:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.2 文本表示 向量空间模型（vector space modle，VSM） 文档（document）：通常是文章中具有一定规模的片段，如句子、句群、段落、段落组直至整篇文章。 项／特征项（term/feature term）：特征项是VSM中最小的不可分的语言单元，可以是字、词、词组或短语等。一个文档的内容被看成是它含有的特征项所组成的集合，表示为：Document＝D（t1，t2，…，tn），其中tk是特征项，1≤k≤n。 项的权重（term weight）：对文档n个特征项依据一定原则赋予权重$w_k$，D＝D（t1，w1;t2，w2;…;tn，wn），简记为D＝D（w1，w2，…，wn） VSM定义：给定一个文档D（t1，w1;t2，w2;…;tn，wn），D符合以下两条约定： 各个特征项tk（1≤k≤n）互异（即没有重复）； 各个特征项tk无先后顺序关系（即不考虑文档的内部结构） 特征项$t_k$看作n维坐标系，权重$w_k$作为坐标值，文本表示维n维向量 向量的相似性度量（similarity）：任意两个文档D1和D2之间的相似系数Sim（D1，D2）指两个文档内容的相关程度（degree of relevance） 向量内积：$\\operatorname{Sim}\\left(D_{1}, D_{2}\\right)=\\sum_{k=1}^{n} w_{1 k} \\times w_{2 k}$ 考虑归一化，向量余弦：$\\operatorname{Sim}\\left(D_{1}, D_{2}\\right)=\\cos \\theta=\\frac{\\sum_{k=1}^{n} w_{1 k} \\times w_{2 k}}{\\sum_{k=1}^{n} w_{1 k}^{2} \\sum_{k=1}^{n} w_{2 k}^{2}}$ 除了VSM以外表示方法： 词组表示法： 提高不显著 提高了特征向量语义含量，但降低了特征向量统计质量，使特征向量更加稀疏 概念表示法 用概念（concept）作为特征向量的特征表示 用概念代替单个词可以在一定程度上解决自然语言的歧义性和多样性给特征向量带来的噪声问题，有利于提高文本分类的效果 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:2:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.3 文本特征选择 文本特征可以是：字、词、短语、概念等等 常用方法： 文档频率（document frequency, DF）特征提取法 信息增益（information gain, IG）法 χ2统计量（CHI）法 互信息（mutual information, MI）方法 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:3:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.3.1 文档频率DF 文档频率（DF）= 包含某特征项的文档数量 / 总文档数量 舍弃DF过小（没有代表性）、过大（没有区分度）的特征 优点：降低向量计算复杂度，可能提高分类准确率，因为去掉了一部分噪声特征，简单易行 缺陷：理论根据不足。根据信息论，某些低频率特征往往包含较多信息 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:3:1","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.3.2 信息增益（IG）法 信息增益法：根据某特征项$t_i$使得期望信息或者信息熵的有效减少量（信息增益）来判断其重要程度以取舍 信息增益 = 不考虑任何特征时文档的熵 - 考虑该特征后文档的熵 $\\begin{aligned} \\operatorname{Gain}\\left(t_{i}\\right)=\u0026 \\text { Entropy }(S)-\\text { Expected Entropy }\\left(S_{t_{i}}\\right) \\\\=\u0026\\left{-\\sum_{j=1}^{M} P\\left(C_{j}\\right) \\times \\log P\\left(C_{j}\\right)\\right}-\\left{P\\left(t_{i}\\right) \\times\\left[-\\sum_{j=1}^{M} P\\left(C_{j} \\mid t_{i}\\right) \\times \\log P\\left(C_{j} \\mid t_{i}\\right)\\right]\\right.\\ \u0026\\left.+P\\left(\\bar{t}_{i}\\right) \\times\\left[-\\sum_{i=1}^{M} P\\left(C_{j} \\mid \\bar{t}_{i}\\right) \\times \\log P\\left(C_{j} \\mid \\bar{t}_{i}\\right)\\right]\\right} \\end{aligned}$ 信息增益法是理论上最好的特征选取方法，但实际上许多高信息增益的特征出现频率较低，选取特征数目少时往往存在数据稀疏问题，分类效果差 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:3:2","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.3.3 $\\chi^2$统计量/开方检验 $\\chi^2$统计量（CHI）衡量的是特征项ti和类别Cj之间的相关联程度，并假设ti和Cj之间符合具有一阶自由度的$\\chi^2$分布 $\\chi^{2}\\left(t_{i}, C_{j}\\right)=\\frac{N \\times(A \\times D-C \\times B)^{2}}{(A+C) \\times(B+D) \\times(A+B) \\times(C+D)}$ 两种实现方法 最大值法：分别计算$t_i$对于每个类别的CHI值，然后在整个训练语料上： $\\chi_{\\mathrm{MAX}}^{2}\\left(t_{i}\\right)=\\max _{j=1}^{M} x\\left{\\chi^{2}\\left(t_{i}, C_{j}\\right)\\right}$ 平均值法：计算各特征对于各类别的平均值 $\\chi_{\\mathrm{AVG}}^{2}\\left(t_{i}\\right)=\\sum_{j=1}^{M} P\\left(C_{j}\\right) \\chi^{2}\\left(t_{i}, C_{j}\\right)$ 保留统计量高于给定阈值的特征 开方检验的缺点：忽略了词频，夸大了低频词的作用（低频词缺陷）。 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:3:3","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.3.4 互信息（MI）法 基本思想：互信息越大，特征ti和类别Cj共现的程度越大。 $\\begin{aligned} I\\left(t_{i}, C_{j}\\right) \u0026=\\log \\frac{P\\left(t_{i}, C_{j}\\right)}{P\\left(t_{i}\\right) P\\left(C_{j}\\right)} \\\\ \u0026=\\log \\frac{P\\left(t_{i} \\mid C_{j}\\right)}{P\\left(t_{i}\\right)} \\\\ \u0026 \\approx \\log \\frac{A \\times N}{(A+C) \\times(A+B)} \\end{aligned}$ 若特征ti和类别Cj无关，则P（ti，Cj）＝P（ti）×P（Cj），那么， I（ti，Cj）＝0 两种处理方法 最大值法：$I_{\\mathrm{MAX}}\\left(t_{i}\\right)=\\max _{j=1}^{M} \\mathrm{x}\\left[P\\left(C_{j}\\right) \\times I\\left(t_{i}, C_{j}\\right)\\right]$ 平均值法：$I_{\\mathrm{AVG}}\\left(t_{i}\\right)=\\sum_{j=1}^{M} P\\left(C_{j}\\right) I\\left(t_{i}, C_{j}\\right)$ ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:3:4","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"其他方法 DTP（distance to transition point）方法［Moyotl-Hernández and Jiménez-Salazar, 2005］ 期望交叉熵法 文本证据权法 优势率方法［Mademnic and Grobelnik, 1999］ “类别区分词”的特征提取方法［周茜等，2004］ 基于粗糙集（rough set）的特征提取方法 TFACQ［Hu et al., 2003］ 强类信息词（strong information class word, SCIW）方法［Li and Zong, 2005a］ ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:3:5","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.4 特征权重计算方法 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:4:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.5 分类器设计 常用分类算法 朴素的贝叶斯分类法（naΪve Bayesian classifier） 基于支持向量机（support vector machines,SVM）的分类器 k-最近邻法（k-nearest neighbor, kNN） 神经网络法（neural network, NNet） 决策树（decision tree）分类法 模糊分类法（fuzzy classifier） Rocchio分类方法 Boosting算法 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:5:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.5.1 朴素贝叶斯分类器 朴素贝叶斯( naive Bayes)是一种最简单常用的概率生成式模型（Generative Model），生成式模型是指有多少类，我们就学习多少个模型，分别计算新测试样本 $x$跟三个类别的联合概率$P(x, y)$，再根据贝叶斯公式计算选取使得$P(y \\mid x)$最大的作为分类。而判别式模型（Discrimitive Model）训练数据得到分类函数和分界面（如SVM），不能反应训练数据本身的特性。 基本思想：利用特征项和类别的联合概率来估计给定文档的类别概率。假设文本是基于词的一元模型。 假设现有的类别$C=(c_1,c_2…c_m)$，则文档最可能属于$\\hat{c}=\\underset{c \\in C}{\\operatorname{argmax}} P(c \\mid d)$类，使用贝叶斯公式转换为如下形式： $$ \\hat{c}=\\underset{c \\in C}{\\operatorname{argmax}} P(c \\mid d)=\\underset{c \\in C}{\\operatorname{argmax}} \\frac{P(d \\mid c) P(c)}{P(d)} $$ 分母相同可以忽略，得到： $$ \\hat{c}=\\underset{c \\in C}{\\operatorname{argmax}} P(c \\mid d)=\\underset{c \\in C}{\\operatorname{argmax}} P(d \\mid c) P(c) $$ 这个公式由两部分组成，前面那部分$P(d|c)$ 称为似然函数，后面那部分$P(c)$ 称为先验概率。使用词袋模型来表示文档$d$，文档$d$的每个特征表示为：$d={f_1,f_2,f_3……f_n}$，那么这里的特征$f_i$ 其实就是单词$w_i$ 出现的频率（次数），公式转化为： $$ \\hat{c}=\\underset{c \\in C}{\\operatorname{argmax}} \\overbrace{P\\left(f_{1}, f_{2}, \\ldots, f_{n} \\mid c\\right)}^{\\text {likelihood }} \\overbrace{P(c)}^{\\text {prior }} $$ 朴素贝叶斯的“朴素”表现在假设各个特征之间相互独立（条件独立性假设），则$P\\left(f_{1}, f_{2} \\ldots \\ldots_{n} \\mid c\\right)=P\\left(f_{1} \\mid c\\right){\\times} P\\left(f_{2} \\mid c\\right){\\times} \\ldots \\ldots{\\times} P\\left(f_{n} \\mid c\\right)$，故而公式变为 $$ c_{N B}=\\underset{c \\in C}{\\operatorname{argmax}} P(c) \\prod_{f \\in F} P(f \\mid c) $$ 因为每个概率的值很小，多个相乘则可能出现下溢（underflower）， 引入对数函数$log$，在$log\\ space$中进行计算： $$ c_{N B}=\\underset{c \\in C}{\\operatorname{argmax}} \\log P(c)+\\sum_{i \\in \\text {positions}} \\log P\\left(w_{i} \\mid c\\right) $$ 文档采用DF向量表示法： $P\\left(\\right.$ Doc $\\left.\\mid C_{i}\\right)=\\prod_{t_{j} \\in V} P\\left(\\operatorname{Doc}\\left(t_{j}\\right) \\mid C_{i}\\right)$ $P($ Doc $)=\\sum_{i}\\left[P\\left(C_{i}\\right) \\prod_{t_{i} \\in V} P\\left(\\operatorname{Doc}\\left(t_{i}\\right) \\mid C_{i}\\right)\\right]$ $P\\left(C_{i} \\mid\\right.$ Doc $)=\\frac{P\\left(C_{i}\\right) \\prod_{t_{j} \\in V} P\\left(\\operatorname{Doc}\\left(t_{j}\\right) \\mid C_{i}\\right)}{\\sum_{i}\\left[P\\left(C_{i}\\right) \\prod_{t_{j} \\in V} P\\left(\\operatorname{Doc}\\left(t_{j}\\right) \\mid C_{i}\\right)\\right]}$ 拉普拉斯估计：$P\\left(\\operatorname{Doc}\\left(t_{j}\\right) \\mid C_{i}\\right)=\\frac{1+N\\left(\\operatorname{Doc}\\left(t_{j}\\right) \\mid C_{i}\\right)}{2+\\left|D_{c_{i}}\\right|}$ 分子加1和分母加2背后的基本原理是这样的：在执行实际的试验之前，我们假设已经有两次试验，一次成功和一次失败 文档采用TF向量表示法： $P\\left(C_{i} \\mid\\right.$ Doc $)=\\frac{P\\left(C_{i}\\right) \\prod_{t_{i} \\in V} P\\left(t_{j} \\mid C_{i}\\right)^{\\mathrm{TF}\\left(t_{i}, \\text { Doc }\\right)}}{\\sum_{j}\\left[P\\left(C_{j}\\right) \\prod_{t_{i} \\in V} P\\left(t_{i} \\mid C_{j}\\right)^{\\mathrm{TF}\\left(t_{i}, \\mathrm{D}_{0}\\right)}\\right]}$ 拉普拉斯估计：$P\\left(t_{i} \\mid C_{i}\\right)=\\frac{1+\\operatorname{TF}\\left(t_{i}, C_{i}\\right)}{|V|+\\sum_{j} \\operatorname{TF}\\left(t_{j}, C_{i}\\right)}$ 加一平滑，对每个类别下所有划分的计数加1 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:5:1","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.5.2 SVM分类器 对于多类模式识别问题通常需要建立多个两类分类器 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:5:2","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.5.3 k-最邻近法（kNN） 在训练集中找邻近的k个文档，对其中每类的每个文档进行权重（余弦相似度）求和，作为该类和测试文档的相似度，决策规则： $y\\left(x, C_{j}\\right)=\\sum_{d_{i} \\in k \\mathrm{NN}} \\operatorname{sim}\\left(x, d_{i}\\right) y\\left(d_{i}, C_{j}\\right)-b_{j}$ $y(d_i，C_j)$为1表示di属于分类Cj，0表不属于。 $b_j$为二元决策的阈值 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:5:3","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.5.4 神经网络（NNet）分类器 输入单词或者更复杂特征向量，机器学习输入到分类的非线性映射 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:5:4","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.5.5 线性最小平方拟合法（linear least-squares fit, LLSF） 从训练集和分类文档中学习得到多元回归模型（multivariate regression model） $\\boldsymbol{F}_{\\mathrm{LS}}=\\arg \\min _{F}|\\boldsymbol{F} \\times \\boldsymbol{A}-\\boldsymbol{B}|^{2}$ 矩阵A和矩阵B描述的是训练数据（对应栏分别是输入和输出向量）；FLS为结果矩阵，定义了从任意文档到加权分类向量的映射。对这些分类的权重映射值排序，同时结合阈值算法，就可以来判别输入文档所属的类别。阈值是从训练中学习获取的 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:5:5","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.5.6 决策树分类器 树的根结点是整个数据集合空间，每个分结点是对一个单一变量的测试，该测试将数据集合空间分割成两个或更多个类别，即决策树可以是二叉树也可以是多叉树。每个叶结点是属于单一类别的记录。 训练集生成决策树，测试集修剪决策树 一般可通过递归分割的过程构建决策树，其生成过程通常是自上而下的，目的为最佳分割 从根结点到叶结点都有一条路径，这条路径就是一条决策“规则” 信息增益是决策树训练中常用的衡量给定属性区分训练样本能力的定量标准 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:5:6","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.5.7 模糊分类器 任何一个文本或文本类都可以通过其特征关键词描述，因此，可以用一个定义在特征关键词类上的模糊集来描述它们。 判定分类文本T所属的类别可以通过计算文本T的模糊集FT分别与其他每个文本类的模糊集Fk的关联度SR实现，两个类的关联度越大说明这两个类越贴近 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:5:7","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.5.8 Rocchio分类器 Rocchio分类器是情报检索领域经典的算法 基本思想： 为每个训练文本C建立特征向量 用训练文本特征向量为每类建立原始向量（类向量） 对待分类文本，距离最近的类就是所属类别 距离：向量点积、余弦相似度等 如果C类文本的原型向量为w1，已知一组训练文本，可以预测w1改进的第j个元素值为 $w_{1 j}^{\\prime}=\\alpha w_{1 j}+\\beta \\frac{\\sum_{i \\in C} x_{i j}}{n_{C}}-\\gamma \\frac{\\sum_{i \\in C} x_{i j}}{n-n_{C}}$ nC是训练样本中正例个数，即属于类别C的文本数；xij是第i个文本特征向量的第j个元素值；α、β、γ为控制参数。α控制了上一次计算所得的w对本次计算所产生的影响，β和γ分别控制正例训练集和反例训练集对结果的影响。 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:5:8","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.5.9 基于投票的分类方法 多分类器组合 核心思想：k个专家判断的有效组合应该优于某个专家个人的判断结果 投票算法： Bagging算法（民主） 票数最多的作为最终类别 Boosting算法（精英） Boosting推进：每次将分类错误的样本加入下一个弱分类器的训练 Adaboosting自适应推进：提高错误点的权值，加权投票（精度高的弱分类器权重大） Boosting 1984年Valiant提出的”可能近似正确”-PAC(Probably Approximately Correct)学习模型 强与弱 强学习：学习效果好 弱学习：仅比随机好 Boost（Schapire 1990）：任意 弱学习算法 -\u003e （任意正确率）强学习算法，加强过程多项式复杂度 Adaboost（Freund and Schapire）： 不需要提前知道弱学习算法先验知识 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:5:9","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.6 文本分类性能测评 正确率（Precision）：$P=\\frac{T P}{T P+F P}$ 召回率（Recall）：$R=\\frac{T P}{T P+F N}$ $F_{\\beta}$值（P与R加权调和平均）：$F_{\\beta}=\\frac{\\beta^{2}+1}{\\frac{\\beta^{2}}{r}+\\frac{1}{p}}=\\frac{\\left(\\beta^{2}+1\\right) \\times p \\times r}{\\beta^{2} \\times p+r}$ $F_1$值（P与R调和平均值）：$F_{1}=\\frac{1}{\\frac{1}{2} \\frac{1}{P}+\\frac{1}{2} \\frac{1}{R}}=\\frac{2 P R}{P+R}$ 宏平均（Macro-averaging）：先对每一个类统计指标值，然后在对所有类求算术平均值。 微平均（Micro-averaging）：对数据集中的每一个实例不分类别进行统计建立全局混淆矩阵，然后计算相应指标。 微平均更多地受分类器对一些常见类（这些类的语料通常比较多）分类效果的影响，而宏平均则可以更多地反映对一些特殊类的分类效果。在对多种算法进行对比时，通常采用微平均算法。 平衡点（break-even point）评测法［Aas and Eikvil, 1999］：通过调整分类器的阈值，调整正确率和召回率的值，使其达到一个平衡点的评测方法 11点平均正确率方法［Taghva et al., 2004］：为了更加全面地评价一个分类器在不同召回率情况下的分类效果，调整阈值使得分类器的召回率分别为：0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1，然后计算出对应的11个正确率，取其平均值 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:6:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"13.7 情感分类 情感分析（sentiment analysis）：借助计算机帮助用户快速获取、整理和分析相关评价信息，对带有情感色彩的主观性文本进行分析、处理、归纳和推理［Pang and Lee, 2008］。情感分析包含较多的任务，如情感分类（sentiment classification）、观点抽取（opinion extraction）、观点问答和观点摘要等。 情感分类是指根据文本所表达的含义和情感信息将文本划分成褒扬的或贬义的两种或几种类型，是对文本作者倾向性和观点、态度的划分，因此有时也称倾向性分析（opinion analysis） 情感分类的特殊性：情感的隐蔽性、多义性和极性不明显性 按机器学习方法分类 有监督学习方法 半监督学习方法 无监督学习方法 按照研究问题分类 领域相关性研究 领域适应性（domain adaptation）研究 数据不平衡问题研究 ","date":"2021-01-20","objectID":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/:7:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第13章 - 文本分类与情感分类","uri":"/blog/snlp-ch13-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB/"},{"categories":["NLP"],"content":"ch9 语义分析 NLP最终目的一定程度上是在语义理解的基础上实现响应的操作 语义计算十分困难：模拟人脑思维过程，建立语言、知识与客观世界之间的可计算逻辑关系，并实现具有高区分能力的语义计算模型，至今未解 语义分析任务 词层次：词义消歧（word sense disambiguation，WSD） 句子层面：语义角色标注（semantic role labeling，SRL） 篇章层面：指代消歧/共指消歧（coreference resolution）、篇章语义分析 ","date":"2021-01-19","objectID":"/blog/snlp-ch9-%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/:0:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第9.1章 - 语义分析","uri":"/blog/snlp-ch9-%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/"},{"categories":["NLP"],"content":"9.1 词义消歧概述 最小语用单位：词 词义消歧任务：确定一个多义词在给定的上下文语境中的具体含义 早期：规则 20世纪80年代后：统计 有监督的消歧方法（supervised disambiguation） 无监督的消歧方法（unsupervised disambiguation） 统计消歧基本观点：一个词的不同语义一般发生在不同上下文 有监督：词语义的上下文分类问题（classification task） 无监督（clustering task） 聚类算法对同一个多义词的所有上下文进行等价类划分 识别时，将上下文与各词义等价类比较 基于词典信息的消歧方法（dictionary-based disambiguation） 测试数据：为避免手工标注的困难，采用制造人工数据的方法获取大规模训练和测试数据。 制造的人工数据称为伪词（pseudoword），[Manning and Schütze，1999]基本思路是将两个词汇合并，如banana-door，替代所有语料中的banana和door（消歧就是判断到底是哪个词） ","date":"2021-01-19","objectID":"/blog/snlp-ch9-%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/:1:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第9.1章 - 语义分析","uri":"/blog/snlp-ch9-%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/"},{"categories":["NLP"],"content":"ch8 句法分析 基本任务：确定句子的句法结构（syntactic structure）或句子中词汇之间的依存关系 分类： 句法结构分析（syntactic structure parsing）/成分结构分析（constituent struture parsing）/短语结构分析（phrase structure parsing） 依存关系分析（dependency parsing）/依存句法分析/依存结构分析/依存分析 句法结构分析 完全句法分析（full syntactic parsing）/完全短语结构分析（full phrase structure parsing）：以获取整个句子句法结构为目的 局部分析（partial parsing）/浅层分析（shallow parsing）：以获取局部成为（如基本名词短语（base NP））为目的 ","date":"2021-01-19","objectID":"/blog/snlp-ch8-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/:0:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第8.1章 - 句法分析","uri":"/blog/snlp-ch8-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/"},{"categories":["NLP"],"content":"8.1 句法结构分析概述 ","date":"2021-01-19","objectID":"/blog/snlp-ch8-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/:1:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第8.1章 - 句法分析","uri":"/blog/snlp-ch8-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/"},{"categories":["NLP"],"content":"8.1.1 基本概念 句法结构分析：对输入的单词序列（一般为句子）判断其构成是否合乎给定的语法，分析出合乎语法的句子的句法结构。 句法结构表示：句法分析数（syntactic parsing tree），简称分析树（parsing tree） 句法结构分析器（syntactic parser）简称分析器（parser）：完成句法结构分析的程序模块 任务： 判断输入的字符串是否属于某种语言（通常系统默认知道，一般不考虑） 消除输入句子中词法和结构等方面的歧义 分析输入句子的内部结构，如成分构成、上下文关系等 eg. 句子“The can can hold the water”分析树： 主要困难：句法结构歧义的识别与消解 构造句法分析器： 语法的形式化表示和词条信息描述 分析算法设计 ","date":"2021-01-19","objectID":"/blog/snlp-ch8-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/:1:1","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第8.1章 - 句法分析","uri":"/blog/snlp-ch8-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/"},{"categories":["NLP"],"content":"8.1.2 语法形式化 语法形式化（grammar formalism）属于句法理论研究的范畴，目前NLP中广泛使用： 上下文无关文法（CFG） 基于约束的文法（constraint-based grammar）的简单形式，又称合一语法（unification grammar），具有优越性，广泛采用 常用基于约束的语法： 功能合一语法（functional unification grammar，FUG）［Kay，1984］ 树链接语法（tree-adjoining grammar，TAG）［Joshi et al.，1975］ 词汇功能语法（lexical-functional grammar，LFG）［Bresnan，1982］ 广义的短语结构语法（generalized phrase structure grammar，GPSG）［Gazdar et al.，1985］ 中心语驱动的短语结构语法（head-driven phrase structure grammar，HPSG）［Pollard and Sag，1994］ ","date":"2021-01-19","objectID":"/blog/snlp-ch8-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/:1:2","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第8.1章 - 句法分析","uri":"/blog/snlp-ch8-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/"},{"categories":["NLP"],"content":"8.1.3 基本方法 分类： 基于规则：由人工组织语法规则，建立语法知识库，通过条件约束和检查来实现句法结构歧义的消除。 基于统计 基于规则的句法分析 句法分析算法：CYK分析算法、欧雷分析算法、线图分析算法、移进-规约算法、GLR分析算法、左角分析算法等 分析方法根据句法分析树形成方向 自顶向下 自底向上 两者结合 主要优点： 分析算法可以利用手工编写的语法规则分析出输入句子所有可能的句法结构 对于特定的领域和目的，利用手工编写的有针对性的规则能够较好地处理输入句子中的部分歧义和一些超语法（extra-grammatical）现象 缺陷： 对于中长句子，分析复杂程序难以实现 即使能分析出所有可能结构，也难以在巨大候选中有效消歧 手工编写的主观性，难以覆盖大领域所有复杂语言 手工编写工作量大，且不利于移植到其他领域 [Samuelsson and Wiren, 2000]规则方法成功运用于程序设计语言编译器，却难以处理自然语言句法分析 形式化文法的生成能力问题。程序设计语言是严格的CFG的子类，自然语言文法表达能力更强 自然语言句子存在更多、更复杂的结构歧义 随着英语句子中介词短语组合个数的增加，介词引起的歧义结构的复杂程度不断加深，这个组合个数即为开塔兰数（Catalan numbers） eg. The rat the cat the dog chased caught died 另外，自然语言处理中的句法分析器的先验知识的覆盖程度永远是有限的，句法分析器总是可能遇到未曾学习过的新的语言现象 基于统计的句法分析 目前研究较多的统计句法分析方法是语法驱动的（grammar-driven），其基本思想是由生成语法（generative grammar）定义被分析的语言及其分析出的类别，在训练数据中观察到的各种语言现象的分布以统计数据的方式与语法规则一起编码。在句法分析的过程中，当遇到歧义情况时，统计数据用于对多种分析结果的排序或选择。 基于概率上下文无关文法（probabilistic （或stochastic）context-free grammar, PCFG或SCFG）的短语结构分析方法可以说是目前最成功的语法驱动的统计句法分析方法。 ","date":"2021-01-19","objectID":"/blog/snlp-ch8-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/:1:3","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第8.1章 - 句法分析","uri":"/blog/snlp-ch8-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/"},{"categories":["NLP"],"content":"8.9 依存句法理论简介 依存句法（dependence grammar）：又称从属关系语言（grammaire de dépendance），用词与词之间的依存关系来描述语言结构的框架 依存语法认为“谓语”中的动词是一个句子的中心，其他成分与动词直接或间接地产生联系 周国光将依存语法定义为一种结构语法：主要研究以谓词为中心而构句时由深层语义结构映现为表层句法结构的状况及条件，谓词与体词之间的同现关系，并据此划分谓词的词类 价：一个动词所能支配的行动元（名词词组）的个数即为该动词的价数 依存：就是指词与词之间支配与被支配的关系，这种关系不是对等的 支配者（governor，regent，head）：处于支配地位的成分 从属者（modifier，subordinate，dependency）：处于被支配地位的成分 三种基本等价的依存结构表达方式（投射树对句子的结构表达能力更强一些）： ［Robinson，1970］依存结构四条公理： 一个句子只有一个独立的成分 句子的其他成分都从属于某一成分 任何一个成分都不能依存于两个或两个以上的成分 如果成分A直接从属于成分B，而成分C在句子中位于A和B之间，那么，成分C或者从属于A，或者从属于B，或者从属于A和B之间的某一成分 这四条公理相当于对依存图和依存树的形式约束：单一父结点（single headed）、连通（connective）、无环（acyclic）和可投射（projective），并由此来保证句子的依存分析结果是一棵有“根”（root）的树结构。这为依存语法的形式化描述及在计算机语言学中的应用奠定了基础。 ［冯志伟，1998］依存结构树满足5个条件（直观、实用）： 单纯结点条件：只有终结点，没有非终结点； 单一父结点条件：除根结点没有父结点外所有的结点都只有一个父结点； 独根结点条件：一个依存树只能有一个根结点，它支配其他结点； 非交条件：依存树的树枝不能彼此相交； 互斥条件：从上到下的支配关系和从左到右的前于关系之间是互相排斥的，如果两个结点之间存在着支配关系，它们之间就不能存在前于关系。 依存语法与短语结构语法（phrase structure grammar，PSG）相比最大的优势是它直接按照词语之间的依存关系工作，依存语法几乎不使用词性和短语类等句法语义范畴，没有Chomsky的形式化重写规则，几乎所有的语言知识都体现在词典中，是基于词语法理论的。 ","date":"2021-01-19","objectID":"/blog/snlp-ch8-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/:2:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第8.1章 - 句法分析","uri":"/blog/snlp-ch8-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/"},{"categories":["NLP"],"content":"7.5 词性标注 ","date":"2021-01-18","objectID":"/blog/snlp-ch7.5-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/:1:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.5章 - 词性标注","uri":"/blog/snlp-ch7.5-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/"},{"categories":["NLP"],"content":"7.5.1 概述 词性（part-of-speech）是词汇基本的语法属性，也称词类 主要难点 汉语缺乏词形态变化，不能从形态变化判别词类 常用词兼类现象严重 兼类词：有多种词性的词 研究者主观原因：词性划分目的和标准不统一 ","date":"2021-01-18","objectID":"/blog/snlp-ch7.5-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/:1:1","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.5章 - 词性标注","uri":"/blog/snlp-ch7.5-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/"},{"categories":["NLP"],"content":"7.5.2 基于统计模型的词性标注 基于统计模型（n-gram、一阶马尔科夫）的词性标注方法 代表：1983年I.Marshall建立的LOB语料库词性标注系统CLAWS（Constituent-Likelihood Automatic Word-tagging System） HMM的词性标注：参数估计 随机初始参数：过于缺乏限制 利用词典信息约束模型参数（Jelinek方法） “词汇-词汇标记”对没有在词典中，令该词生成概率为0，否则为可能被标记的词性个数的倒数 $b_{j . l}=\\frac{b_{j . l}^{} C\\left(w^{l}\\right)}{\\sum_{w^{m}} b_{j . m}^{} C\\left(w^{m}\\right)}$ $b_{j . i}^{*}=\\left{\\begin{array}{ll}0, \u0026 \\text { 如果 } t^{j} \\text { 不是词 } w^{l} \\text { 所允许的词性 } \\\\ \\frac{1}{T\\left(w^{strong textl}\\right)}, \u0026 \\text { 其他情况 }\\end{array}\\right.$ 等价于用最大似然估计来估算概率$P(w^k \\mid t^i)$以初始化HMM，并假设每个词与其每个可能的词性标记出现的概率相等 词汇划分等价类，以类为单位进行参数估计，大大减少了参数个数 元词（metawords）$u_L$：所有具有相同可能词性的词汇划分为一组 类似Jelinek方法处理元词：$b_{j . l}=\\frac{b_{j . L}^{} C\\left(u_{L}\\right)}{\\sum_{u_{L}^{\\prime}} b_{j . L^{\\prime}}^{} C\\left(u_{L^{\\prime}}\\right)}$ $b_{j . L}^{*}=\\left{\\begin{array}{ll}0, \u0026 j \\notin L \\\\ \\frac{1}{L}, \u0026 \\text { 否则 }\\end{array}\\right.$ HMM训练：前向后向算法 模型参数对训练语料的适应性问题 ","date":"2021-01-18","objectID":"/blog/snlp-ch7.5-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/:1:2","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.5章 - 词性标注","uri":"/blog/snlp-ch7.5-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/"},{"categories":["NLP"],"content":"7.5.3 基于规则的词性标注 按兼类词搭配关系和上下文语境建造词类消歧规则 早期：人工构造 语料发展：基于机器学习的规则自动提取 基于转换的错误驱动的（transformation-based and error-driven）学习方法 劣势：学习时间过长 改进：[周明等, 1998]每次迭代只调整受到影响的小部分转换规则，而不需要搜索所有转换规则 [李晓黎等, 2000]数据采掘方法获取汉语词性标注 ","date":"2021-01-18","objectID":"/blog/snlp-ch7.5-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/:1:3","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.5章 - 词性标注","uri":"/blog/snlp-ch7.5-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/"},{"categories":["NLP"],"content":"7.5.4 统计方法与规则方法结合 理性主义方法与经验主义方法相结合 [周强，1995]规则与统计结合 基本思想： 对初始标注结果，首先用规则排除常见、明显的歧义 再通过统计排歧，处理剩余多类词并进行未登录词的词性推断 最后人工校对 [张民，1998] [周强，1995]的方法规则作用于是非受限的，而且没有考虑统计的可信度，使规则与统计的作用域不明确 引入置信区间，构造基于置信区间的评价函数，实现统计与规则并举 HMM，前向后向算法计算状态i的词w出现次数 $F\\left(t_{i-1}, t_{i}\\right)=\\sum_{t_{i-2}}\\left[F\\left(t_{i-2}, t_{i-1}\\right) \\times P\\left(t_{i} \\mid t_{i-1}, t_{i-2}\\right) \\times P\\left(w_{i-1} \\mid t_{i-1}\\right)\\right]$ $B\\left(t_{i-1}, t_{i}\\right)=\\sum_{t_{i+1}}\\left[B\\left(t_{i}, t_{i-1}\\right) \\times P\\left(t_{i-1} \\mid t_{i}, t_{i-1}\\right) \\times P\\left(w_{i-1} \\mid t_{i-1}\\right)\\right]$ $\\phi(w)_{i}=\\underset{t}{\\operatorname{argmax}} \\sum_{t_{i-1}}\\left[F\\left(t_{i-1}, t_{i}\\right) \\times B\\left(t_{i-1}, t_{i}\\right) \\times P\\left(w_{i} \\mid t_{i}\\right)\\right]$ 假设兼类词w的候选词性为T1，T2，T3，其对应概率的真实值分别为p1，p2，p3，词w的词性为Ti（i＝1,2,3）时的出现次数为$\\phi(w)_{T_i}$ $\\hat{p}_{i}=\\frac{\\phi(w)_{T_{i}}}{\\sum_{j=1}^{3} \\phi(w)_{T_{j}}}$ i=1，2，3时，记$\\phi(w)_{T_i}$为n1,n2,n3（令n1\u003en2\u003en3） p1与p2相差小时，错误可能性较大 阈值法：$p_1/p_2$是否大于阈值作为是否选择$T_1$也无法区别n1=300,n2=100与n1=3,n2=1的情况（前者显然更加可靠） 可信度方法：根据n1，n2计算出的p1，p2只是p1，p2的近似值，我们必须估计出这种近似的误差，对p1/p2进行修正，然后再对修正后的p1/p2进行判别 可信度方法 由于ln（p1/p2）比p1/p2更快地逼近正态分布［Dagan and Itai,1994］，因此，可应用单边区间估计方法计算ln（p1/p2）的置信区间。 假设希望的错误率（desired error probability）（显著性水平）为α（0＜α＜1），则可信度为1-α，服从正态分布的随机变量X的置信区间为$Z_{1-\\alpha} \\sqrt{\\operatorname{vax} X}$ 置信系数$Z_{1-\\alpha}$ 标准差$\\operatorname{vax} X=\\operatorname{vax}\\left[\\ln \\frac{\\hat{p}_{1}}{\\hat{p}_{2}}\\right] \\approx \\frac{1}{n_{1}}+\\frac{1}{n_{2}}$ 最终评价函数 $\\ln \\frac{n_{1}}{n_{2}} \\geqslant \\theta+Z_{1-\\alpha} \\quad \\sqrt{\\frac{1}{n_{1}}+\\frac{1}{n_{2}}}$ 对统计标注结果的筛选，只对那些被认为可疑的标注结果，才采用规则方法进行歧义消解，而不是对所有的情况都既使用统计方法又使用规则方法 ","date":"2021-01-18","objectID":"/blog/snlp-ch7.5-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/:1:4","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.5章 - 词性标注","uri":"/blog/snlp-ch7.5-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/"},{"categories":["NLP"],"content":"7.5.5 词性标注中的生词处理方法 规则：生词处理通常与词形分词和兼类词消解一起进行 统计：通过合理处理词汇的发射频率解决 假设一个词汇序列W＝w1w2…wN对应的词性序列为T＝t1t2…tN，那么，词性标注问题就是求解使条件概率P（T|W）最大的T，即 $\\hat{T}=\\underset{T}{\\arg \\max } P(T \\mid W)=\\underset{T}{\\operatorname{argmax}} P(T) \\times P(W \\mid T)$ 对于一阶马尔科夫过程； $\\hat{T}=\\underset{t_{1} \\cdot t_{2}, \\cdots, t_{\\mathrm{N}}}{\\operatorname{argmax}} P\\left(t_{1}\\right) P\\left(w_{1} \\mid t_{1}\\right) \\prod_{i=2}^{N} P\\left(t_{i} \\mid t_{i-1}\\right) P\\left(w_{i} \\mid t_{i}\\right)$ $P(t_i \\mid t_{i-1})$为HMM中的状态转移概率，$P(W_i \\mid t_i)$为词汇发射概率 假设词汇序列W中有生词$x_j$，其词性标注为$t_j$ $\\begin{aligned} \\hat{T}=\u0026 \\underset{t_{1}, t_{2}, \\cdots, t_{N}}{\\operatorname{argmax}} P\\left(t_{1}\\right) P\\left(w_{1} \\mid t_{1}\\right) \\\\ \u0026 \\cdots P\\left(t_{j} \\mid t_{j-1}\\right) P\\left(x_{j} \\mid t_{j}\\right) \\prod_{i=j-1}^{N} P\\left(t_{i} \\mid t_{i-1}\\right) P\\left(w_{i} \\mid t_{i}\\right) \\end{aligned}$ [赵铁军等，2001]将生词词汇发射概率赋值为1 简单高效，但缺乏统计先验知识，正确率受到影响 [张孝非等，2003]将词汇序列W加入训练集 HMM假设：$P\\left(t_{j} \\mid x_{j}\\right) \\approx \\sum_{k=1}^{M} P\\left(t_{k} \\mid w_{j-1}\\right) P\\left(t_{j} \\mid t_{k}\\right)$ Bayes公式计算发射频率：$P\\left(x_{j} \\mid t_{j}\\right)=\\frac{P\\left(x_{j}\\right)}{P\\left(t_{j}\\right)} \\times P\\left(t_{j} \\mid x_{j}\\right)$ 带入：$P\\left(x_{j} \\mid t_{j}\\right) \\approx \\frac{P\\left(x_{j}\\right)}{P\\left(t_{j}\\right)} \\times \\sum_{k=1}^{M} P\\left(t_{k} \\mid w_{j-1}\\right) P\\left(t_{j} \\mid t_{k}\\right)$ 最大似然估计：$\\begin{aligned} P\\left(x_{j} \\mid t_{j}\\right) \u0026 \\approx \\frac{C\\left(x_{j}\\right)}{C\\left(t_{j}\\right)} \\sum_{k=1}^{M} P\\left(t_{k} \\mid w_{j-1}\\right) P\\left(t_{j} \\mid t_{k}\\right) \\\\ \u0026=\\frac{1}{C\\left(t_{j}\\right)} \\sum_{k=1}^{M}\\left[\\frac{C\\left(w_{j-1} t_{k}\\right)}{C\\left(w_{j-1}\\right)} \\times \\frac{C\\left(t_{k} t_{j}\\right)}{C\\left(t_{k}\\right)}\\right] \\end{aligned}$ ","date":"2021-01-18","objectID":"/blog/snlp-ch7.5-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/:1:5","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.5章 - 词性标注","uri":"/blog/snlp-ch7.5-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/"},{"categories":["NLP"],"content":"7.3 命名实体识别 ","date":"2021-01-17","objectID":"/blog/snlp-ch7.3-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/:1:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.3章 - 命名实体识别","uri":"/blog/snlp-ch7.3-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/"},{"categories":["NLP"],"content":"7.3.1 方法概述 实体概念在文本中的引用（entity mention，指称项）三种形式 命名性指称 名词性指称 代词性指称 “［［中国］乒乓球男队主教练］［刘国梁］出席了会议，［他］指出了当前经济工作的重点。” 实体概念“刘国梁”的指称项有三个 “中国乒乓球男队主教练”是名词性指称 “刘国梁”是命名性指称 “他”是代词性指称 任务发展： 在MUC-6组织NERC任务之前，主要关注的是人名、地名和组织机构名这三类专有名词的识别。 自MUC-6起，地名被进一步细化为城市、州和国家。后来也有人将人名进一步细分为政治家、艺人等小类 在CoNLL组织的评测任务中扩大了专有名词的范围，包含了产品名的识别 在其他一些研究工作中也曾涉及电影名、书名、项目名、研究领域名称、电子邮件地址和电话号码等。尤其值得关注的是，很多学者对生物信息学领域的专用名词（如蛋白质、DNA、RNA等）及其关系识别做了大量研究工作。 本节主要关注人名、地名和组织机构名这三类专有名词的识别方法。 方法发展： 早期：规则 20世纪90年代后期以来：统计机器学习，主要四类方法： ","date":"2021-01-17","objectID":"/blog/snlp-ch7.3-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/:1:1","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.3章 - 命名实体识别","uri":"/blog/snlp-ch7.3-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/"},{"categories":["NLP"],"content":"7.3.2 基于CRF的命名实体识别 可以说是命名实体识别最成功的方法 原理： 与基于字的汉语分词方法一样，将命名实体识别过程看作序列标注问题 基本思路： 分词 人名、简单地名、简单组织机构名识别 复合地名、复合组织机构名识别 常用标注语料库：北京大学计算语言学研究所标注的现代汉语多级加工语料库 训练： 将分词语料的标记符号转化成用于命名实体序列标注的标记 确定特征模板： 观察窗口：以当前位置的前后n（一般取2~3）个位置范围内的字串及其标记作为观察窗口 由于不同的命名实体一般出现在不同的上下文语境中，因此，对于不同的命名实体识别一般采用不同的特征模板 训练CRF模型参数$\\lambda$ ","date":"2021-01-17","objectID":"/blog/snlp-ch7.3-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/:1:2","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.3章 - 命名实体识别","uri":"/blog/snlp-ch7.3-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/"},{"categories":["NLP"],"content":"7.3.3 基于多特征的命名实体识别 命名实体识别：各种方法都是充分发现和利用实体上下文特征、实体内部特征，特征颗粒度有大（词性和角色级特征）有小（词形特征） [吴友政，2006]基于多特征融合的汉语命名实体识别方法 在分词和词性标注的基础上进一步进行命名实体识别 4个子模型 词形上下文模型：估计在给定词形上下文语境中产生实体的概率 词性上下文模型：估计在给定词性上下文语境中产生实体的概率 词形实体模型：估计在给定实体类型的情况下词形串作为实体的概率 词性实体模型：估计在给定实体类型的情况下词性串作为实体的概率 （1）模型描述 词形： 字典中任何一个字或词单独构成一类 人名（Per）、人名简称（Aper）、地名（Loc）、地名简称（Aloc）、机构名（Org）、时间词（Tim）和数量词（Num）各定义为一类 词形语言模型定义了$|V|+7$个词形，$|V|$表示词典规模 词形序列WC：词性构成的序列 词性： 北大计算语言学研究所开发的汉语文本词性标注标记集 人名简称词性、地名简称词性 共47个词性标记 词性序列TC 命名实体识别 输入：带有词性标注的词序列 $\\mathrm{WT}=w_{1} / t_{1} \\quad w_{2} / t_{2} \\quad \\cdots \\quad w_{i} / t_{i} \\quad \\cdots \\quad w_{n} / t_{n}$ 在分词和标注的基础上：对部分词语拆分、组合（确定实体边界）、和重新分类（确定实体类别） 输出：最优“词形/词性”序列$WC^*/TC^*$ $W C^{*} / \\mathrm{TC}^{*}=\\mathrm{wc}_{1} / \\mathrm{tc}_{1} \\quad \\mathrm{wc}_{2} / \\mathrm{tc}_{2} \\quad \\cdots \\quad \\mathrm{wc}_{i} / \\mathrm{tc}_{i} \\quad \\cdots \\quad \\mathrm{wc}_{m} / \\mathrm{tc}_{m}$ 算法： 词形特征模型 根据词性序列W产生候选命名实体，用Viterbi确定最优词形序列$WC^*$ 词性特征模型 根据词性序列T产生候选命名实体，用Viterbi确定最优词性序列$TC^*$ 混合模型/多特征识别算法 词形和词性混合模型是根据词形序列W和词性序列T产生候选命名实体，一体化确定最优序列WC*/TC*，即本节将要介绍的基于多特征的识别算法 多特征识别算法 输入： 词序列：$W=w_{1} \\quad w_{2} \\quad \\cdots \\quad w_{i} \\quad \\cdots \\quad w$ 词性序列：$t_{1} \\quad t_{2} \\quad \\cdots \\quad \\cdots \\quad t_{i} \\quad \\cdots \\quad t_{n}$ 词形特征模型：$\\mathrm{WC}^{*}=\\underset{\\mathrm{WC}}{\\operatorname{argmax}} P(\\mathrm{WC}) \\times P(W \\mid \\mathrm{WC})$ 词性特征模型：$\\mathrm{T} \\mathrm{C}^{*}=\\underset{\\mathrm{TC}}{\\operatorname{argmax}} P(\\mathrm{TC}) \\times P(T \\mid \\mathrm{TC})$ 混合：$\\begin{aligned} \u0026\\left(\\mathrm{WC}^{*}, \\mathrm{TC}^{*}\\right) \\\\=\u0026\\left.\\operatorname{argmax}_{(\\mathrm{WC}, \\mathrm{TO}}\\right) P(\\mathrm{WC}, \\mathrm{TC} \\mid W, T) \\\\=\u0026 \\operatorname{argmax}_{(\\mathrm{WC}, \\mathrm{TC})} P(\\mathrm{WC}, \\mathrm{TC}, W, T) / P(W, T) \\\\ \\approx \u0026 \\operatorname{argmax}_{(\\mathrm{WC}, \\mathrm{TO}} P(\\mathrm{WC}, W) \\times[P(\\mathrm{TC}, T)]^{\\beta} \\\\ \\approx \u0026 \\operatorname{argmax}_{(\\mathrm{WC}, \\mathrm{TO}} P(\\mathrm{WC}) \\times P(W \\mid \\mathrm{WC}) \\times[P(\\mathrm{TC}) \\times P(T \\mid \\mathrm{TC})]^{-3} \\end{aligned}$ β是平衡因子，平衡词形特征和词性特征的权重 词形上下文模型P（WC） 词性上下文模型P（TC） 实体词形模型P（W|WC） 实体词性模型P（T|TC） （2）词形和词性上下文模型 三元语法模型近似： $P(\\mathrm{WC}) \\approx \\prod_{i=1}^{m} P\\left(\\mathrm{wc}_{i} \\mid \\mathrm{wc}_{i-2} \\mathrm{wc}_{i-1}\\right)$ $P(\\mathrm{TC}) \\approx \\prod_{i=1}^{m} P\\left(\\mathrm{tc}_{i} \\mid \\mathrm{tc}_{i-2} \\mathrm{tc}_{i-1}\\right)$ （3）实体模型 考虑到每一类命名实体都具有不同的内部特征，因此，不能用一个统一的模型刻画人名、地名和机构名等实体模型。例如，人名识别可采用基于字的三元模型，地名和机构名识别可能更适合于采用基于词的三元模型等。 为提高外国人名识别性能，划分为日本人名、欧美人名、俄罗斯人名 实体模型： 人名实体模型 地名和机构名实体模型 单字地名实体模型 简称机构名实体模型 （4）专家知识 在基于统计模型的命名实体识别中，最大的问题是数据稀疏严重，搜索空间太大，从而影响系统的性能和效率。引入专家系统知识来限制候选实体产生： 人名识别的专家知识 地名识别的专家知识 机构名识别的专家知识 （5）模型训练 4个参数 词性上下文模型P（TC）和词形上下文模型P（WC）从《人民日报》标注语料中学习 中国人名、外国人名、地名、机构名的实体词性和词形模型从实体列表语料中训练 数据稀疏问题严重：Back-off数据平滑，引入逃逸概率计算权值 $$\\begin{aligned} \u0026 \\hat{P}\\left(w_{n} \\mid w_{1} \\cdots w_{n-1}\\right) \\\\=\u0026 \\lambda_{N} P\\left(w_{n} \\mid w_{1} \\cdots w_{n-1}\\right)+\\lambda_{N-1} P\\left(w_{n} \\mid w_{2} \\cdots w_{n-1}\\right) \\\\ \u0026+\\cdots+\\lambda \\cdot P\\left(w_{n}\\right)+\\lambda_{0} p_{0} \\end{aligned}$$ 其中$\\lambda_{i}=\\left(1-e_{i}\\right) \\sum_{k=i+1}^{n} e_{k}, 0\u003ci\u003cn, \\lambda_{n}=1-e_{n}$ （6）测试结果 ","date":"2021-01-17","objectID":"/blog/snlp-ch7.3-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/:1:3","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.3章 - 命名实体识别","uri":"/blog/snlp-ch7.3-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/"},{"categories":["NLP"],"content":"7.1 汉语自动分词中的基本问题 词是最小的能够独立运用的语言单位，很多孤立语和黏着语文本不像西方屈折语文本，词与词之间没有空格显示指示词的边界，首先需要自动分词。 汉语自动分词：让计算机系统在汉语文本中的词与词之间自动加上空格或其他边界标记 主要困难：分词规范、歧义切分、未登录词的识别 ","date":"2021-01-17","objectID":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/:1:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.1章 - 自动分词","uri":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/"},{"categories":["NLP"],"content":"7.1.1 汉语分词的规范问题 主要困难： 单字词与词素之间的划界 词与短语（词组）的划界 ","date":"2021-01-17","objectID":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/:1:1","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.1章 - 自动分词","uri":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/"},{"categories":["NLP"],"content":"7.1.2 歧义切分问题 基本切分歧义类型： 交集型切分歧义：汉字串AJB称作交集型切分歧义，如果满足AJ、JB同时为词（A、J、B分别为汉字串）。此时汉字串J称作交集串。 交集串链：交集型切分歧义拥有的交集串的集合 链长：交集串链中交集串的个数 eg. “中国产品质量”字段的链长为4，“部分居民生活水平”字段的链长为6 [孙茂松等，2001]认为，定义7-3中给出的名称“多义组合型切分歧义”是不太科学的（实际上，某些交集型切分歧义也是多义组合的），容易引起混淆，与“交集型”这个纯形式的名称相呼应，称作“包孕型”或者“覆盖型”可能更恰当 [董振东，1997]称之为“偶发性歧义” 多义组合型切分歧义：汉字串AB称作多义组合型切分歧义，如果满足A、B、AB同时为词 eg. “将来”、“现在”、“才能”、“学生会” [孙茂松等, 2001]补充定义：文本中至少存在一个上下文语境C，在C的约束下，A、B在语法和语义上都成立 [董振东，1997]称之为“固有歧义” [侯敏等，1995]认为还有“混合型”，集交集型与组合型的特点，交集型字段内包含组合型字段： 这篇文章写得太平淡了。 这墙抹得太平了。 即使太平时期也不应该放松警惕。 ","date":"2021-01-17","objectID":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/:1:2","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.1章 - 自动分词","uri":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/"},{"categories":["NLP"],"content":"7.1.3 未登录词问题 未登录词又称生词（unknown word），两种解释 词表没有收录的词 训练语料中未曾出现的词，又称集外词（out of vocabulary, OOV） 因词表在大规模语料中容易获取，通常将OOV与未登录词看作一回事 未登录词类型： 新出现的普通词汇：eg. 奥力给、不讲武德 专有名词（proper names）：人名、地名、组织机构名 命名实体（named entity）：专有名词 + 时间和数字表达（日期、数量值、百分比、序数、货币数量等） 专业名词和研究领域名称：eg. 三聚氰胺、苏丹红、禽流感、堰塞湖 其他专用名词，如新出现的产品、电影、书籍名 黄昌宁等人（2003）统计，未登录词约九成为专有名词，其余为新词 实际应用中未登录词的影响远大于歧义切分： 需要说明的是，在汉语分词中对命名实体词汇的识别处理是指将命名实体中可独立成词的切分单位正确地识别出来，而不是指识别整个实体的左右边界。 ","date":"2021-01-17","objectID":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/:1:3","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.1章 - 自动分词","uri":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/"},{"categories":["NLP"],"content":"7.2 汉语分词方法 分词方法： 基于词表的方法 正向最大匹配法（forward maximum matching method, FMM） 逆向最大匹配法（backward maximum matching method, BMM） 双向扫描法 逐词扫描法 基于统计模型的方法（结合n元语法） HMM CRF SVM 深度学习 规则方法与统计方法相结合 ","date":"2021-01-17","objectID":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/:2:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.1章 - 自动分词","uri":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/"},{"categories":["NLP"],"content":"7.2.1 N-最短路径方法 分词两阶段：1. 粗分 2. 歧义排除和未登录词识别 基于N-最短路径方法的汉语词语粗分模型[张华平等, 2002] 旨在提高召回率并兼顾准确率 基本思想： 根据词典，找出字符串中所有可能的词，构造词语切分有向无环图 每个词对应一条有向边，边长为权值 求出N条最短路，作为粗分结果集（算并列长度，最后集合大小\u003e=N） 建立词边： 考虑边长影响，分为两种模型 非统计粗分模型：所有词权重对等，即词边长均为1 NSP：结点V0到Vn的前N个最短路径的集合 N-最短路方法：将词语粗分问题转化为求解有向无环图G的NSP 求解方法：贪心，即Dijkstra简单扩展 记录每个结点N个最短路和前驱 回溯求解NSP 复杂度：字符串长度n，最短路径数N，某字作为词尾的平均次数k（总词数/末端词数，即切分图中结点入度平均值），算法复杂度为$O(n\\times N\\times k)$ 统计粗分模型 词权重设置为词频负对数：$-\\ln P(w_i)$ $P(W \\mid C)=\\frac{P(W) P(C \\mid W)}{P(C)} \\sim P(W)$ $P(W)=\\prod_{i=1}^{m} P\\left(w_{i}\\right)$ 越高频，越是捷径 求min：$p^*(W)=-\\ln P(W)= \\sum_{i=1}^{m}\\left[-\\ln P\\left(w_{i}\\right)\\right]$ 同理用最短路算法求解 实验结果[张华平等, 2002]：在N＝10的情况下，非统计粗分模型和统计粗分模型切分句子的召回率分别为99.73％和99.94％，均高于最大匹配方法和最短路径方法获得的召回率。 ","date":"2021-01-17","objectID":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/:2:1","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.1章 - 自动分词","uri":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/"},{"categories":["NLP"],"content":"7.2.2 基于词的n元语法模型的分词方法 典型的生成式模型 基本思想： 用词典对句子简单匹配，找出所有可能的词典词 将词典词和所有单个字作为结点，构造n元切分词图 边上概率表示代价，利用搜索算法（如Viterbi算法）找出代价最小路径 未登录词与歧义切分一体化处理：改进的信源信道模型的分词方法[J.Gao等, 2003] 受到启发：[Richard Sproat等, 1996]的基于加权的有限状态转换机（weighted finite-state transducer）模型与未登录词识别一体化切分的实现方法 改进的信源信道模型分词 将汉语词定义为4类 词表中有的词 文本中任意一个经词法派生出来的词或短语为一个词，如重叠形式（高高兴兴，说说话、天天）、前缀派生（非党员、副部长）、后缀派生（全面性、朋友们）、中缀派生（看得出、看不出）、动词加时态助词（克服了、蚕食着）、动词加趋向动词（走出、走出来）、动词的分离形式（长度不超过3个字，如：洗了澡、洗过澡），等等 文本中被明确定义的任意一个实体名词（如：日期、时间、货币、百分数、温度、长度、面积、体积、重量、地址、电话号码、传真号码、电子邮件地址等）是一个词。 文本中任意一个专有名词（人名、地名、机构名）是一个词。 假设随机变量S为一个汉字序列，W是S上所有可能切分出来的词序列，分词过程应该是求解使条件概率$P(W\\mid S)$最大的切分出来的词序列 $W^{*}=\\underset{W}{\\operatorname{argmax}} P(W \\mid S)$ 贝叶斯：$W^*=\\underset{W}{\\operatorname{argmax}} \\frac{P(W) P(S \\mid W)}{P(S)}$ 分母为归一化因子：$W^{*}=\\underset{W}{\\operatorname{argmax}} P(W) P(S \\mid W)$ 按下表可以把一个可能的词序列W转换成一个可能的词类序列$C＝c_1c_2…c_N$ $W^*$改写为：$C^{*}=\\underset{C}{\\operatorname{argmax}} P(C) P(S \\mid C)$ P(C)即语言模型 P(S|C)称生成模型 对于语言模型，如采用三元语法： $P(C)=P\\left(c_{1}\\right) P\\left(c_{2} \\mid c_{1}\\right) \\prod_{i=3}^{N} P\\left(c_{i} \\mid c_{i-2} c_{i-1}\\right)$ 生成模型： 独立性假设：词类$c_i$生成汉字串$s_i$概率只与$c_i$自身有关，而与其上下文无关 则：$P(S \\mid C) \\approx \\prod_{i=1}^{N} P\\left(s_{i} \\mid c_{i}\\right)$ [黄昌宁等，2003]实验： 词表、派生词表 语料：新闻文本 模型训练： 语料类别标记：FMM切分语料、专有名词、实体名词标注 最大似然估计：估计统计语言模型概率参数 用语言模型重新切分标注，得到刷新的训练语料，重复2、3直到收敛 交集型歧义字段（OAS）处理：最大匹配算法检测OAS，用特定类 $$ 取代全体OAS，训练语言模型P(C)；类 $$ 的生成模型的参数通过消歧规则或机器学习方法估计 组合型歧义字段（CAS）处理：对高频、切分分布均匀的70条CAS训练二值分类器，用分类器进行消歧 实验结果[黄昌宁等，2003]：自动分词的正确率和召回率分别达到了96.3％和97.4％ ","date":"2021-01-17","objectID":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/:2:2","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.1章 - 自动分词","uri":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/"},{"categories":["NLP"],"content":"7.2.3 由字构词的汉语分词方法 由字构词（character-based tagging）的汉语分词方法[Xue and Converse,2002] 思想：将分词过程看作字的分类问题 规定每个字只有4个词位：词首（B）、词中（M）、词尾（E）和单独成词（S） 原理：将分词结果表示成字标注形式，分词问题转化为序列标注问题 对于汉语句子$C^{n}={c}_{1} {c}_{2} \\ldots {c}_{ {n}}$ $P\\left(t_{1}^{n} \\mid c_{1}^{n}\\right)=\\prod_{k=1}^{n} P\\left(t_{k} \\mid t_{1}^{k-1}, c_{1}^{n}\\right) \\approx \\prod_{k=1}^{n} P\\left(t_{k} \\mid t_{k-1}, c_{k-2}^{k-2}\\right)$ $t_k\\in {B, M, E, S}$ 特征窗口 一般取w=5个字，前后各两个字 窗口中抽取特征，常用特征模板： (a) $c_{k} (k=-2,-1,0,1,2)$ (b) $c_{k} c_{k+1} (k=-2,-1,0,1)$ (c) $c_{-1} c_{1}$ (d) $T\\left(c_{-2}\\right) T\\left(c_{-1}\\right) T\\left(c_{0}\\right) T\\left(c_{1}\\right) T\\left(c_{2}\\right)$ 前面三类特征模板（a）～（c）是窗口内的字及其组合特征 模板（d）与定义的字符类别信息有关，主要是为了处理数字、标点符号和英文字符等有明显特征的词 有了特征，利用常用判别式模型（最大熵、CRF、SVM、感知机）进行参数训练，然后利用解码算法找到最优切分结果 由字构词优势：平衡看待词表词与未登录词识别问题，分词过程为字重组的简单过程；学习架构上，既可以不必专门强调词表词信息，也不用专门设计特定的未登录词识别模块，因此，大大简化了分词系统的设计。 ","date":"2021-01-17","objectID":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/:2:3","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.1章 - 自动分词","uri":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/"},{"categories":["NLP"],"content":"7.2.4 基于词感知机算法的汉语分词方法 平均感知机（averaged perceptron）：使用词相关的特征 假设x∈X是输入句子，y∈Y是切分结果，其中X是训练语料集合，Y是X中句子标注结果集合。我们用GEN（x）表示输入句子x的切分候选集，用$\\phi(x, y)\\in R^d$表示训练实例（x, y）对应的特征向量，α表示参数向量，其中$R^d$是模型的特征空间。那么，给定一个输入句子x，其最优切分结果满足如下条件： $$F(x)=\\arg \\max _{y \\in \\operatorname{GEN}(x)}{\\Phi(x, y) \\cdot \\alpha}$$ 训练算法如下： 词感知机思路： 解码器每次读入一个字，生成所有候选词，候选词两种： a. 作为上一个候选词末尾 b. 作文下一个候选字开始 解码器维持源列表、目标列表（滚动数组），每读入一个词，与源列表每个候选组合为两个新候选（合并为新词或者作为下一个词的开始），新候选放入目标列表 处理完成后，copy 目标列表 to 源列表，clear 目标列表，读入下一个词，重复（2） 该解码算法类似于全切分方法，理论上会生成所有$w^{l-1}$个切分结果（l为句长），为提升切分速度，限制目标列表tgt保留B个最高得分的候选（eg. B=16）。对tgt列表中切分候选打分和排序采用平均感知机分类器算法[Zhang and Clark, 2007]，使用如下特征： ","date":"2021-01-17","objectID":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/:2:4","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.1章 - 自动分词","uri":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/"},{"categories":["NLP"],"content":"7.2.5 基于字的生成式模型和区分式模型相结合的汉语分词方法 汉语分词两大主流方法 基于词的n元语法模型（生成式模型） 集内词效果好，集外词效果差 基于字的序列标注模型（区分式模型） 集外词效果好，集内词效果差 ［Wang et al., 2012］两个处于词边界的字的依赖关系和两个处于词内部的字的依赖关系不同（容易理解，词内部字依赖关系更强） 两大方法优缺点： 基于词的生成式模型实际上隐含地考虑了这种处于不同位置字之间 的依赖关系，而在基于字的判别式模型中却无法考虑这种依赖关系。 但是，区分式模型能够充分利用上下文特征信息等，有较大的灵活性。因 此，基于字的区分式模型具有较强的鲁棒性。 基于字的n元语法模型[Wang et al., 2009, 2010a, 2012] 结合基于字的生成式、判别式模型 将字替换为\u003c字-标记\u003e对，即 $P\\left(w_{1}^{m} \\mid c_{1}^{n}\\right) \\equiv P\\left([c, t]_{1}^{n} \\mid c_{1}^{n}\\right)=\\frac{P\\left(c_{1}^{n} \\mid[c, t]_{1}^{n}\\right) \\times P\\left([c, t]_{1}^{n}\\right)}{P\\left(c_{1}^{n}\\right)}$ 3-gram \u0026 Bayes: $P\\left(w_{1}^{m}\\right)=\\prod_{i=1}^{m} P\\left(w_{i} \\mid w_{1}^{j-1}\\right) \\approx \\prod_{i=1}^{m} P\\left(w_{i} \\mid w_{i-2}^{i-1}\\right)$ 简化： $P\\left([c, t]_{1}^{n}\\right) \\approx \\prod_{i=1}^{n} P\\left([c, t]_{i} \\mid[c, t]_{i-k}^{i-1}\\right)$ 基于字的n-gram特点： 以字为基本单位，但考虑了字与字的依赖关系，对词典词处理能力优于基于字的判别式模型 但没有考虑未来信息（下文），对未登录词处理能力仍弱于基于字的判别式模型 改进：集成式分词模型[Wang et al., 2010a, 2012] 结合基于字的判别式、基于字的生成式 线性插值法整合两个模型 Score $\\left(t_{k}\\right)=\\alpha \\times \\log \\left(P\\left([c, t]_{k} \\mid[c, t]_{k-2}^{k-1}\\right)\\right)+(1-\\alpha) \\times \\log \\left(P\\left(t_{k} \\mid t_{k-1}, c_{k-2}^{k-2}\\right)\\right)$ ","date":"2021-01-17","objectID":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/:2:5","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.1章 - 自动分词","uri":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/"},{"categories":["NLP"],"content":"7.2.6 其他分词方法 [Wu, 2003a]句法分析与自动分词相结合 利用句法结构消除组合型歧义 效果差 组合型歧义少 句法分词本身的歧义，倒打一耙 句法分析器的语法规则使用范围有限 [Gao et al., 2005]汉语分词的语用方法（pragmatic approach） 词：根据它们在实际使用和处理中的需要从语用上定义的切分单位 语用数学框架：切分已知词和检测不同类型的生词能够以一体化的方式同步进行 切分标准：假设不存在独立于应用的通用切分标准，不同任务需要多重切分标准和不同的词汇粒度 由字构词的方法改进 ［Zhang et al., 2006a, 2006b］为提升词典词召回率（recall），张瑞强等人提出了基于“子词”（sub-word）的判别式模型 ［Zhao et al., 2006a, 2010］赵海等人还比较了不同词位数量对该模型的影响，他们的实验表明，基于6个词位的效果最好 将汉语分词与词性标注两项任务同时进行，以达到同时提升两项任务性能的目的，一直是这一领域研究的一个重要方向，这种方法往往需要耗费更多的时间代价 ","date":"2021-01-17","objectID":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/:2:6","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.1章 - 自动分词","uri":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/"},{"categories":["NLP"],"content":"7.2.7 分词方法比较 测评语料： 测评指标： 测评结果： 存在难题： 跨领域分词性能 非规范文本：微博、短信，存在大量新词、流行语 ","date":"2021-01-17","objectID":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/:2:7","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.1章 - 自动分词","uri":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/"},{"categories":["NLP"],"content":"补充：语言结构类型 孤立语，如：汉语 缺乏词性变化 词序严格 虚词十分重要 复合词多，派生词少 屈折语，如：印欧语系诸语言，英语、德语、法语 屈折：内部屈折，词内部的语音形式的变化 词性变化丰富，用以表示词间关系 一种词性变化的语素可以表示几种不同的语法意义 词尾和词干/词根结合紧密，脱离词尾，词根不能独立存在 黏着语，如：日语、土耳其语、维吾尔语、芬兰语 词前面和中间不变，只是词尾变化表示语法意义 变词语素的一种变化只表示一种语法意义 词根与变词语素不紧密，两者有恒大独立性 复综语/编插语/多式综合语，如：美洲印第安语、爱斯基摩语、巴斯克语 分不出词和句子，一个词的构成也是另一个词的组成 没有能独立使用的词，只能许多成分相互编插组合在一起，连缀为句子使用 ","date":"2021-01-17","objectID":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/:2:8","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.1章 - 自动分词","uri":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/"},{"categories":["NLP"],"content":"References 叶蜚声《语言学纲要》 ","date":"2021-01-17","objectID":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/:3:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第7.1章 - 自动分词","uri":"/blog/snlp-ch7.1-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D/"},{"categories":["NLP"],"content":"6.1 概述 概率图模型（probabilistic graphical models）：在概率模型的基础上，使用基于图的方法表示概率分布/概率密度/密度函数，是一种通用化的不确定性知识表示和处理方法。 结点：变量 边：变量概率关系 边是否有向：有向概率图模型、无向概率图模型 应用： DBN：动态系统的推断和预测 HMM：语音识别、汉语自动分词、词性标注、统计机器翻译 Kalman filter：信号处理 Markov networks / Markov random field(MRF) CRF：序列标注、特征选择、机器翻译 Boltzman machine：依存句法分析、语义角色标注 概率图模型演变 生成式/产生式模型 vs. 区分式/判别式模型 本质区别：观测序列x与状态序列y的决定关系 生成式模型 假定y决定x 对联合分布$p(x, y)$建模，估计生成概率最大的生成序列来获取y 特征：一般有严格独立性假设，特征事先给定 优点：灵活、变量关系清楚、模型可以增量学习获得、可用于数据不完整情况 典型：n-gram、HMM、Naive Bayes、概率上下文无关文法 判别式模型 假定x决定y 对后验概率$p(y|x)$建模，从x提取特征，学习参数，使条件概率符合一定形式的最优 特征：任意给定，一般通过函数表示 优点：处理多类或一类与其他类差异比较灵活简单 弱点：模型描述能力有限、变量关系不清、一般为有监督，不能扩展为无监督 典型：最大熵模型、条件随机场、SVM、最大熵马尔科夫模型（maximum-entropy Markov model, MEMM）、感知机（perceptron） ","date":"2021-01-16","objectID":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/:1:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第6章 - 概率图模型","uri":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"6.2 贝叶斯网络 贝叶斯网络又称信度网络/信念网络（belief networks） 理论基础：贝叶斯公式 形式：DAG 结点：随机变量（可观测量、隐含变量、未知参量或假设等） 有向边：条件依存关系 如图，联合概率函数为：$P(H, S, N)=P(H \\mid S, N) \\times P(S \\mid N) \\times P(N)$ 构造贝叶斯网络 表示：在某一随机变量的集合$x＝{X_1，L，X_n}$上给出其联合概率分布P。 推断：回答关于变量的询问，如当观察到某些变量（证据变量）时，推断另一些变量子集的变化。 常用精确推理方法： 变量消除法（variable elimination）：基本任务是计算条件概率$p(X_Q|X_E＝x)$，其中，$X_Q$是询问变量的集合，$X_E$为已知证据的变量集合。其基本思想是通过分步计算不同变量的边缘分布按顺序逐个消除未观察到的非询问变量 团树（clique tree）：使用更全局化的数据结构调度各种操作，以获得更加有益的计算代价 常用近似推理算法： 重要性抽样法（importance sampling） 随机马尔科夫链蒙特卡洛（Markov chain Monte Carlo, MCMC）模拟法 循环信念传播法（loopy belief propagation） 泛化信念传播法（generalized belief propagation） 学习：参数学习和结构学习 参数学习：决定变量之间相互关联的量化关系，即依存强度估计 即对每个结点X，计算给定父结点条件下X的概率，概率分布可以是任意形式，通常为离散分布或高斯分布 常用参数学习方法： 最大似然估计 最大后验概率法 期望最大化方法（EM） 贝叶斯估计方法（贝叶斯图中常用） 结构学习：寻找变量之间的图关系 很简单情况：专家构造。多数使用系统中人工构造贝叶斯网络几乎不可能。 自动学习贝叶斯网络的图结构 贝叶斯网络是一种不定性因果关联模型，能够在已知有限的、不完整、不确定信息的条件下进行学习和推理 应用：广泛应用于故障诊断和维修决策等领域；汉语自动分词和词义消歧 ","date":"2021-01-16","objectID":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/:2:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第6章 - 概率图模型","uri":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"6.3 马尔科夫模型 随机过程又叫随机函数，是随时间而随机变化的过程。 离散的一阶马尔可夫链（Markov chain） $P(q_{t}=s_{i} \\mid q_{t-1}=s_{j}, q_{t-1}=s_{k}, \\cdots)=P(q_{t}=s_{j} \\mid q_{t-1}=s_{i})$ 马尔科夫模型/可视马尔科夫模型（visible Markov model，MM/VMM） 只考虑上式独立于时间t的随机过程：$P(q_{t}=s_{j} \\mid q_{t-1}=s_{i})=a_{i j}, \\quad 1 \\leqslant i, j \\leqslant N$ 满足：$a_{i j} \\geqslant 0$，$\\sum_{i=1}^{N} a_{i j}=1$ 有$N$个状态的一阶马尔可夫过程有$N^2$次状态转移，可表示成状态转移矩阵 eg. 一段文字中s1：名词，s2：动词，s3：形容词，转移矩阵： 假设名词开头，则O=“名动形名”概率为： $\\begin{aligned} P(O \\mid M) \u0026=P(s_{1}, s_{2}, s_{3}, s_{1} \\mid M) \\\\ \u0026=P(s_{1}) \\cdot P(s_{2} \\mid s_{1}) \\cdot P(s_{3} \\mid s_{2}) \\cdot P(s_{1} \\mid s_{3}) \\\\ \u0026=1 \\times a_{12} \\times a_{23} \\times a_{31} \\\\ \u0026=0.5 \\times 0.2 \\times 0.4 \\\\ \u0026=0.04 \\end{aligned}$ 马尔科夫模型可视为随机的非确定有限状态机 - 序列概率为转移弧概率乘积： n-gram与马尔科夫模型 2-gram就是一阶马尔科夫模型 对于$n\\ge 3$的n-gram确定数量的历史，可以通过将状态空间描述成多重前面状态的交叉乘积的方式，转化为马尔科夫模型，可以称之为m阶马尔科夫模型，m为历史数。 n元语法模型就是n-1阶马尔可夫模型 ","date":"2021-01-16","objectID":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/:3:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第6章 - 概率图模型","uri":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"6.4 隐马尔科夫模型 VMM每个状态代表可观察的事件，限制了模型适应性，提出HMM HMM：观察到的事件是隐蔽的状态转换过程的随机函数，模型为双重随机过程 类比：口袋取球，室外人只看到球 HMM记为五元祖$\\mu=(\\mathrm{S}, \\mathrm{K}, \\mathrm{A}, \\mathrm{B}, \\pi)$ S：状态结合 K：输出符号集合 A：状态转移概率 B：符号发射概率 $\\pi$：初始状态概率分布 基本问题： 估计问题：给定观察序列$O=O_1O_2…O_T$和模型$\\mu=(\\mathrm{A}, \\mathrm{B}, \\pi)$，快速计算$P(O\\mid \\mu)$ 序列问题：给定观察序列$O=O_1O_2…O_T$和模型$\\mu=(\\mathrm{A}, \\mathrm{B}, \\pi)$，快速有效选择“最优”状态序列$Q=q_1q_2…q_T$解释观察序列 训练问题/参数估计问题：给定观察序列$O=O_1O_2…O_T$，如何根据最大似然估计求参数？即如何调节模型$\\mu=(\\mathrm{A}, \\mathrm{B}, \\pi)$的参数，使得$P(O\\mid \\mu)$最大。 解决：前后向算法及参数估计 ","date":"2021-01-16","objectID":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/:4:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第6章 - 概率图模型","uri":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"6.4.1 求解观察序列的概率 估计问题/解码（decoding）问题：给定观察序列$O=O_1O_2…O_T$和模型$\\mu=(\\mathrm{A}, \\mathrm{B}, \\pi)$，快速计算$P(O\\mid \\mu)$ 推导： 对于任意的状态序列$Q=q_{1} q_{2} \\ldots q_{T}$，有： $$\\begin{aligned} P(O \\mid Q, \\mu) \u0026=\\prod_{t=1}^{T-1} P(O_{t} \\mid q_{t}, q_{t+1}, \\mu) \\\\ \u0026=b_{q_{1}}(O_{1}) \\times b_{q_{2}}(O_{2}) \\times \\cdots \\times b_{q_{T}}(O_{T}) \\end{aligned}$$ 并且 $$P(Q \\mid \\mu)=\\pi_{q_{1}} a_{q_{1} q_{2}} a_{q_{2} q_{3}} \\cdots a_{q_{T-1} q_{T}}$$ 由于 $$P(O, Q \\mid \\mu)=P(O \\mid Q, \\mu) P(Q \\mid \\mu)$$ 因此 $$\\begin{aligned} P(O \\mid \\mu) \u0026=\\sum_{Q} P(O, Q \\mid \\mu) \\\\ \u0026=\\sum_{Q} P(O \\mid Q, \\mu) P(Q \\mid \\mu) \\\\ \u0026=\\sum_{Q} \\pi_{q_{1}} b_{q_{1}}(O_{1}) \\prod_{t=1}^{T-1} a_{q_{t} q_{t+1}} b_{q_{t+1}}(O_{t+1}) \\end{aligned}$$ 算法改进： 问题：在N状态、T时间长度时，上述推导需要穷尽$N^T$个所有可能的状态序列，指数爆炸 改进：基于DP的前向算法/前向计算过程（forward procedure），$O(N^2T)$ 描述：HMM的DP问题一般用格架（trellis/lattice）的组织形式描述 前向算法 前向变量：$\\alpha_{t}(i)=P(O_{1} O_{2} \\cdots O_{t}, q_{t}=s_{i} \\mid \\mu)$ 算法思想：先快速计算前向变量$\\alpha_t(i)$，再据此算出$P(O\\mid \\mu)$ 显而易见，$P(O\\mid \\mu)$为所有T长度的状态下观察序列出现概率和 $P(O \\mid \\mu)=\\sum_{s_{i}} P(O_{1} O_{2} \\cdots O_{\\tau}, q_{T}=s_{i} \\mid \\mu)=\\sum_{i=1}^{N} \\alpha_{T}(i)$ DP思想：t+1的前向变量可以由t时刻所有前向变量归纳计算 $\\alpha_{t-1}(j)=(\\sum_{i=1}^{N} \\alpha_{t}(i) a_{i j}) b_{j}(O_{t+1})$ 前向算法描述（forward procedure） 初始化：$\\alpha_{1}(\\mathrm{i})=\\pi b_{i}(O_{1}), 1 \\leq i \\leq N$ 归纳计算：$\\alpha_{i+1}(j)=(\\sum_{i=1}^{N} \\alpha_{t}(i) a_{i j}) b_{j}(O_{t+1}), \\quad 1 \\leqslant t \\leqslant T-1$ 求和终结：$P(O \\mid \\mu)=\\sum_{i=1}^{N} \\alpha_{T}(i)$ 复杂度：共T时间，每个时间计算N个前向变量，每个前向变量需要考虑上一时刻的的N个前向变量，所以复杂度为$O(N^2T)$ 后向算法 与前向算法功能相同，用于快速计算$P(O \\mid \\mu)$ 后向变量：$\\beta_{t}(i)=P(O_{t-1} O_{t-2} \\cdots O_{T} \\mid q_{t}=s_{i}, \\mu)$ DP思想： $\\beta_{t}(i)=\\sum_{j=1}^{N} a_{i j} b_{j}(O_{t+1}) \\beta_{t-1}(j)$ 后向算法描述（backward precedure） 初始化：$\\beta_{\\mathrm{T}}(\\mathrm{i})=1, \\quad 1 \\leq \\mathrm{i} \\leq \\mathrm{N}$ 归纳计算：$\\beta_{i}(i)=\\sum_{j=1}^{N} a_{i j} b_{j}(O_{t+1}) \\beta_{i+1}(j), \\quad T-1 \\geqslant t \\geqslant 1 ; 1 \\leqslant i \\leqslant N$ 求和终结：$P(O \\mid \\mu)=\\sum_{i=1}^{N} \\pi_{i} b_{i}(O_{1}) \\beta_{1}(i)$ 前后向结合算法 $$\\begin{aligned} P(O, q_{t}=s_{i} \\mid \\mu) \u0026=P(O_{1} \\cdots O_{T}, q_{t}=s_{i} \\mid \\mu) \\\\ \u0026=P(O_{1} \\cdots O_{t}, q_{t}=s_{i}, O_{t-1} \\cdots O_{T} \\mid \\mu) \\\\ \u0026=P(O_{1} \\cdots O_{t}, q_{t}=s_{i} \\mid \\mu) \\times P(O_{t+1} \\cdots O_{T} \\mid O_{1} \\cdots O_{t}, q_{t}=s_{i}, \\mu) \\\\ \u0026=P(O_{1} \\cdots O_{t}, q_{t}=s_{i} \\mid \\mu) \\times P(O_{t+1} \\cdots O_{T} \\mid q_{t}=s_{i}, \\mu) \\\\ \u0026=\\alpha_{t}(i) \\beta_{t}(i) \\end{aligned}$$ $$P(O \\mid \\mu)=\\sum_{i=1}^{N} \\alpha_{t}(i) \\times \\beta_{t}(i), \\quad 1 \\leqslant t \\leqslant T$$ ","date":"2021-01-16","objectID":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/:4:1","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第6章 - 概率图模型","uri":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"6.4.2 维特比（Viterbi）算法 求解序列问题：给定观察序列$O=O_1O_2…O_T$和模型$\\mu=(\\mathrm{A}, \\mathrm{B}, \\pi)$，快速有效选择“最优”状态序列$Q=q_1q_2…q_T$解释观察序列 “最优状态序列”的标准不唯一 使该状态序列的每一个状态都单独具有最大概率： $\\gamma_{t}(i)=P(q_{t}=s_{i} \\mid O, \\mu)$最大 贝叶斯：$\\gamma_{t}(i)=P(q_{t}=s_{i} \\mid O, \\mu)=\\frac{P(q_{t}=s_{i}, O \\mid \\mu)}{P(O \\mid \\mu)}$ 前后向算法：$\\gamma_{t}(i)=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{\\sum_{i=1}^{N} \\alpha_{t}(i) \\times \\beta_{i}(i)}$ 时间t最优状态：$\\hat{q}_{t}=\\underset{1 \\leqslant \\leqslant N}{\\operatorname{argmax}}[\\gamma_{i}(i)]$ 断序问题：忽略了状态间的关系，可能导致两状态转移概率为0，则最优状态序列不合法 在给定模型$\\mu$和观察序列$O$的条件下，使条件概率$P（Q\\mid O，\\mu）$最大的状态序列 $\\hat{Q}=\\underset{Q}{\\operatorname{argmax}} P(Q \\mid O, \\mu)$ 避免了断序问题 维特比算法运用DP搜索求解 维特比算法 维特比变量：在时间t时，HMM沿着某一条路径到达状态$s_i$，并输出观察序列$O_1O_2…O_t$的最大概率 $\\delta_{t}(i)=\\max _{q_{1} \\cdot q_{2}, \\cdots, q_{i-1}} P(q_{1}, q_{2}, \\cdots, q_{t}=s_{i}, O_{1} O_{2} \\cdots O_{t} \\mid \\mu)$ 递归关系：$\\delta_{t+1}(i)=\\max _{j}[\\delta_{t}(j) \\cdot a_{j i}] \\cdot b_{i}(O_{t+1})$ 算法描述 初始化： $\\delta_{1}(i)=\\pi_{i} b_{i}(O_{1}), \\quad 1 \\leqslant i \\leqslant N$ $\\psi_{1}(i)=0$ 归纳计算： $\\delta_{i}(j)=\\max _{1 \\leqslant i \\leqslant N}[\\delta_{i-1}(i) \\cdot a_{i j}] \\cdot b_{j}(O_{t}), \\quad 2 \\leqslant t \\leqslant T ; 1 \\leqslant j \\leqslant N$ 记忆回退路径： $\\psi_{t}(j)=\\underset{1 \\leqslant i \\leqslant N}{\\operatorname{argmax}}[\\delta_{i-1}(i) \\cdot a_{i j}] \\cdot b_{j}(O_{t}), \\quad 2 \\leqslant t \\leqslant T ; 1 \\leqslant i \\leqslant N$ 终结： $\\hat{Q}_{T}=\\underset{1 \\leqslant i \\leqslant N}{\\operatorname{argmax}}[\\delta_{T}(i)]$ $\\hat{P}(\\hat{Q}_{T})=\\max _{1 \\leqslant i \\leqslant N}[\\delta_{T}(i)]$ 路径回溯： $\\hat{q}_{t}=\\psi_{t+1}(\\hat{q}_{t-1}), \\quad t=T-1, T-2, \\cdots, 1$ 复杂度：易知，与前后向算法一致，为$O(N^2T)$ 改进：实际应用常求n-best个最佳路径，在格架每个结点记录m-best（m\u003cn）状态 ","date":"2021-01-16","objectID":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/:4:2","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第6章 - 概率图模型","uri":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"6.4.3 HMM参数估计 训练问题/参数估计问题：给定观察序列$O=O_1O_2…O_T$，如何调节模型$\\mu=(\\mathrm{A}, \\mathrm{B}, \\pi)$的参数，使得$P(O\\mid \\mu)$最大： $\\underset{\\mu}{\\arg \\max } P(O_{\\text {training }} \\mid \\mu)$ 模型参数：构成$\\mu$的$\\pi_i, a_{ij}, b_j(k)$ 可以采用最大似然估计： $\\bar{\\pi}_{i}=\\delta(q_{1}, s_{i})$ $\\delta(x, y)$为Kronecker函数，x=y时为1，否则为0 $\\begin{aligned} \\bar{a}_{i j} \u0026=\\frac{Q \\text { 中从状态 } q_{i} \\text { 转移到 } q_{j} \\text { 的次数 }}{ Q \\text { 中所有从状态 } q_{i} \\text { 转移到另一状态(包括 } q_{j} \\text { 自身 }) \\text { 的次数 }} \\\\ \u0026=\\frac{\\sum_{t=1}^{T-1} \\delta(q_{t}, s_{i}) \\times \\delta(q_{t-1}, s_{j})}{\\sum_{t=1}^{T-1} \\delta(q_{t}, s_{i})} \\end{aligned}$ $\\begin{aligned} \\bar{b}_{j}(k) \u0026=\\frac{Q \\text { 中从状态 } q_{j} \\text { 输出符号 } v_{k} \\text { 的次数 }}{Q \\text { 到达 } q_{j} \\text { 的次数 }} \\\\ \u0026=\\frac{\\sum_{t=1}^{T} \\delta(q_{t}, s_{j}) \\times \\delta(O_{t}, v_{k})}{\\sum_{t=1}^{T} \\delta(q_{t}, s_{j})} \\end{aligned}$ 由于HMM的状态序列Q无法观察，因此这种最大似然估计方法不可行，可以采用EM算法 期望最大化（expectation maximization， EM）算法 可用于含有隐变量的统计模型的参数最大似然估计 基本思想（迭代爬山）：在模型参数限制下随机赋值参数，得到模型$\\mu_0$，计算隐变量期望值，用期望替代实际次数（未知）计算新参数值，反复迭代，直到收敛与最大似然估计。 可以达到局部最优 Baum-Welch算法或称前向后向算法（forward-backward algorithm）用于具体实现这种EM方法 Baum-Welch算法/前向后向算法（forward-backward algorithm） 思路： 期望： $\\begin{aligned} \\hat{\\xi}_{t}(i, j) \u0026=P(q_t=s_i, q_{t+1}=s_j\\mid O, \\mu) \\\\\u0026=\\frac{P(q_{t}=s_{i}, q_{t-1}=s_{j}, O \\mid \\mu)}{P(O \\mid \\mu)} \\\\ \u0026=\\frac{\\alpha_{t}(i) a_{i j} b_{j}(O_{t-1}) \\beta_{t+1}(j)}{P(O \\mid \\mu)} \\\\ \u0026=\\frac{\\alpha_{t}(i) a_{i j} b_{j}(O_{t-1}) \\beta_{t-1}(j)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{t}(i) a_{i j} b_{j}(O_{t+1}) \\beta_{t-1}(j)} \\end{aligned}$ $\\gamma_{t}(i)=\\sum_{j=1}^{N} \\hat{\\xi}_{t}(i, j)$ 估计： $\\bar{\\pi}_{i}=P(q_{1}=s_{i} \\mid O, \\mu)=\\gamma_{1}(i)$ $\\begin{aligned} \\bar{a}_{i j} \u0026=\\frac{Q \\text { 中从状态 } q_{i} \\text { 转移到 } q_{j} \\text { 的期望次数 }}{ Q \\text { 中所有从状态 } q_{i} \\text { 转移到另一状态(包括 } q_{j} \\text { 自身 }) \\text { 的期望次数 }} \\\\ \u0026=\\frac{\\sum_{i=1}^{T-1} \\xi_{t}(i, j)}{\\sum_{t=1}^{T-1} \\gamma_{t}(i)} \\end{aligned}$ $\\begin{aligned} \\bar{b}_{j}(k)=\u0026 \\frac{Q \\text { 中从状态 } q_{j} \\text { 输出符号 } v_{k} \\text { 的期望次数 }}{Q \\text { 到达 } q_{j} \\text { 的期望次数 }} \\\\ \u0026=\\frac{\\sum_{i=1}^{T} \\gamma_{t}(j) \\times \\delta(O_{t}, v_{k})}{\\sum_{t=1}^{T} \\gamma_{t}(j)} \\end{aligned}$ 算法描述： 初始化，随机给$\\pi_i, a_{ij}, b_j(k)$赋值，满足约束： $\\sum_{i=1}^{N} \\pi_{i}=1$ $\\sum_{j=1}^{N} a_{i j}=1, 1 \\leqslant i \\leqslant N$ $\\sum_{k=1}^{M} b_{j}(k)=1, 1 \\leqslant j \\leqslant N$ 得到模型$\\mu_0$。令i=0，执行EM估计如下： EM计算 E-步骤：由模型$μ_i$根据期望公式计算期望值$\\xi_t(i, j)$和$\\gamma_t(i)$； M-步骤：用E-步骤得到的期望值，根据估计公式重新估计参数$\\pi_i, a_{ij}, b_j(k)$的值，得到模型$\\mu_{i＋1}$。 循环计算：令i＝i＋1。重复执行EM计算，直到$\\pi_i, a_{ij}, b_j(k)$收敛。 HMM实际应用，注意 多个概率连乘引起浮点数下溢 Viterbi算法只涉及乘法和求最大值，可以对概率连乘取对数，避免下溢并加快运算 前向后向算法中，采用$|\\log {P}(O \\mid \\mu_{i+1})-\\log P({O} \\mid \\mu_{ {i}})|\u003c\\varepsilon$判断收敛。但执行EM计算时有加法运算，这就使得EM计算中无法采用对数运算，在这种情况下，可以设置一个辅助的比例系数，将概率值乘以这个比例系数以放大概率值，避免浮点数下溢。在每次迭代结束重新估计参数值时，再将比例系数取消。 ","date":"2021-01-16","objectID":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/:4:3","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第6章 - 概率图模型","uri":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"6.5 层次化的隐马尔科夫模型hierarchical hidden Markov models, HHMM） 提出原因：NLP应用中，因处理序列具有递归特性，尤其长度较大是，HMM复杂度剧增 HHMM结构：多层随机过程构成。在HHMM中每个状态本身就是一个独立的HHMM，因此一个HHMM的状态产生一个观察序列，而不是一个观察符号。 状态： 终止状态：双圈，用于控制转移过程返回上层状态 生产状态（production state）：只有生产状态才能通过常规HMM机制，即根据输出符号的概率分布产生可观察的输出符号（图中未标出） 内部状态：不直接产生可观察符号的隐藏状态 状态转移： 垂直转移（vertical transition）：不同层间转移 水平转移（horizontal transition）：同层转移 观察序列的产生：状态转移到某生成，产生一个观察输出后，终止状态控制转移过程返回到激活该层状态转移的上层状态。这一递归转移过程将形成一个生产状态序列，而每个生产状态生成一个观察输出符号，因此生产状态序列将为顶层状态生成一个观察输出序列。 形式化描述： 状态$q_i^d(d\\in {1,…, D})$，i为状态下标，d为层次标号 内部状态转移概率矩阵：$\\begin{aligned} A^{q^{d}} \u0026={a_{i j}^{q^{d}}} \\\\ \u0026={P(q_{j}^{d+1} \\mid q_{i}^{d+1})} \\end{aligned}$，其中$a_{i j}^{a^{d}}$表示从状态i水平转移到状态j的概率 子状态初始分布矩阵：$\\begin{aligned} \\Pi^{q^{d}} \u0026={\\pi^{d}(q_{i}^{d+1})} \\\\ \u0026={P(q_{i}^{d+1} \\mid q^{d})} \\end{aligned}$ 参数输出概率矩阵：$\\begin{aligned} B^{q_{i}^{D}} \u0026={b^{q_{i}^{D}}(k)} \\\\ \u0026={P(\\sigma_{k} \\mid q_{i}^{D})} \\end{aligned}$ HHMM参数集合：$\\lambda={{A^{q^{d}}}{\\Pi^{q^{d}}}{B^{q^{D}}}}$ 与HMM一样，HHMM也有估计问题、序列问题和训练问题，详见原文[Fine et al., 1998] ","date":"2021-01-16","objectID":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/:5:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第6章 - 概率图模型","uri":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"6.6 马尔科夫网络 马尔科夫网络 无向图模型，可以表示贝叶斯网络无法表示的一些依赖关系，如循环依赖；另一方面，不能表示贝叶斯网络能够表示的某些关系，如推导关系 一组有关马尔科夫性质的随机变量的联合概率分布模型 由无向图G和定义于G上的势函数组成 完全子图（complete subgraph）又称团（clique） 团的完全子图称为子团 团势能（clique potentials） 无向图不使用条件概率密度对模型进行参数化，使用一种参数化因子：团势能 又称团势能函数/势函数（clique potential function），是定义在团上的非负实函数 每个团对应一个势函数，表示团的一个状态 $\\mathbf{x}_C$来表示团C中所有的结点，用$\\phi(\\mathbf{x}_C)$表示团势能。 如图6-11中团：$\\mathbf{x}_{\\mathbf{C}_{1}}={x_{1}, \\quad x_{2}}, \\quad \\mathbf{x}_{\\mathbf{C}_{2}}={x_{1}, \\quad x_{3}, \\quad x_{4}}$ 势能非负，故一般定义 $\\phi(\\mathbf{x}_C)=\\exp(-E(\\mathbf{x}_C))$，$E(\\mathbf{x}_C)$为$\\mathbf{x}_C$的能量函数 如果分布P $\\phi(x_1，x_2，…，x_n)$的图模型可以表示为一个马尔可夫网络H，当C是H上完全子图的集合时，我们说H上的分布P $\\phi(x_1，x_2，…，x_n)$可以用C的团势能函数$\\phi(\\mathbf{x}_C)$进行因子化：$\\phi＝\\phi_1(\\mathbf{x}_{C_1}),…,\\phi_K(\\mathbf{x}_{C_K})$。P $\\phi(x1，x2，…，xn)$可以看作H上的一个吉布斯分布（Gibbs distribution），其概率分布密度为： $ p(x_{1}, x_{2}, \\cdots, x_{n})=\\frac{1}{Z} \\prod_{i=1}^{K} \\phi_{i}(\\mathbf{x}{C{i}}) $ 其中，Z是一个归一化常量，称为划分函数（partition function）。 其中，$x_{C_i} \\subseteq {x_1，x_2，…，x_n}$（1≤i≤K），并且满足$\\bigcup_{i=1}^{K} x_{C_i}={x_1,x_2,…,x_n }$。 显然，在无向图模型中每个$C_i$对应于一个团，而相应的吉布斯分布就是整个图模型的概率分布。 图6-11中的两个团$x_{C_1}＝{x_1，x_2}$和$x_{C_2}＝{x_1，x_3，x_4}$就可以定义相应的吉布斯分布，因为满足条件$x_{C_1} \\cup x_{C_2}＝{x_1，x_2，x_3，x_4}$。 因子化的乘积运算可以变成加法运算 $p(x_{1}, x_{2}, \\cdots, x_{n})=\\frac{1}{Z} \\exp {-\\sum_{i=1}^{K} E_{c_{i}}(x_{c_{i}})}=\\frac{1}{Z} \\exp {-E(\\mathbf{x})}$ 其中，$\\sum_{i=1}^{K} E_{C_{i}}(x_{C_{i}})$ ","date":"2021-01-16","objectID":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/:6:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第6章 - 概率图模型","uri":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"6.7 最大熵模型 ","date":"2021-01-16","objectID":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/:7:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第6章 - 概率图模型","uri":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"6.7.1 最大熵原理 熵最大的概率概率分布最真实地反应了事件的分布情况，因为熵最大时随机变量最不确定，最难准确预测其行为。 即：在已知部分信息的前提下，关于未知分布最合理的推断应该是符合已知信息最不确定或最大随机的推断 ","date":"2021-01-16","objectID":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/:7:1","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第6章 - 概率图模型","uri":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"6.7.2 最大熵模型的参数训练 最大熵模型参数训练任务：选取有效特征$f_i$及其权重$\\lambda_i$ 各种特征条件和歧义候选可以组合出很多特征函数，必须进行筛选，常用筛选方法： 选取在训练数据中频次超过一定阈值的候选特征 互信息 增量式特征选择方法（比较复杂，不常用） 参数$\\lambda$获取：通用迭代算法（generalized iterative scaling，GIS） ","date":"2021-01-16","objectID":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/:7:2","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第6章 - 概率图模型","uri":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"6.8 最大熵马尔科夫模型（maximum-entropy Markov model, MEMM） 又称条件马尔科夫模型（conditional Markov model，CMM） 结合HMM与最大熵模型特点，广泛用于序列标注问题 HMM存在问题： 很多序列标注任务中，尤其当不能枚举观察输出时，需要用大量特征来刻画观察序列 很多NLP任务中，问题是已知观察序列求解状态序列，HMM采用生成式的联合概率模型（状态序列与观察序列的联合概率$P(S_T, O_T)$）求解这种条件概率问题$P(S_T\\mid O_T)$，不适合处理很多特征描述观察序列的情况 MEMM直接采用条件概率模型$P(S_T\\mid O_T)$，使得观察输出可以用特征表示，借助最大熵框架进行特征选取 HMM与MEMM区别： HMM中$\\mu$解码求解的是：$\\underset{S_{T}}{\\operatorname{argmax}} P(O_{T} \\mid S_{T}, \\mu)$ MEMM中M解码器求解的是：$\\underset{S_{T}}{\\operatorname{argmax}} P(S_{T} \\mid O_{T}, \\mu)$ HMM当前观察输出只取决于当前状态，MEMM当前观察输出还可能取决于前一时刻的状态 MEMM思路 概率因子化为马尔可夫转移概率，该转移概率依赖于当前时刻的观察和前一时刻的状态：$P(S_{1} \\cdots S_{T} \\mid O_{1} \\cdots O_{T})=\\prod_{t=1}^{T} P(S_{t} \\mid S_{t-1}, O_{t})$ 对于前一时刻每个可能的状态取值$S_{t－1}＝s’$和当前观察输出$O_t＝o$，当前状态取值$S_t＝s$的概率通过最大熵分类器建模： $P(s \\mid s^{\\prime}, o)=P_{j}(s \\mid o)=\\frac{1}{Z(o, s)} \\exp (\\sum_{a} \\lambda_{a} f_{a}(o, s))$ $Z(o, s′)$为归一化因子，$f_a(o, s)$为特征函数，$λ_a$为特征函数的 权重，可以利用GIS算法从训练样本中估计出来 $f_{a}(o_{t}, s_{t})=f_{(b, r)}(o_{t}, s_{t})={\\begin{array}{ll}1, \u0026 b(o_{t})=\\text { True, } s_{t}=r \\\\ 0, \u0026 \\text { 其他 }\\end{array}.$ HMM中用于参数估计的Baum-Welch算法修改后可用于MEMM的状 态转移概率估计。 MEMM特点 有向图和无向图的混合模型，主体还是有向图框架。 相比HMM，MEMM最大优点为允许使用任意特征刻画观察序列，这一特性有利于针对特定任务充分利用领域知识设计特征 MEMM比起HMM、CRFs训练更高效，HMM和CRF训练需要前后向算法作为内部循环，MEMM估计状态转移概率可以独立进行 MEMM缺点：标记偏置问题（label bias） ","date":"2021-01-16","objectID":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/:8:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第6章 - 概率图模型","uri":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"6.9 条件随机场 条件随机场（conditional random fields，CRFs） 是用来标注和划分序列结构数据的概率化结构模型 对于给定的输出标识序列Y和观测序列X，CRF通过定义条件概率$P(Y|X)$，而不是联合概率分布$P(X, Y)$来描述模型 CRF也可以看作一个无向图模型或者马尔科夫随机场 CRF定义：无向图每个结点对应随机变量$Y_v$，其取值范围为可能的标记集合${y}$。如果以观察序列X为条件，每一个随机变量$Y_v$都满足以下马尔科夫特性： $p(Y_{v} \\mid X, Y_{w}, w \\neq v)=p(Y_{v} \\mid X, Y_{w}, w \\sim v)$ 其中$w\\sim v$表示两结点邻近。那么，$(X, Y)$为一个条件随机场 CRF也需要解决三类基本问题：特征选取、参数训练和解码 CRF特点： 相比HMM，主要优点是条件随机性，只需要考虑已经出现的观测状态的特性，没有独立性的严格要求，对于整个序列内部的信息和外部观测信息均可有效利用，避免了MEMM和其他针对线性序列模型的条件马尔可夫模型会出现的标识偏置问题。 CRF具有MEMM的一切优点，两者的关键区别在于，MEMM使用每一个状态的指数模型来计算给定前一个状态下当前状态的条件概率，而CRF用单个指数模型来计算给定观察序列与整个标记序列的联合概率。因此，不同状态的不同特征权重可以相互交替代换 ","date":"2021-01-16","objectID":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/:9:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第6章 - 概率图模型","uri":"/blog/snlp-ch6-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"ch3 形式语言与自动机 注：本章笔记参考（王柏、杨娟）《形式语言与自动机》 ","date":"2021-01-15","objectID":"/blog/snlp-ch3-%E5%BD%A2%E5%BC%8F%E8%AF%AD%E8%A8%80%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%9C%BA/:1:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第3章 - 形式语言与自动机","uri":"/blog/snlp-ch3-%E5%BD%A2%E5%BC%8F%E8%AF%AD%E8%A8%80%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%9C%BA/"},{"categories":["NLP"],"content":"3.1 基本概念 图、树、字符串 ","date":"2021-01-15","objectID":"/blog/snlp-ch3-%E5%BD%A2%E5%BC%8F%E8%AF%AD%E8%A8%80%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%9C%BA/:1:1","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第3章 - 形式语言与自动机","uri":"/blog/snlp-ch3-%E5%BD%A2%E5%BC%8F%E8%AF%AD%E8%A8%80%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%9C%BA/"},{"categories":["NLP"],"content":"3.2 形式语言 语言定义和运算 字母表 T：字符的有限集合。 字符串：T中字符构成序列。 字符串运算：concatenation, 逆 $\\omega^T$ 或 $\\overline{\\omega}$、幂、闭包（$T^*, T^+$） 语言：设T为字母表，任何集合$L \\subseteq T^*$是字母表T上的一个语言。 语言运算：并、交、补、差、积、幂 文法 （1）概念 文法：定义语言的数学模型 表示方法： 有限集合：列举法 无线集合：文法产生系统、机器识别系统 元语言：描述语言的语言，文法是一种元语言 对象语言：描述的语言 （2）Chomsky文法体系 可被替代 -\u003e G = (N, T, P, S) N 非终结符的有限集合 T 终结符的有限集合 P 生成式有限集合 S 起始符 （3）推倒与句型 直接推导：由生成式$A\\rightarrow \\beta$得直接推导：$\\alpha A\\gamma \\Rightarrow \\alpha\\beta\\gamma$ 推导序列：称$\\alpha_{0}\\Rightarrow\\alpha_{1}\\Rightarrow\\ldots\\Rightarrow\\alpha_{n}$长度为n的推导序列 推导出：$\\alpha \\xRightarrow[G]{*} \\alpha^{\\prime}$, $\\alpha \\xRightarrow[G]{+} \\alpha^{\\prime}$ 句型：推导序列每一步产生的字符串 句子：只含有终结符句型 语言：句子的集合 （4）Chomsky文法分类 按产生式的形式分类： 分类 别称 特点 对应语言 对应自动机 0型文法 无限制文法PSG 无限制 递归可枚举语言 图灵机TM 1型文法 上下文有关文法CSG 左长小于右 上下文有关语言CSL 线性有界自动机LBA 2型文法 上下文无关文法CFG 左长等于1 上下文无关语言CFL 下推自动机PDA 3型文法 正则RG、左/右线性RLG/LLG 左/右线性 正则语言RL 有限自动机FA 关系：$L\\left(G_{0}\\right) \\supseteqq L\\left(G_{1}\\right) \\supseteqq L\\left(G_{2}\\right) \\supseteqq L\\left(G_{3}\\right)$ CFG识别句子的派生树 派生树也称语法树（syntactic tree）、分析树（parsing tree）、推导树 二义性文法：文法G对于同一个句子的分析树 \u003e= 2 ","date":"2021-01-15","objectID":"/blog/snlp-ch3-%E5%BD%A2%E5%BC%8F%E8%AF%AD%E8%A8%80%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%9C%BA/:1:2","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第3章 - 形式语言与自动机","uri":"/blog/snlp-ch3-%E5%BD%A2%E5%BC%8F%E8%AF%AD%E8%A8%80%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%9C%BA/"},{"categories":["NLP"],"content":"3.3 自动机 （1）有限自动机FA DFA与NFA （2）正则文法与自动机 RG \u003c-\u003e FA （3）CFG与下推自动机PDA CNF(Chomsky Normal Form)文法格式：$A \\rightarrow BC \\mid a$ 2型文法（CFG）可以转换为等价CNF CFG \u003c-\u003e PDA （4）图灵机TM 图灵机与双向有限自动机的区别：图灵机可以改变“带(tape)”上的符号 0型文法 \u003c-\u003e TM （5）线性限界自动机LBA LBA：确定的单带图灵机，其读／写头不能超越原输入带上字符串的初始和终止位置 各类自动机的区别：信息存储空间的差异。 FA：状态 PDA：状态 + 堆栈 LBA：状态 + 输入/输出带 TM：无限制 ","date":"2021-01-15","objectID":"/blog/snlp-ch3-%E5%BD%A2%E5%BC%8F%E8%AF%AD%E8%A8%80%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%9C%BA/:1:3","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第3章 - 形式语言与自动机","uri":"/blog/snlp-ch3-%E5%BD%A2%E5%BC%8F%E8%AF%AD%E8%A8%80%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%9C%BA/"},{"categories":["NLP"],"content":"3.4 自动机在NLP中的应用 有限自动机又称为有限状态机（finite state machine, FSM） 单词拼写检查 [Oflazer,1996]FA用于拼写检查，[Damerau,1964]最小编辑距离 单词形态分析 有限状态转换机（finite state transducer, FST） FST在状态转移时输出，而FA/FSM只转移，不输出 识别heavy单词原型 产生如下两条关于单词heavy的形态分析规则： heavier→heavy＋er heaviest→heavy＋est 词性消歧（part-of-speech tagging） 词性标注方法之一：FST [Roche and Schabes, 1995] 词性标注规则 -\u003e FST FST -\u003e 扩展为全局操作 合并FST为一个 将FST转化为确定的FST ","date":"2021-01-15","objectID":"/blog/snlp-ch3-%E5%BD%A2%E5%BC%8F%E8%AF%AD%E8%A8%80%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%9C%BA/:1:4","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第3章 - 形式语言与自动机","uri":"/blog/snlp-ch3-%E5%BD%A2%E5%BC%8F%E8%AF%AD%E8%A8%80%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%9C%BA/"},{"categories":["NLP"],"content":" 语言模型（language model, LM） 目前主要采用：n元语法模型（n-gram model），构建简单、直接，但同时也因为数据缺乏而必须采取平滑（smoothing）算法 ","date":"2021-01-15","objectID":"/blog/snlp-ch5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:0:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第5章 - 语言模型","uri":"/blog/snlp-ch5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"5.1 n元语法（n-gram） 对于句子$s=w_1w_2…w_l$，其概率计算公式： $$\\begin{aligned} p(s) \u0026=p\\left(w_{1}\\right) p\\left(w_{2} \\mid w_{1}\\right) p\\left(w_{3} \\mid w_{1} w_{2}\\right) \\cdots p\\left(w_{l} \\mid w_{1} \\cdots w_{l-1}\\right) \\\\ \u0026=\\prod_{i=1}^{l} p\\left(w_{i} \\mid w_{1} \\cdots w_{i-1}\\right) \\end{aligned}$$ 为减少自由参数，将历史$w_1w_2…w_{i-1}$映射为等价类$E\\left(w_{1} w_{2} \\ldots w_{i-1}\\right)$，假定 $p\\left(w_{i} \\mid w_{1}, w_{2}, \\cdots, w_{i-1}\\right)=p\\left(w_{i} \\mid E\\left(w_{1}, w_{2}, \\cdots, w_{i-1}\\right)\\right)$ 历史划分成等价类方法：最近n-1词相同（n-gram）： $E\\left(w_{1} w_{2} \\ldots w_{i-1} w_{i}\\right)=E\\left(v_{1} v_{2} \\ldots v_{k-1} v_{k}\\right)$当仅当$\\left(w_{i-n+2} \\ldots w_{i-1} w_{i}\\right)=\\left(v_{k-n+2} \\ldots v_{k-1} v_{k}\\right)$ n-gram 一般取n=3 n=1：词$w_i$独立与历史，一元文法记作uni-gram、monogram n=2：词$w_i$仅与前一个历史词$w_{i-1}$有关，二元文法模型称一阶马尔科夫链（Markov Chain），记作bigram、bi-gram n=3：词$w_i$仅与前两个历史词有关，三元文法称二阶马尔科夫链，记作trigram、tri-gram 二元语法模型： $p(s)=\\prod_{i=1}^{l} p\\left(w_{i} \\mid w_{1} \\ldots w_{i-1}\\right) \\approx \\prod_{i=1}^{l} p\\left(w_{i} \\mid w_{i-1}\\right)$ 假设 $w_0=\u003cBOS\u003e$ 句首标记，结尾 $\u003cEOS\u003e$ 句尾标记 $\\begin{aligned} p(\\text { Mark wrote a book }) \u0026=p(\\text { Mark } \\mid\\langle B O S\\rangle) \\times p(\\text { wrote } \\mid \\text { Mark }) \\\\ \\times p(a \\mid \\text { wrote }) \u0026 \\times p(\\text { book } \\mid a) \\times p(\\langle E O S\\rangle \\mid \\text { book }) \\end{aligned}$ 最大似然估计（maximum likelihood estimation, MLE），统计频率然后归一化得到：$p\\left(w_{i} \\mid w_{i-1}\\right)=\\frac{c\\left(w_{i-1} w_{i}\\right)}{\\sum_{w_{i}} c\\left(w_{i-1} w_{i}\\right)}$ n元语法模型 $p(s)=\\prod_{i=1}^{l-1} p\\left(w_{i} \\mid w_{i-n+1}^{i-1}\\right)$ 约定$w_{-n+2}$到$w_0$为 $\u003cBOS\u003e$ ， $w_{l+1}=\u003cEOS\u003e$ 最大似然估计：$p\\left(w_{i} \\mid w_{i-n+1}^{i-1}\\right)=\\frac{c\\left(w_{i-n+1}^{i}\\right)}{\\sum_{w_{i}} c\\left(w_{i-n+1}^{i}\\right)}$ ","date":"2021-01-15","objectID":"/blog/snlp-ch5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:1:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第5章 - 语言模型","uri":"/blog/snlp-ch5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"5.2 语言模型性能的评价 常用度量： 模型计算出测试数据的概率 对句子$\\left(t_{1}, t_{2}, \\ldots, t_{l_{T}}\\right)$构造的测试集T： $p(T)=\\prod_{i=1}^{l_{T}} p\\left(t_{i}\\right)$ cross-entropy $H_{p}(T)=-\\frac{1}{W_{T}} \\log _{2} p(T)$ 表示利用压缩算法对数据集中$W_T$个词进行编码，每个编码平均比特位数 perplexity 困惑度 $P P_{T}(T)=2^{H P(T)}$ 模型分配给测试集T中每一个词汇的概率的几何平均值的倒数 在英语文本中，n元语法模型计算的困惑度范围大约为50～1000之间（对应的交叉熵范围为6～10个比特位），具体值与文本的类型有关 ","date":"2021-01-15","objectID":"/blog/snlp-ch5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:2:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第5章 - 语言模型","uri":"/blog/snlp-ch5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"5.3 数据平滑 ","date":"2021-01-15","objectID":"/blog/snlp-ch5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:3:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第5章 - 语言模型","uri":"/blog/snlp-ch5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"5.3.1 问题的提出 数据平滑（data smoothing）：避免零概率问题 基本思想：劫富济贫，提高低概率、降低高概率 加一法： $p\\left(w_{i} \\mid w_{i-1}\\right)=\\frac{1+c\\left(w_{i-1} w_{i}\\right)}{\\sum_{w_{i}}\\left[1+c\\left(w_{i-1} w_{i}\\right)\\right]}=\\frac{1+c\\left(w_{i-1} w_{i}\\right)}{|V|+\\sum_{w_{i}} c\\left(w_{i-1} w_{i}\\right)}$ $|V|$为词汇表单词个数 ","date":"2021-01-15","objectID":"/blog/snlp-ch5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:3:1","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第5章 - 语言模型","uri":"/blog/snlp-ch5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"5.3.2 加法平滑方法 假设每一个n元语法发生的次数比实际次数多$\\delta$次，$0 \\leq \\delta \\leq 1$ $p_{\\text {add }}\\left(w_{i} \\mid w_{i-n+1}^{i-1}\\right)=\\frac{\\delta+c\\left(w_{i-n-1}^{i}\\right)}{\\delta|V|+\\sum_{w_{i}} c\\left(w_{i-n+1}^{i}\\right)}$ ","date":"2021-01-15","objectID":"/blog/snlp-ch5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:3:2","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第5章 - 语言模型","uri":"/blog/snlp-ch5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"5.3.2 古德-图灵（Good-Turing）估计法 基本思路：假定出现$r$次的n元语法出现$r^*$次： $r^{*}=(r+1) \\frac{n_{r+1}}{n_{r}}$ $n_r$是训练语料中恰好出现r次的n元语法的数目 归一化： $p_{r}=\\frac{r^{*}}{N}$，$N=\\sum_{r=0}^{\\infty} n_{r} r^{*}$ N等于分布最初计数：$N=\\sum_{r=0}^{\\infty} n_{r} r^{*}=\\sum_{r=0}^{\\infty}(r+1) n_{r+1}=\\sum_{r=1}^{\\infty} n_{r} r$ 所有事件概率和：$\\sum_{r\u003e0} n_{r} p_{r}=1-\\frac{n_{1}}{N}\u003c1$ 有$n_{1} / {N}$的概率分给r=0的未见事件 ","date":"2021-01-15","objectID":"/blog/snlp-ch5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/:3:3","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第5章 - 语言模型","uri":"/blog/snlp-ch5-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"categories":["NLP"],"content":"4.1 语料库技术 ","date":"2021-01-14","objectID":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/:1:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第4章 - 语料库与语言知识库","uri":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"categories":["NLP"],"content":"4.1.1 概述 语料库（corpus base）就是存放语言材料的数据库 语料库语言学（corpus linguistics）就是基于语料库进行语言学研究的一门学问 研究自然语言机读文本（或称“电子文本”）的采集、存储、标注、检索、统计等方法的一门学问，其目的是通过对客观存在的大规模真实文本中的语言事实进行定量分析，为语言学研究或自然语言处理系统开发提供支持。 ","date":"2021-01-14","objectID":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/:1:1","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第4章 - 语料库与语言知识库","uri":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"categories":["NLP"],"content":"4.1.2 语料库语言学的发展 发展阶段 20世纪50年代中期以前：早期的语料库语言学 语言习得、音系研究、方言学与语料库技术的结合 1957年至20世纪80年代初期：沉寂时期 chomsky否定早期语料研究： 语言运用是语言能力的外显，语料经验模式方向有误 语料永远不完整、不充分（如存在递归结构） 20世纪80年代至今：复苏与发展时期 第二代语料库相继建成 基于语料的研究项目大量增加 复苏原因： 规则不能覆盖所有语言事实 CS发展使语料库规模剧增 ","date":"2021-01-14","objectID":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/:1:2","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第4章 - 语料库与语言知识库","uri":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"categories":["NLP"],"content":"4.1.3 语料库的类型 语种：单语种、多语种 记载媒介：单媒体、多媒体 地域：国家、国际 平衡语料库与平行语料库 平衡七项原则[张普, 2003]：语料的真实性、语料的可靠性、语料的科学性、语料的代表性、语料的权威性、语料的分布性和语料的流通性 其中，语料的分布性还要考虑语料的科学领域分布、地域分布、时间分布和语体分布等。 平行语料库： 同一语言语料平行：语料选取的时间、对象、比例、文本数、文本长度等几乎是一致的 多语言平行采样加工 用途 通用语料库 专用语料库 分布时间 共时语料库 共时研究：研究大树的横断面所见的细胞和细胞关系，即研究一个共时平面中的元素与元素的关系 历时语料库 历时研究：研究一个历时切面中元素与元素关系的演化 判断历时语料库原则 是否动态：语料库必须是开放的、动态的； 本是否具有量化的流通度属性：所有的语料都应来源于大众传媒， 具有与传媒特色相应的流通度属性。 其量化的属性值也是动态的； 深加工是否基于动态的加工方法：随语料的动态变化采集， 并进行动态地加工； 是否取得动态的加工结果：语料的加工结果也应是动态的和历时的。 加工程度 生语料 标注语料库 具有词性标注 句法结构信息标注（树库） 语义信息标注 ","date":"2021-01-14","objectID":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/:1:3","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第4章 - 语料库与语言知识库","uri":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"categories":["NLP"],"content":"4.1.4 汉语语料库建设中的问题 语料库建设的规范问题 产权保护和国家语料库建设问题 ","date":"2021-01-14","objectID":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/:1:4","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第4章 - 语料库与语言知识库","uri":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"categories":["NLP"],"content":"4.1.5 典型语料库介绍 LDC中文树库（Chinese Tree Bank, CTB） 在CTB中，汉语词性被划分为33类 包括4类动词和谓语性形容词（Verb, Adjective，分别记作： VC, VE, VV, VA） 3类名词（Noun，分别记作：NR, NT, NN） 1类处所词（Localizer，记作：LC） 1类代词（Pronoun, PN） 3类限定词和数词（Determiner and Number，分别记作：DT, CD, OD） 1类量词（Measure word，记作：M） 1类副词（Adverb，记作：AD） 1类介词（Preposition，记作：P） 2类连词（Conjunction，分别记作：CC,CS） 8类语气词（Particle，分别记作：DEC, DEG, DER, DEV, SP, AS, ETC, MSP） 8类包括外来词、标点、感叹词等在内的其他词类（分别记作：IJ, ON, PU, JJ, FW, LB, SB,BA） CTB包括23类句法标记（syntactic tag） 17类短语：形容词短语（adjective phrase, ADJP）、副词开头的副词短语（adverbial phrase headed by AD, ADVP）、量词短语（classifier phrase, CLP）、补语性嵌套句的从属连词引起的分句（clause headed by complementizer, CP）、XP＋DEG结构构成的短语（DNP）、限定词短语（determinerphrase,DP）、XP＋DEV结构构成的短语（DVP）、片段语（fragment,FRAG）、简单分句（simple clause headed by INFL, IP）、XP＋LC结构构成的短语（LCP）、用于解释说明性的列表标记短语（listmarker,LST）、名词短语（noun phrase, NP）、介词短语（preposition, PP）、插入语（parenthetical, PRN）、数量词短语（quantifier phrase, QP）、非一致性并列短语（unidentical coordination phrase, UCP）和动词短语（verb phrase, VP） 6个动词复合形式的标记（VCD, VCP,VNV, VPT, VRD, VSB） 一些句法结构成分标记，如：主语（-SBJ）、谓语（-PRD）、宾语（-OBJ）等。 命题库（PropBank）、名词化树库（NomBank）和语篇树库（Penn Discourse Tree Bank, PDTB） PropBank的目标是对原树库中的句法结点标注上特定的论元标记（argument label） ， 使其保持语义角色的相似性 NomBank标注的是树库中名词的词义和相关的论元信息。 目标是开发一个标注语篇结构信息的大规模语料库， 主要标注与语篇连通方式（discourse connectives） 相关的一致关系（coherence relation） 布拉格依存树库（Prague Dependency Treebank, PDT） PDT包含三个层次： 形态层(morphological layer)：PDT的最低层， 包含全部的形态信息标注； 分析层(analytic layer)：PDT的中间层， 主要是依次关系中的表层句法信息标注， 层次概念上接近于Penn Treebank中的句法标注； 深层语法层(tectogrammatical layer)：PDT的最高层， 表达句子的深层语法结构。 BTEC口语语料 其目标是开展语音翻译的国际合作研究， 开发实用的语音翻译技术。 现代汉语口语语料库 台湾中研院语料库 LDC语音数据库 ","date":"2021-01-14","objectID":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/:1:5","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第4章 - 语料库与语言知识库","uri":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"categories":["NLP"],"content":"4.2 语言知识库 语言知识库：词汇知识库、句法规则库，还是语法信息库、语义概念库 “语言知识库”比“语料库”包含更广泛的内容 一类是词典、规则库、语义概念库等，其中的语言知识表示是显性的，可采用形式化结构描述 另一类语言知识存在于语料库之中，每个语言单位的出现，其范畴、意义、用法都是确定的 ","date":"2021-01-14","objectID":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/:2:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第4章 - 语料库与语言知识库","uri":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"categories":["NLP"],"content":"4.2.1 WordNet 英语机读词汇知识库 WordNet的建立有三个基本前提： “可分离性假设（separability hypothesis）”，即语言的词汇成分可以被离析出来并专门针对它加以研究。 “模式假设（patterning hypothesis）”：一个人不可能掌握他运用一种语言所需的所有词汇，除非他能够利用词义中存在的系统的模式和词义之间的关系。 “广泛性假设（comprehensiveness hypothesis）”：计算语言学如果希望能像人那样处理自然语言，就需要像人那样储存尽可能多的词汇知识［Miller et al.,1993］。 WordNet是一个按语义关系网络组织的巨大词库，多种词汇关系和语义关系被用来表示词汇知识的组织方式。词形式（word form）和词义（word meaning）是WordNet源文件中可见的两个基本构件，词形式以规范的词形表示，词义以同义词集合（synset）表示。词汇关系是两个词形式之间的关系，而语义关系是两个词义之间的关系。 ","date":"2021-01-14","objectID":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/:2:1","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第4章 - 语料库与语言知识库","uri":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"categories":["NLP"],"content":"4.2.2 FrameNet FrameNet是基于框架语义学（frame semantics）并以语料库为基础建立的在线英语词汇资源库，其目的是通过样本句子的计算机辅助标注和标注结果的自动表格化显示，来验证每个词在每种语义下语义和句法结合的可能性（配价，valence）范围 ","date":"2021-01-14","objectID":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/:2:2","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第4章 - 语料库与语言知识库","uri":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"categories":["NLP"],"content":"4.2.3 EDR EDR电子词典（EDR Electronic Dictionary）是由日本电子词典研究院（Japan Electronic Dictionary Research Institute, Ltd.）开发的面向自然语言处理的词典。该词典由11个子词典（sub-dictionary）组成，包括概念词典、词典和双语词典等。 ","date":"2021-01-14","objectID":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/:2:3","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第4章 - 语料库与语言知识库","uri":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"categories":["NLP"],"content":"4.2.4 北京大学综合型语言知识（CLKB） 现代汉语语法信息词典（grammatical knowledge base, GKB），含8万词的360万项语法属性描述； 汉语短语结构规则库，含600多条语法规则； 现代汉语多级加工语料库（word-sense tagging corpus, STC），实现词语切分并标注词类的基本标注语料库1.5亿字，其中精加工的有5200万字，标注义项的有2800万字； 多语言概念词典，含10万个以同义词集表示的概念； 平行语料库，含对译的英汉句对100万； 多领域术语库，有35万汉英对照术语。 ","date":"2021-01-14","objectID":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/:2:4","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第4章 - 语料库与语言知识库","uri":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"categories":["NLP"],"content":"4.2.5 知网（HowNet） 知网（HowNet）是机器翻译专家董振东和董强经过十多年的艰苦努力创建的语言知识库，是一个以汉语和英语的词语所代表的概念为描述对象，以揭示概念与概念之间以及概念所具有的属性之间的关系为基本内容的常识知识库。 知网是一个知识系统，而不是一部语义词典。知网用概念与概念之间的关系以及概念的属性与属性之间的关系形成一个网状的知识系统，这是它与其他树状词汇数据库的本质不同。 义原（Sememes）：原子语义，最基本的、不易于再分割其意义的最小单位 ","date":"2021-01-14","objectID":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/:2:5","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第4章 - 语料库与语言知识库","uri":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"categories":["NLP"],"content":"4.2.6 概念层次网络（Hierarchical Network of Concepts, HNC） 概念层次网络（Hierarchical Network of Concepts, HNC）是中国科学院声学研究所黄曾阳建立的面向整个自然语言理解的理论框架 局部联想脉络是HNC理论的基本内容之一，它由五元组、语义网络和概念组合结构组成，它是计算机把握并理解语言概念的基本前提，其基本思路和做法是：把概念分为抽象概念和具体概念，对抽象概念用语义网络和五元组来表达，对具体概念采取挂靠展开近似表达的方法。 在HNC理论中，五元组、语义网络和概念组合结构用来表达抽象概念。五元组是指{动态、静态、属性、值、效应}五大特性，它们是词性的基元，用以表达概念的外在表现。 三大语义网络 基本概念语义网络 基元概念语义网络 逻辑概念语义网络 ","date":"2021-01-14","objectID":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/:2:6","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第4章 - 语料库与语言知识库","uri":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"categories":["NLP"],"content":"4.3 语言知识库与本体论 ","date":"2021-01-14","objectID":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/:3:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第4章 - 语料库与语言知识库","uri":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"categories":["NLP"],"content":"本体概念 本体论（ontology）：关于“onto”的“logos”，即研究一切有关“存在”（希腊文onto是“存在、有、是”的意思，英文译为being）的学问或理念（logos）。 本体论补充 即“是什么”，连接主词与谓词的系词问题 我们应该如何把握抽象的对象？ 巴门尼德：如果我们对主词一无所知，那么它的所有规定性都来自于它的谓词。 世界的本源是不生不灭的、单一的、完美无缺的。 亚里士多德：不断追溯更高的属并找到种差就可以定义万事万物，但上溯到最终会发现有一件事物无法定义，没有更高的属，也不存在种差，即being（存在） 后人将亚里士多德使用系词结构“X是什么”对事物下定义，不可避免地遇到的一系列本质问题，成为形而上学（metaphysics） 计算机科学领域中，本体（Ontology）是共享概念模型明确的形式化规范说明。 本体定义的四层含义 概念模型（Conceptualization）：通过抽象出客观世界中一些现象的相关概念而得到的模型，其表示的含义独立于具体的环境状态。 明确（Explicit）：所使用的概念及使用这些概念的约束都有明确定义。 形式化（Formal）：本体是计算机可读的，即计算机可处理。 共享（Share）：共同认可的知识，反映相关领域中公认的概念集，它所针对的是团体而非个体。 本体的目标：捕获相关领域的知识 语义网（Semantic Web）：希望构建一个通用本体，提供一种计算机能够理解的、结构化的语义描述机制，及一系列推理规则以实现自动化推理。语义网将不仅仅为人类更能为计算机带来语义内容，使计算机能够“理解”Web内容，进而实现信息处理的自动化。 为什么用本体？ 在万维网中，我们可能会用不同的术语来表达相同的含义，或者一个术语含有多个含义。因此，消除术语差异是很有必要的。目前我们的解决方案就是，对某个领域建立一个公共的本体。鼓励大家在涉及该领域的时候都使用公共本体里的术语和规则。 知识复用问题 概念化（conceptualization）是知识形式化表达的基础，是所关心领 域中的对象、概念和其他实体，以及它们之间的关系 T.R.Gruber: An ontology is an explicit specification of a conceptualization. 每一个知识库、基于知识库的信息系统以及基于知识共享的智能agent都内含一个概念化的世界，或是显式的，或是隐式的。而本体论是对某一概念化所作的一种显式解释说明。本体论中的对象以及它们之间的关系是通过知识表达语言的词汇来描述的。因此，可以通过定义一套知识表达的专门术语来定义一个本体，以人可以理解的术语描述领域世界的实体、对象、关系以及过程等，并通过形式化的公理来限制和规范这些术语的解释和使用。 本体就是一个描述特定领域概念的知识库，其内容不仅包括领域的主要概念，还包括它们之间的关系。面向不同应用的系统都可以利用本体所提供的领域知识完成特定的任务，例如事件信息抽取，信息检索等。 本体是语言相关的，而概念化则是语言无关的。概念化是比本体论（仅限于信息科学中）更为广泛的概念，前者更接近领域的事实和哲学上的本体论 本体的核心概念是知识共享，通过减少概念和术语上的歧义，建立一个统一的框架或规范模型，使得来自不同背景、持不同观点和目的的人员之间的理解和交流，以及不同系统之间的互操作或数据传输成为可能，并保持语义上的一致。 在一个领域中，本体构成了该领域任意知识表达系统的核心 ","date":"2021-01-14","objectID":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/:3:1","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第4章 - 语料库与语言知识库","uri":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"categories":["NLP"],"content":"本体的构建 方式：自顶向下、自底向上 自动本体构建 第一部分用于确定领域中的概念集合 第二部分是关系发现（relationship discovery），用以识别和提取概念之间的关系。 核心领域本体是对领域中的核心概念进行建模。 本体构建与语言知识库建设密切联系 一方面，本体理论对于语言知识库的建设具有一定的借鉴意义。 另一方面，本体的构建过程又离不开自然语言处理技术。 本体语言：XML、XML Schema、RDF、RDF Schema、OWL 本体可视化： ","date":"2021-01-14","objectID":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/:3:2","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第4章 - 语料库与语言知识库","uri":"/blog/snlp-ch4-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"categories":["NLP"],"content":"ch2 预备知识 ","date":"2021-01-14","objectID":"/blog/snlp-ch2-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%A6%82%E7%8E%87%E8%AE%BA%E4%BF%A1%E6%81%AF%E8%AE%BAsvm/:1:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第2章 - 预备知识：概率论、信息论、SVM","uri":"/blog/snlp-ch2-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%A6%82%E7%8E%87%E8%AE%BA%E4%BF%A1%E6%81%AF%E8%AE%BAsvm/"},{"categories":["NLP"],"content":"2.1 概率论 最大似然估计：$q_{N}\\left(s_{k}\\right)=\\frac{n_{N}\\left(s_{k}\\right)}{N}$， $\\lim _{N \\rightarrow \\infty} q_{N}\\left(s_{k}\\right)=P\\left(s_{k}\\right)$ 条件概率：$P(A \\mid B)=\\frac{P(A \\cap B)}{P(B)}$ 乘法规则：$P(A \\cap B)=P(B) P(A \\mid B)=P(A) P(B \\mid A)$ $P\\left(A_{1} \\cap \\cdots \\cap A_{n}\\right)=P\\left(A_{1}\\right) P\\left(A_{2} \\mid A_{1}\\right) P\\left(A_{3} \\mid A_{1} \\cap A_{2}\\right) \\cdots P\\left(A_{n} \\mid \\bigcap_{i=1}^{n-1} A_{i}\\right)$ 全概率公式：$P(A)=\\sum_{i} P\\left(A \\mid B_{i}\\right) P\\left(B_{i}\\right)$ 贝叶斯法则：$P\\left(B_{j} \\mid A\\right)=\\frac{P\\left(A \\mid B_{j}\\right) P\\left(B_{j}\\right)}{P(A)}=\\frac{P\\left(A \\mid B_{j}\\right) P\\left(B_{j}\\right)}{\\sum_{i=1}^{n} P\\left(A \\mid B_{i}\\right) P\\left(B_{i}\\right)}$ 随机变量X的分布函数：$P(X \\leqslant x)=F(x), \\quad-\\infty\u003cx\u003c\\infty$ 二项式分布$\\mathrm{X} \\sim \\mathrm{B}(\\mathrm{n}, \\mathrm{p})$：$p_{i}=\\left(\\begin{array}{c}n \\\\ i\\end{array}\\right) p^{i}(1-p)^{n-i}, \\quad i=0,1, \\cdots, n$ $(X_1, X_2)$ 的联合分布：$p_{ij}=P\\left(X_{1}=a_{i}, X_{2}=b_{j}\\right), \\quad i=1,2, \\ldots ; j=1,2, \\ldots$ 条件概率分布：$P\\left(X_{1}=a_{i} \\mid X_{2}=b_{j}\\right)=\\frac{p_{i j}}{\\sum_{k} p_{k j}}, \\quad i=1,2, \\cdots$ 贝叶斯决策理论： $P\\left(w_{i} \\mid x\\right)=\\frac{p\\left(x \\mid w_{i}\\right) P\\left(w_{i}\\right)}{\\sum_{j=1}^{c} p\\left(x \\mid w_{j}\\right) P\\left(w_{j}\\right)}$ $p\\left(x \\mid w_{i}\\right) P\\left(w_{i}\\right)=\\max _{j=1,2, \\cdots, c} p\\left(x \\mid w_{j}\\right) P\\left(w_{j}\\right),$ 则 $x \\in w_{i}$ 随机变量$X$的期望（rhs级数收敛时）：$E(X)=\\sum_{k=1}^{\\infty} x_{k} p_{k}$ 随机变量$X$的方差：$\\begin{aligned} \\operatorname{var}(X) \u0026=E\\left((X-E(X))^{2}\\right) \\\\ \u0026=E\\left(X^{2}\\right)-E^{2}(X) \\end{aligned}$ ","date":"2021-01-14","objectID":"/blog/snlp-ch2-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%A6%82%E7%8E%87%E8%AE%BA%E4%BF%A1%E6%81%AF%E8%AE%BAsvm/:1:1","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第2章 - 预备知识：概率论、信息论、SVM","uri":"/blog/snlp-ch2-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%A6%82%E7%8E%87%E8%AE%BA%E4%BF%A1%E6%81%AF%E8%AE%BAsvm/"},{"categories":["NLP"],"content":"2.2 信息论 熵 离散型随机变量$X$的熵：$H(X)=-\\sum_{x \\in \\mathbf{R}} p(x) \\log _{2} p(x)$ 又称自信息（Self-information） 描述不确定性 最大熵：在已知部分知识的前提下，关于未知分布最合理的推断应该是符合已知知识最不确定或最大随机的推断。 用熵最大的模型推断某种语言现象的可能性（？）：$\\hat{p}=\\underset{p \\in C}{\\operatorname{argmax}} H(p)$ 联合熵与条件熵 联合熵：$H(X, Y)=-\\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(x, y)$ 一对随机变量平均所需信息量 条件熵：$\\begin{aligned} H(Y \\mid X) \u0026=\\sum_{x \\in X} p(x) H(Y \\mid X=x) \\\\ \u0026=\\sum_{x \\in X} p(x)\\left[-\\sum_{y \\in Y} p(y \\mid x) \\log p(y \\mid x)\\right] \\\\ \u0026=-\\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(y \\mid x) \\end{aligned}$ 熵的连锁规则： $\\begin{aligned} H(X, Y) \u0026=-\\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log [p(x) p(y \\mid x)] \\\\ \u0026=-\\sum_{x \\in X} \\sum_{y \\in Y} p(x, y)[\\log p(x)+\\log p(y \\mid x)] \\\\ \u0026=-\\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(x)-\\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(y \\mid x) \\\\ \u0026=-\\sum_{x \\in X} p(x) \\log p(x)-\\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(y \\mid x) \\\\ \u0026=H(X)+H(Y \\mid X) \\end{aligned}$ 一般情况：$H\\left(X_{1}, X_{2}, \\ldots, X_{n}\\right)=H\\left(X_{1}\\right)+H\\left(X_{2} \\mid X_{1}\\right)+H\\left(X_{3} \\mid X_{2}, X_{2}\\right)+\\ldots+H\\left(X_{n} \\mid X_{n-1}, X_{n-2}, \\ldots, X_{1}\\right)$ 字符串的熵率：$H_{\\mathrm{rate}}=\\frac{1}{n} H\\left(X_{1 n}\\right)=-\\frac{1}{n} \\sum_{x_{1 y}} p\\left(x_{1 n}\\right) \\log p\\left(x_{1 n}\\right)$ 某语言为随机过程$L=\\left(X_{i}\\right)$，其熵率：$H_{\\text {rate }}(L)=\\lim _{n \\rightarrow \\infty} \\frac{1}{n} H\\left(X_{1}, X_{2}, \\cdots, X_{n}\\right)$ 互信息（mutual information, MI） $H(X)-H(X \\mid Y)=H(Y)-H(Y \\mid X)$ $\\begin{aligned} I(X ; Y) \u0026=H(X)-H(X \\mid Y) \\\\ \u0026=H(X)+H(Y)-H(X, Y) \\\\ \u0026=\\sum_{x} p(x) \\log \\frac{1}{p(x)}+\\sum_{y} p(y) \\log \\frac{1}{p(y)}+\\sum_{x, y} p(x, y) \\log p(x, y) \\\\ \u0026=\\sum_{x, y} p(x, y) \\log \\frac{p(x, y)}{p(x) p(y)} \\end{aligned}$ 知道Y后X的不确定性的减少量 体现了两个变量的依赖程度 完全依赖：$I(X ; X)=H(X)-H(X \\mid X)=H(X)$，故熵也称自信息 完全独立：$I(X ; Y)=0,$ 即 $p(x, y)=p(x) p(y)$ $I(X ; Y)\\gg 0$：高度相关 $I(X ; Y)\\ll 0$：Y加大X的不确定性 条件互信息：$I(X ; Y \\mid Z)=I((X ; Y) \\mid Z)=H(X \\mid Z)-H(X \\mid Y, Z)$ 互信息连锁规则：$\\begin{aligned} I\\left(X_{1 n} ; Y\\right) \u0026=I\\left(X_{1}, Y\\right)+\\cdots+I\\left(X_{n} ; Y \\mid X_{1}, \\cdots, X_{n-1}\\right) \\\\ \u0026=\\sum_{i=1}^{n} I\\left(X_{i} ; Y \\mid X_{1}, \\cdots, X_{i-1}\\right) \\end{aligned}$ 相对熵（relative entropy） 又称Kullback-Leibler差异（Kullback-Leibler divergence），或简称KL距离/KL散度 $D(p | q)=\\sum_{x \\in X} p(x) \\log \\frac{p(x)}{q(x)}=E_{p}\\left(\\log \\frac{p(x)}{q(x)}\\right)$ 互信息是联合分布与独立分布的相对熵 $I(X ; Y)=\\sum_{x, y} p(x, y) \\log \\frac{p(x, y)}{p(x) p(y)}=D(p(x, y) | p(x) p(y))$ 条件相对熵：$D(p(y \\mid x) | q(y \\mid x))=\\sum_{x} p(x) \\sum_{y} p(y \\mid x) \\log \\frac{p(y \\mid x)}{q(y \\mid x)}$ 相对熵连锁规则：$D(p(x, y) | q(x, y))=D(p(x) | q(x))+D(p(y \\mid x) | q(y \\mid x))$ 交叉熵（cross entropy） 随机变量X和模型q的交叉熵： $\\begin{aligned} H(X, q) \u0026=H(X)+D(p | q) \\\\ \u0026=-\\sum_{x} p(x) \\log q(x) \\\\ \u0026=E_{p}\\left(\\log \\frac{1}{q(x)}\\right) \\end{aligned}$ 语言$L＝(X_i)～p(x)$与其模型q的交叉熵： $H(L, q)=-\\lim _{n \\rightarrow \\infty} \\frac{1}{n} \\sum_{x_{1}^{n}} p\\left(x_{1}^{n}\\right) \\log q\\left(x_{1}^{n}\\right)$ 假定语言L是稳态（stationary）遍历的（ergodic）随机过程，则： $H(L, q)=-\\lim _{n \\rightarrow \\infty} \\frac{1}{n} \\log q\\left(x_{1}^{n}\\right)$ n足够大时，近似为$-\\frac{1}{N} \\log \\left(q\\left(x_{1}^{N}\\right)\\right)$，交叉熵越小表示模型越接近真实语言模型 困惑度（perplexity） $\\mathrm{PP}_{q}=2^{H(L, q)} \\approx 2^{-\\frac{1}{n} \\log q\\left(i_{1}^{n}\\right)}=\\left[q\\left(l_{1}^{n}\\right)\\right]^{-\\frac{1}{n}}$ 等价地，语言模型设计的任务就是寻找（对于测试数据）困惑度最小的模型 噪声信道模型（noisy channel model） 二进制对称信道（binary symmetric channel, BSC） 信道容量（capacity）：$C=\\max _{p(X)} I(X ; Y)$ 用降低传输速率来换取高保真通信的可能性 即平均互信息量的最大值 NLP不需要编码，句子可视为已经编码的符号序列： 给定输出求最可能输入： 贝叶斯公式：$\\begin{aligned} \\hat{I} \u0026=\\underset{I}{\\operatorname{argmax}} p(I \\mid O)=\\underset{I}{\\operatorname{argmax}} \\frac{p(I) p(O \\mid I)}{p(O)} \\\\ \u0026=\\underset{I}{\\operatorname{argmax}} p(I) p(O \\mid I) \\end{aligned}$ $p(I)$为语言模型（language model），是指在输入语言中“词”序列的概率分布 $p(O \\mid I)$为信道概率（channel probability） ","date":"2021-01-14","objectID":"/blog/snlp-ch2-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%A6%82%E7%8E%87%E8%AE%BA%E4%BF%A1%E6%81%AF%E8%AE%BAsvm/:1:2","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第2章 - 预备知识：概率论、信息论、SVM","uri":"/blog/snlp-ch2-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%A6%82%E7%8E%87%E8%AE%BA%E4%BF%A1%E6%81%AF%E8%AE%BAsvm/"},{"categories":["NLP"],"content":"2.3 支持向量机（support vector machine, SVM） 线性分类 $\\begin{aligned} f(x) \u0026=\\langle w \\cdot x\\rangle+b \\\\ \u0026=\\sum_{i=1}^{n} w_{i} x_{i}+b \\end{aligned}$ 最优超平面： 以最大间隔分开数据 多分类问题： 每类一个超平面 决策函数：$c(x)=\\underset{1 \\leqslant i \\leqslant m}{\\operatorname{argmax}}\\left(\\left\\langle w_{i} \\cdot x\\right\\rangle+b_{i}\\right)$ 线性不可分 非线性问题：映射样本x到高维特征空间，再使用线性分类器 假设集：$f(x)=\\sum_{i=1}^{N} w_{i} \\varphi_{i}(x)+b$ 决策规则：$f(x)=\\sum_{i=1}^{l} \\alpha_{i} y_{i}\\left\\langle\\varphi\\left(x_{i}\\right) \\cdot \\varphi(x)\\right\\rangle+b$ 线性分类器重要性质：可以表示成对偶形式 决策规则（分类函数）可以用测试点和训练点的内积来表示 核（kernel）函数方法 用原空间中的函数实现高维特征空间的内积 构造核函数 $K(x, z)=\\langle\\varphi(x) \\cdot \\varphi(z)\\rangle$ 决策规则：$f(x)=\\sum_{i=1}^{l} \\alpha_{i} y_{i} K\\left(x_{i}, x\\right)+b$ 核函数必要条件： 对称：$K(x, z)=\\langle\\varphi(x) \\cdot \\varphi(z)\\rangle=\\langle\\varphi(z) \\cdot \\varphi(x)\\rangle=K(z, x)$ 内积性质：$\\begin{aligned} K(x, z)^{2} \u0026=\\langle\\varphi(x) \\cdot \\varphi(z)\\rangle^{2} \\leqslant|\\varphi(x)|^{2}|\\varphi(z)|^{2} \\\\ \u0026=\\langle\\varphi(x) \\cdot \\varphi(x)\\rangle\\langle\\varphi(z) \\cdot \\varphi(z)\\rangle=K(x, x) K(z, z) \\end{aligned}$ 核函数充分条件：Mercer定理条件，X的任意有限子集，相应的矩阵是半正定的。 核函数充分必要条件 矩阵$K=\\left(K\\left(x_{i}, x_{j}\\right)\\right)_{i, j=1}^{n}$半正定（即特征值非负） 常用核函数：多项式核函数、径向基函数、多层感知机、动态核函数等 ","date":"2021-01-14","objectID":"/blog/snlp-ch2-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%A6%82%E7%8E%87%E8%AE%BA%E4%BF%A1%E6%81%AF%E8%AE%BAsvm/:1:3","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第2章 - 预备知识：概率论、信息论、SVM","uri":"/blog/snlp-ch2-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%E6%A6%82%E7%8E%87%E8%AE%BA%E4%BF%A1%E6%81%AF%E8%AE%BAsvm/"},{"categories":["NLP"],"content":"ch0 序 本书框架 NLP理论 ch1-9 基础：概率论、信息论、形式语言与自动机 语料库和词汇知识库 语言模型 隐马尔可夫模型 词法分析、词性标注 句法分析 语义分析 NLP应用 ch10-15 机器翻译、语音翻译、文本分类、信息检索、问答系统、知识抽取、口语信息处理与人际对话系统 理论基础 基于统计的NLP：经验主义 基于规则的NLP：理性主义 ie. symbolic approach! 中文房间的个人思考 房中人不懂汉语，只懂英语，而手册是进行语言转换，假设各语言表现力是等价的，那么中文和英文是同一语义的不同表现形式，于是我们可以说房中人简介理解了中文的含义（即获得与语义），这与智能是相互独立的 “房中人不能理解中文”不成立？ ","date":"2021-01-13","objectID":"/blog/snlp-ch1-%E7%BB%AA%E8%AE%BA/:1:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第1章 - 绪论","uri":"/blog/snlp-ch1-%E7%BB%AA%E8%AE%BA/"},{"categories":["NLP"],"content":"ch1 绪论 ","date":"2021-01-13","objectID":"/blog/snlp-ch1-%E7%BB%AA%E8%AE%BA/:2:0","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第1章 - 绪论","uri":"/blog/snlp-ch1-%E7%BB%AA%E8%AE%BA/"},{"categories":["NLP"],"content":"1.1 基本概念 linguistic sciences 语言学（linguistics）是指对语言的科学研究。 语音学（phonetics）是研究人类发音特点，特别是语音发音特点，并提出各种语音描述、分类和转写方法的科学。 默认：NLP = NLU = 计算语言学 “理解”的标准：图灵测试 ","date":"2021-01-13","objectID":"/blog/snlp-ch1-%E7%BB%AA%E8%AE%BA/:2:1","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第1章 - 绪论","uri":"/blog/snlp-ch1-%E7%BB%AA%E8%AE%BA/"},{"categories":["NLP"],"content":"1.2 研究内容与困难 主要研究内容 机器翻译（machine translation, MT） 自动文摘（automatic summarizing或automatic abstracting） 信息检索（information retrieval） 文档分类（document categorization/classification） 问答系统（question-answering system） 信息过滤（information filtering） 信息抽取（information extraction） 文本挖掘（text mining） 舆情分析（public opinion analysis） 隐喻计算（metaphorical computation） 文字编辑和自动校对（automatic proofreading） 作文自动评分 光读字符识别（optical character recognition, OCR） 语音识别（speech recognition） 文语转换（text-to-speech conversion） 说话人识别／认证／验证（speaker recognition/identification/verification） 层次 形态学（morphology）：形态学（又称“词汇形态学”或“词法”）是语言学的一个分支，研究词的内部结构，包括屈折变化和构词法两个部分。 语法学（syntax）：研究句子结构成分之间的相互关系和组成句子序列的规则。其关注的中心是：为什么一句话可以这么说，也可以那么说？ 语义学（semantics）：是一门研究意义，特别是语言意义的学科.其重点在探明符号与符号所指的对象之间的关系，从而指导人们的言语活动。它所关注的重点是：这个语言单位到底说了什么？ 语用学（pragmatics）：是现代语言学用来指从使用者的角度研究语言，特别是使用者所作的选择、他们在社会互动中所受的制约、他们的语言使用对信递活动中其他参与者的影响。 面临的困难 歧义消解（disambiguation）问题 未知语言现象的处理问题 ","date":"2021-01-13","objectID":"/blog/snlp-ch1-%E7%BB%AA%E8%AE%BA/:2:2","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第1章 - 绪论","uri":"/blog/snlp-ch1-%E7%BB%AA%E8%AE%BA/"},{"categories":["NLP"],"content":"1.3 基本方法及其发展 两种研究方法 理性主义（rationalist）方法：主张建立符号处理系统，由人工整理和编写初始的语言知识表示体系（通常为规则），构造相应的推理程序，系统根据规则和程序，将自然语言理解为符号结构——该结构的意义可以从结构中的符号的意义推导出来。按照这种思路，在自然语言处理系统中，一般首先由词法分析器按照人编写的词法规则对输入句子的单词进行词法分析，然后，语法分析器根据人设计的语法规则对输入句子进行语法结构分析，最后再根据一套变换规则将语法结构映射到语义符号（如逻辑表达式、语义网络、中间语言等）。 经验主义（empiricist）方法：也是从假定人脑所具有的一些认知能力开始的。因此，从这种意义上讲，两种方法并不是绝对对立的。但是，经验主义的方法认为人脑并不是从一开始就具有一些具体的处理原则和对具体语言成分的处理方法，而是假定孩子的大脑一开始具有处理联想（association）、模式识别（patternrecognition）和通用化（generalization）处理的能力，这些能力能够使孩子充分利用感官输入来掌握具体的自然语言结构。在系统实现方法上，经验主义方法主张通过建立特定的数学模型来学习复杂的、广泛的语言结构，然后利用统计学、模式识别和机器学习等方法来训练模型的参数，以扩大语言使用的规模。因此，经验主义的自然语言处理方法是建立在统计方法基础之上的，因此，我们又称其为统计自然语言处理（statisticalnaturallanguageprocessing）方法。 在统计自然语言处理方法中，一般需要收集一些文本作为统计模型建立的基础，这些文本称为语料（corpus）。经过筛选、加工和标注等处理的大批量语料构成的数据库叫做语料库（corpusbase）。由于统计方法通常以大规模语料库为基础，因此，又称为基于语料（corpusbased）的自然语言处理方法。 发展： 对半个多世纪发展两点重要认识 对于句法分析，基于单一标记的短语结构规则是不充分的 短语结构规则在真实文本中的分布呈现严重的扭曲 本领域中称得上里程碑式的成果 复杂特征集和合一语法的提出 语言学研究中词汇主义的建立 语料库方法和统计语言模型的广泛运用 经验主义方法和理性主义方法相互融合 ","date":"2021-01-13","objectID":"/blog/snlp-ch1-%E7%BB%AA%E8%AE%BA/:2:3","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第1章 - 绪论","uri":"/blog/snlp-ch1-%E7%BB%AA%E8%AE%BA/"},{"categories":["NLP"],"content":"1.4 研究现状 已开发完成一批颇具影响的语言资源库，部分技术已达到或基本达到实用化程度，并在实际应用中发挥着巨大作用。 许多新的研究方向不断出现。 许多理论问题尚未得到根本性的解决。 ","date":"2021-01-13","objectID":"/blog/snlp-ch1-%E7%BB%AA%E8%AE%BA/:2:4","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第1章 - 绪论","uri":"/blog/snlp-ch1-%E7%BB%AA%E8%AE%BA/"},{"categories":["NLP"],"content":"1.5 本书内容安排 ","date":"2021-01-13","objectID":"/blog/snlp-ch1-%E7%BB%AA%E8%AE%BA/:2:5","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第1章 - 绪论","uri":"/blog/snlp-ch1-%E7%BB%AA%E8%AE%BA/"},{"categories":["NLP"],"content":"补充 计算语言学 计算语音学 计算词汇学 计算句法学 计算语义学 计算语用学 ","date":"2021-01-13","objectID":"/blog/snlp-ch1-%E7%BB%AA%E8%AE%BA/:2:6","tags":["统计自然语言处理","NLP","statistics","notes"],"title":"《统计自然语言处理》第1章 - 绪论","uri":"/blog/snlp-ch1-%E7%BB%AA%E8%AE%BA/"},{"categories":["Notes"],"content":"Learning Material https://quantumalgorithmzoo.org/ https://qiskit.org// ","date":"2020-11-26","objectID":"/blog/quantum-computing-fundamentals/:1:0","tags":["Quantum"],"title":"Quantum Computing Fundamentals","uri":"/blog/quantum-computing-fundamentals/"},{"categories":["Notes"],"content":"历史 量子：一个物理量如果存在最小的不可分割的基本单位，则这个物理量是量子化的，并把最小单位称为量子。 量子不是粒子，可以理解为“经典”的反义词。 量子力学 等价 波动力学 矩阵力学 基本方程：薛定谔方程 测量影响了粒子本身的状态 量子信息科学 ","date":"2020-11-26","objectID":"/blog/quantum-computing-fundamentals/:2:0","tags":["Quantum"],"title":"Quantum Computing Fundamentals","uri":"/blog/quantum-computing-fundamentals/"},{"categories":["Notes"],"content":"基础 ","date":"2020-11-26","objectID":"/blog/quantum-computing-fundamentals/:3:0","tags":["Quantum"],"title":"Quantum Computing Fundamentals","uri":"/blog/quantum-computing-fundamentals/"},{"categories":["Notes"],"content":"量子力学理论基础 希尔伯特空间（Hilbert space）即完备的内积空间，也就是说一个带有内积的完备向量空间 量子态可以由Hilbert空间的态矢量表示，量子态的可观测量可以用厄米算符来代表 态矢 State Vector（狄拉克符号） 左矢（bra）：$\\langle\\psi|=\\left[\\mathrm{c}_{1}^{*}, c_{2}^{*}, \\ldots, c_{n}^{*}\\right]$ 右矢（ket）：$|\\psi\\rangle=\\left[c_{1}, c_{2}, \\ldots, c_{n}\\right]^{T}$ 相同态矢则左右矢互为共轭转置 运算 内积：$\\langle\\alpha \\mid \\beta\\rangle=\\sum_{i=1}^{n} a_{i}^{*} b_{i}$ 外积：$|\\alpha\\rangle\\langle\\beta|=\\left[\\begin{array}{c}a_{i} b_{j}^{*} \\\\ \\end{array}\\right]_{n \\times n}$ 张量积（Kronecker product）：$A \\otimes B \\equiv\\left[\\begin{array}{cccc}A_{11} B \u0026 A_{12} B \u0026 \\cdots \u0026 A_{1 n} B \\\\ A_{21} B \u0026 A_{22} B \u0026 \\cdots \u0026 A_{2 n} B \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ A_{m 1} B \u0026 A_{m 2} B \u0026 \\cdots \u0026 A_{m n} B\\end{array}\\right]$ 能量不同的量子态： 激发态（excited state）：$|e\\rangle=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]$ 基态（ground state）：$|g\\rangle=\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]$ 量子比特（quantum bits）： $|0\\rangle = |e\\rangle$ $|1\\rangle = |g\\rangle$ 任何叠加态都可以写成其线性组合：$|\\psi\\rangle=\\alpha|0\\rangle+\\beta|1\\rangle$ $\\alpha$、$\\beta$称为振幅，满足归一化条件：$|\\alpha|^{2}+|\\beta|^{2}=1$ 封闭的(closed)量子系统的演化（evolution）由酉变换（unitary transformation）来描述：$\\left|\\psi_{2}\\right\\rangle=U\\left|\\psi_{1}\\right\\rangle$。即量子态演化本质是矩阵乘法。 酉矩阵：$U U^{*}=I$，正交矩阵的推广，可逆 各种形式的酉矩阵被称作量子门。eg. Puali Matrixes（spin matrices）: $\\sigma_{0} \\equiv I \\equiv\\left[\\begin{array}{ll}1 \u0026 0 \\\\ 0 \u0026 1\\end{array}\\right]$ $\\sigma_{1} \\equiv \\sigma_{x} \\equiv X \\equiv\\left[\\begin{array}{ll}0 \u0026 1 \\\\ 1 \u0026 0\\end{array}\\right]$ （quantum NOT gate） $\\sigma_{2} \\equiv \\sigma_{y} \\equiv Y \\equiv\\left[\\begin{array}{cc}0 \u0026 -i \\\\ i \u0026 0\\end{array}\\right]$ $\\sigma_{3} \\equiv \\sigma_{z} \\equiv Z \\equiv\\left[\\begin{array}{cc}1 \u0026 0 \\\\ 0 \u0026 -1\\end{array}\\right]$ Superposition State And Measurement 任何一个态可以写成基在复数空间的线性组合：$|\\psi\\rangle=\\alpha|0\\rangle+\\beta e^{i \\theta}|1\\rangle$ 测量：将态投影到另一个态上，概率是其内积的平方：$P_{\\alpha}=|\\langle\\psi \\mid \\alpha\\rangle|^{2}$ 其他概率投影到正交态：$P_{\\alpha \\perp}=1-P_{\\alpha}$ Phase, Pure State and Mixed State 无法通过测量得到相位信息$\\theta$，量子态的相位是相干性的表现 纯态：具有概率和相位（量子相干性）的量子态，系统的状态由波函数或态矢量描述 混合态：纯态的概率叠加，失去了（部分或全部）相位信息，系统的状态由密度矩阵描述 Density Matrix And Bloch Sphere 密度矩阵： 纯态：$\\rho=|\\psi\\rangle\\langle\\psi|$ 混合态：$\\rho=\\sum_{i} P_{i}\\left|\\psi_{i}\\right\\rangle\\left\\langle\\psi_{i}\\right|$ $\\rho = \\rho^2$当仅当纯态 密度矩阵是对波函数和经典概率分布的推广 孤立系统的密度矩阵满足幺正演化方程，开放系统的密度矩阵演化满足量子主方程 $b_i$是一组规范正交基，密度矩阵每个元素$\\varrho_{i j}=\\left\\langle b_{i}|\\rho| b_{j}\\right\\rangle=\\sum_{k} w_{k}\\left\\langle b_{i} \\mid \\psi_{k}\\right\\rangle\\left\\langle\\psi_{k} \\mid b_{j}\\right\\rangle$ 可观测量A的期望为$\\langle A\\rangle=\\sum_{i} w_{i}\\left\\langle\\psi_{i}|A| \\psi_{i}\\right\\rangle=\\sum_{i}\\left\\langle b_{i}|\\rho A| b_{i}\\right\\rangle=\\operatorname{tr}(\\rho A)$ 密度算符：线性、非负、自伴、迹为1 Bloch sphere 纯态为球面上的点 Z坐标衡量概率 混合态为球内的点 最大的混合态是球心，不存在任何叠加性 观测量和计算基下的测量 可观测量（类似于位置、动量）由自伴算子（self-adjoint operators）来表征，自伴有时也称为Hermitian 自伴算子：厄米算符（Hermitian operator），等于自己的厄米共轭的算符：$M^{\\dagger}=M$ 哈密顿量（Hamiltonian）：质量、电荷，作为参数引入系统 测量算子满足完备性方程（completeness equation）：$\\sum_{i} M_{i}^{\\dagger} M_{i}=I$ 测量方式 投影测量（projective measurements） POVM 测量（Positive Operator-Valued Measure） 复合系统和联合测量 复合系统：拥有两个或两个以上的量子比特的量子系统 张量积：两个向量空间形成一个更大向量空间的运算 满足：分配律、复数系数交换律 由子系统生成复合系统（Composite system） 纠缠（entanglement）：态$|\\psi\\rangle \\in H_{1} \\otimes H_{2}$，不存在$|\\alpha\\rangle \\in H_{1},|\\beta\\rangle \\in H_{2}$，使得$|\\psi\\rangle=|\\alpha\\rangle \\otimes|\\beta\\rangle$。 复合系统的状态演化 两能级的量子系统的状态是通过酉变换来实现演化 复合系统量子态的演化：子系统中量子态的酉变换的张量积 ","date":"2020-11-26","objectID":"/blog/quantum-computing-fundamentals/:3:1","tags":["Quantum"],"title":"Quantum Computing Fundamentals","uri":"/blog/quantum-computing-fundamentals/"},{"categories":["Notes"],"content":"量子程序 量子逻辑门 酉变换 $\\langle\\psi|=\\left\\langle\\varphi\\right| U ^{\\dagger}$ $\\langle\\psi|=\\left\\langle\\varphi\\right| U ^{\\dagger}$ 两个矢量的内积经过同一个酉变换之后保持不变：$\\langle\\varphi|\\psi\\rangle=\\langle\\varphi|U ^{\\dagger}U| \\psi\\rangle$ 矩阵的指数函数 $A^{n}=\\operatorname{diag}\\left(A_{11}^{n}, A_{22}^{n}, A_{33}^{n} \\ldots\\right)$ $\\mathrm{A}^{n}=\\mathrm{UD}^{n} \\mathrm{U}^{\\dagger}$ $e^A=Ue^DU^\\dagger$ 以A为生成元的酉变换：$U(\\theta)=e^{-i\\theta A}$ 以单位矩阵为生成元，构成一种特殊的酉变换：作用与态矢相当于态矢整体乘一个系数，在密度矩阵中该系数会被消去。 该系数称为量子态的整体相位，对系统没有任何影响，因任何测量和操作都无法分辨两个相同的密度矩阵。 单量子比特逻辑门 Pauli matrices（spin matrix）： $X=\\sigma_{x}=\\left(\\begin{array}{cc}0 \u0026 1 \\\\ 1 \u0026 0\\end{array}\\right)$ 非门 $Y=\\sigma_{y}=\\left(\\begin{array}{cc}0 \u0026 -i \\\\ i \u0026 0\\end{array}\\right)$ 绕Y旋转$\\pi$度 $Z=\\sigma_{z}=\\left(\\begin{array}{cc}1 \u0026 0 \\\\ 0 \u0026 -1\\end{array}\\right)$ 绕Z旋转$\\pi$度 泡利矩阵的线性组合是完备的二维酉变换的生成元：$\\mathrm{U}=\\mathrm{e}^{-i \\theta\\left(a \\sigma_{x}+b \\sigma_{y}+c \\sigma_{z}\\right)}$ Hadamard（H）门：$\\mathrm{H}=\\frac{1}{\\sqrt{2}}\\left[\\begin{array}{cc}1 \u0026 1 \\\\ 1 \u0026 -1\\end{array}\\right]$ Rotation Operators： RX($\\theta$)：$R X(\\theta)=e^{\\frac{-i \\theta X}{2}}=\\cos \\left(\\frac{\\theta}{2}\\right) I-i \\sin \\left(\\frac{\\theta}{2}\\right) X=\\left[\\begin{array}{cc}\\cos \\left(\\frac{\\theta}{2}\\right) \u0026 -i \\sin \\left(\\frac{\\theta}{2}\\right) \\\\ -i \\sin \\left(\\frac{\\theta}{2}\\right) \u0026 \\cos \\left(\\frac{\\theta}{2}\\right)\\end{array}\\right]$ RY($\\theta$)：$R Y(\\theta)=e^{\\frac{-i \\theta Y}{2}}=\\cos \\left(\\frac{\\theta}{2}\\right) I-i \\sin \\left(\\frac{\\theta}{2}\\right) Y=\\left[\\begin{array}{cc}\\cos \\left(\\frac{\\theta}{2}\\right) \u0026 -\\sin \\left(\\frac{\\theta}{2}\\right) \\\\ \\sin \\left(\\frac{\\theta}{2}\\right) \u0026 \\cos \\left(\\frac{\\theta}{2}\\right)\\end{array}\\right]$ RZ($\\theta$)：$R Z(\\theta)=e^{\\frac{-i \\theta Z}{2}}=\\cos \\left(\\frac{\\theta}{2}\\right) I-i \\sin \\left(\\frac{\\theta}{2}\\right) Z=\\left[\\begin{array}{cc}e^{\\frac{-i \\theta}{2}} \u0026 0 \\\\ 0 \u0026 e^{\\frac{i \\theta}{2}}\\end{array}\\right] \\sim \\left[\\begin{array}{cc}1 \u0026 \\\\ \u0026 e^{i \\theta}\\end{array}\\right]$ RX, RY, RZ意味着量子态在布洛赫球上分别绕X, Y, Z旋转$\\theta$角度，所以RX、RY带来概率幅的变化，RZ只有相位的变化。这三种操作使量子态在球上自由移动。 多量子和比特逻辑门 所有逻辑操作都是酉变换，所以输入、输出比特数量相等 $|01\\rangle$中0为高位，1为低位，左高右低 CNOT门 $C N O T=\\left[\\begin{array}{cccc}1 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\\\ 0 \u0026 1 \u0026 0 \u0026 0\\end{array}\\right]$ $C N O T=\\left[\\begin{array}{cccc}1 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 1 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 1 \\\\ 0 \u0026 0 \u0026 1 \u0026 0\\end{array}\\right]$ CR门：控制相位门（Controlled phase gate） $C R(\\theta)=\\left[\\begin{array}{cccc}1 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 1 \u0026 0 \u0026 0 \\\\ 0 \u0026 0 \u0026 1 \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 e^{i \\theta}\\end{array}\\right]$ ISWAP门：交换两个比特的状态 由 $\\sigma_{x} \\otimes \\sigma_{x}+\\sigma_{y} \\otimes \\sigma_{y}$ 作为生成元生成 $i S W A P(\\theta)=\\left[\\begin{array}{cccc}1 \u0026 0 \u0026 0 \u0026 0 \\\\ 0 \u0026 \\cos (\\theta) \u0026 -i \\sin (\\theta) \u0026 0 \\\\ 0 \u0026 -i \\sin (\\theta) \u0026 \\cos (\\theta) \u0026 0 \\\\ 0 \u0026 0 \u0026 0 \u0026 1\\end{array}\\right]$ 量子线路与测量操作 量子计算的if和while 基于测量的跳转 基于量子信息的if和while 量子逻辑门知识图谱： ","date":"2020-11-26","objectID":"/blog/quantum-computing-fundamentals/:3:2","tags":["Quantum"],"title":"Quantum Computing Fundamentals","uri":"/blog/quantum-computing-fundamentals/"},{"categories":["Economics"],"content":"“Crash Course Economics” is pretty awesome for beginners! The following notes are basically taken from youtube comments. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:0:0","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"CH1 Fundamentals of Economics ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:1:0","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s1 Intro ECONOMICS is the study of people and choices. Two assumption SCARCITY Everything has COST OPPORTUNITY COST is whatever you give up to do something. SCARCITY is the tension between infinite wants and finite resources. INCENTIVES is a set of external (rather than intrinsic) motivators that explain people’s choices. MACROECONOMICS is the study of production, employment, prices, and policies on a nationwide scale. MICROECONOMICS is the study of how consumers, workers, and firms interact to generate outcomes in specific markets. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:1:1","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s2 Specialization and Trade Economics is the study of scarcity and choices. We have limited resources, so we need a way to analyze the best way to use them. Significant sustained increase in people’s lives happened after Industrial Revolution. Adam Smith concluded that division of labor made countries wealthy. Analogy: one pizza takes few people to be created. One prepares ingredients, another puts it in the oven, another one puts it in the box. This division makes each worker more productive, since each one is focused on a thing they do best and they don’t need to spend time switching between the tasks. Without specializations, if you want something - you have to make it yourself. So if you are good at producing something - specialize at it and then trade with others. Production Possibilities Frontier (PPF) shows different combinations of two goods being produced using all resources efficiently. Every possible combination inside the curve is inefficient. On the curve - efficient. Outside - impossible. The country that can produce more goods of one kind (per time) than another country is having an absolute advantage over another country in production of those goods. Opportunity cost is a cost of production of good, measured in losses in production of another good. Country that can produce good with cheaper opportunity cost, has comparative advantage over another country. Individuals and countries should specialize in producing things in which they have a comparative advantage and then trade with other countries that specialize in something else. This trade is mutually beneficial. If there is one point where economists agree it’s that specialization and trade makes the world better off. Self-sufficiency is inefficiency ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:1:2","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s3 Economic Systems and Macroeconomics As a social order, we have to figure out three things: What will we produce? How to produce it? Who will get it? Two different economic systems: market economies and planned economies. In planned economy, government controls labor, land and capital. Communism is primarily defined by the lack of private property. Class-lessness is a symptom of having no private property. There are no communist countries in the world. Often socialism has some private property and some public ownership. Command economy is totally controlled by government. In market economies, individuals control production to get profit. Invisible hand - the unintended social benefits resulting from individual actions. “It’s not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner, but from the regard to their own interest.” The mechanism of the invisible hand is that if you produce unwanted or shoddy products, a competitor will produce better more desirable products and put you out of business. This results in businesses that produce the things that people want/demand most, at lower prices. Modern economies are neither completely free market nor planned. There’s a spectrum of government involvement. Circular flow model. Modern economy is made above households (individuals like him and you) and businesses. Businesses sell goods and services to households in product market. Households earn the money by selling labor to businesses. Businesses pay for the resources on resource market. Government also buys products and resources, i.e. to buy cars from businesses and hire policemen to drive them. Government gets the money from taxes, households and businesses (and borrowing). ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:1:3","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s4 Supply and Demand Market: any place where buyers and sellers meet to exchange goods and services. An owner of a supermaket values the labor of the cashier more than the money she pays him. Price singnal: the information that markets generate to guide the distribution of the resources. usinesses, and in particular large corporations , are often villainized as greedy, heartless institutions, that take advantage of consumers, but if markets are transparent and buyers are free to choose, then businesses will have a hard time making advantage of people. Supply and demand When the price goes up - people buy less, when the price goes down - people buy more. When the price goes up - the farmer wants to produce more, when the price goes down - the farmer wants to produce less. When quantity supplied = quantity demanded, we get equilibrium price of product. Four market behaviors Supply ↑ Supply ↓ Demand ↑ Demand ↓ ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:1:4","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"CH2 Basic of Macroeconomics ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:2:0","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s5 Macroeconomics Macroeconomics: the study of the entire economy as a whole rather than individual markets. In general policy makers try to achieve three goals: Keep the economy growing over time (gross domestic product - GDP) Limit unemployment (unemployment rate) Keep prices stable (inflation rate) GDP is the value of all final goods and services produced within a country’s border in a specific period of time, usually a year. Transactions where nothing new was produced - don’t count as GDP. Also not including illegal activities. Measured in dollars. Nominal GDP is GDP not adjusted for inflation. Real GDP is GDP adjusted for inflation. Recession: when two successful quarters, or six months, show a decrease in real GDP. Depression - a severe recession. Unemployment rate is calculated by taking the number of people that are unemployed and dividing by the number of people in the labor force, times 100. Discouraged workers - unemployed people that were looking for work, but have given up. There are three types of unemployment: frictional unemployment - the time period between jobs, when a worker is searching for, or transitioning from one job to another. structural unemployment - unemployment caused by lack of demand for a worker’s specific type of labor. cyclical unemployment - unemployment due to recession. Natural rate of unemployment - the lowest rate of unemployment that economy can sustain over a long period. inflation - an increase in a currency supply relative to the number of people using it, resulting in rising prices of goods and services over time. Deflation - a decrease in general price level of goods and services. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:2:1","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s6 Productivity and Growth Why some countries have high GDP and others have low( some countries are rich and other poor): Lack of natural resources. Corrupt governments. GDP per capita(output per person) is used to tell how wealthy a country is. Countries with high GDP/capita have far less infant mortality, poverty and preventable diseases. Productivity and growth: The more a worker produces, the more a worker can earn. Economists argue that the main reason that some countries are rich is because of productiivty. Higher value produce also the growth effect. Productivity is key, but there are limits. People in poor countries need food, water, plumbing, hospitals and medicine, and all of those things are needed to get better efficiency. How much stuff is produced per person(can be called GDP) Factors of production effect the efficiency: Land Workers Capital( and also workers education, knowledge aka human capital) Technology: The sum total of knowledge and information that society has acquired concerning the use of resources to produce goods and services. (Connectivity= productivity). Increasing Productivity has resulted in increasing standards of living(globally and historically). ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:2:2","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"CH3 Inflation, Monetary and Fiscal Policy ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:3:0","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s7 Inflation and Bubbles and Tulips Pirchasing power: the amount of physical goods and services that can be bought by a given amount of monney. Consumer price index(CPI): a statical estimate constructed using the prices of a sample of representative items whose prices are collected preriodically. “Real” means that prices from the past has been adjust for inflation. Norminal means a price from the past that hasn’t been adjust for inflation. So the highest “nominal” box office receipts list is quite different. Since we have to keep the market basket constant over time, a traditional CPI won’t adjust for either new products or increases in product quality. Inflation: an increase in a currency supply relative to the number of people using it, resulting in rising prices of goods and services over time. Demand pull inflation - too much money chasing too few goods. Bubbles - a market phenomenon characterized by surges in asset prices to levels significantly above the fundamental value of that asset. Speculation - trading a financial instrument involving high risk, in expectation of significant returns. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:3:1","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s8 Fiscal Policy and Stimulus Recessionary gap: a situation wherein the real GDP is lower than potential GDP at the full employment level. Inflationary gap: the amount by which the actual gross domestic product (GDP) exceeds potential full-employment GDP. Macroeconomics: the study of the entire economy as a whole rather than individual markets. Fiscal policy: the way a government adjusts its spending levels and tax rates to monitor and influence a nation’s economy. Expansionary Fiscal Policy: stimulates the economy during or anticipation of a business-cycle contraction. Contractionary Fiscal Policy: enacted by a government to reduce the money supply and ultimately the spending in a country. Classical theories assumed that the economy will fix itself in a long run, and that government intervention will, at best, lead to unintended consequences and, at worst, cause massive inflation and debt. Deficit spending: the government spends more money than it collects in tax revenue. Crowding out: where increased public sector spending replaces, or drives down, private sector spending. Keynesian economists maintain that crowding out is only a problem if economy operates at full capacity, where all workers are employed and we’re producing as much as we can. Austerity: raising taxes and cutting government spending to reduce debt. In crisis of 2008 was main policy of EU, which led to worse results than deficit spending policy in US. Multiplier effect: the initial increase in government spending of 100mightturnouttobe175mightturnouttobe175 worth of actual spending in the economy. a. When the economy is booming, multiplier is close to 1x. b. When economy is in recession, the multiplier is around 2x. c. Spending on infrastructure, and aid to state \u0026 local governments , also seems to have fairly high multiplier, about 1.5. But general cuts to payroll and income taxes seems to have a multiplier of about 1. If the government cuts 100intaxes,theeconomyisgoingtogrowbyabout100intaxes,theeconomyisgoingtogrowbyabout100. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:3:2","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s9 Deficits \u0026 Debts Budget deficit: the amount by which a government’s spending exceeds it’s income over a particular period of time. Debt: the accumulation of budget deficits. In the same way our GDP grows every year, due to population growth and productivity increases. And our ability to sustain debt grows along with our income. Default: the investors who loaned the government money lose billions and the government loses all credibility, and it causes massive recession. Debt ceiling: limit on the amount of national debt that can be issued by US Treasury. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:3:3","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s10 Monetary Policy and ther Federal Reserve The Federal Reserve is the central bank of US. Europe has the European Central Bank. Most Central Banks have two jobs: they regulate and oversee the nation’s commercial banks by making sure that banks have enough money in reserve to avoid bank runs. they conduct monetary policy which is increasing or decreasing the money supply to speed up or slow down the overall economy. interest rate - the price of borrowing money. When interest rates are low, borrowers will find it easier to pay back loans so they will borrow more and spend more. When interest rates are high, borrowers borrow less and spend less. Expansionary monetary policy: when central bank wants to speed up the economy, it will increase the money supply, which will decrease interest rates and lead to more borrowing and spending. Contractionary monetary policy: when central bank wants to slow down the economy, they decrease the money supply. Less money available will increase interest rates and decrease borrowing and spending. Liquid assets: an asset that can be converted into cash quickly and with minimal impact to the price received. 3 main ways to change money supply: Reserve Requirement Discount Rate Open market operations: this is when the federal reserve buys or sells short term government bonds. Quantitative easing (Q.E.): when central banks buy longer term assets from banks. Monetary policy: changing money supply to speed up or slow down economy. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:3:4","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s11 Money and Finance Money serve three main purposes: Medium of exchange Store of value Unit of account Financial system Lenders Borrowers Governments Capital: the machinery, tools and factories owned by a business and used in production. Financial system is a network of institutions, markets and contracts that brings lenders and borrowers together. Debt: if you get a loan from the bank, you are obligated to pay back the amount you borrowed plus the amount of interest. Equity: the difference between the value of the assets/interest and the cost of liabilities of something owned. Financial instrument: a tradeable asset of any kind. Financial institution - an establishment that conducts financial transactions such as investments, loans and deposits. Financial markets with instruments like stocks and bonds, allow borrowers to crowdsource the money they need to borrow. They raise their capital from lots of investors, and spread the risk around. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:3:5","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s12 2008 Financial Crisis “History doesn’t repeat itself, but it often rhymes”- Mark Twain ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:3:6","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s13 Recession, Hyperinflation, and Stagflation Hyperinflation: when a country experiences a monthly inflation rate of over 50% or around 13,000% annual inflation. Extreme inflation also forces people to spend as quickly as possible rather than save of lend, so there is no money available to fund new businesses. And all that uncertainty limits foreign investment and trade. The more money you print, the more inflation you get. Economists call the number of times a dollar is spent per year a velocity of money. When people spend their money as quickly as they get it, that increases velocity, which pushes inflation up even faster. Depression. After the initial crash in 1929 the federal reserve dropped interest rates to zero, output and prices fell, and regular people started to expect further price declines. Unemployment rose to 25% and the average family income dropped by around 40%. Stagflation: when output slows down or stops, or stagnates at the same time that prices rise. Stagnant economy + inflation = stagflation. The FED tried to address this by boosting the money supply and cutting interest rates, but output couldn’t rise much because of low productivity and the oil shortage. So all that extra money just triggered inflation. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:3:7","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"CH4 Economic Schools of Thought ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:4:0","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s14 Economic Schools of Thought Economic theories are constantly been proven, disproven and revised. The problem is, when these theories are wrong, millions of people can be adversely affected. The founder of modern economics was a Scottish philosopher, named Adam Smith. In 1776 his book “The wealth of nations” was published. It was an organized discussion about economic theory. When both focus on what they’re best at and then trade, everyone benefits. In 1890 was published a book, called “Principles of economics” by Alfred Marshall. It embodied classical economics. In 1936 John Maynard Keynes published a book “A general theory of money” which launched a field of macroeconomics. They claimed that during recessions it is necessary for the government to get involved by using monetary and fiscal policy to increase output and decrease unemployment. Socialism: system where the means of producing and distributing goods is owned collectively or by a centralized government. Monetarism: focused on price stability and argue the money supply should be increased slowly and predictably to allow for steady growth. Supply side economics (trickle down economics): advocated deregulation and cutting taxes, especially corporate taxes. New neoclassical synthesis: synthesis of classical economic theories and Keynesian economics. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:4:1","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"CH5 Economics in International Relations 1 ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:5:0","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s15 Imports, Exports, and Exchange Rates concept Net exports: the annual difference between a country’s exports and imports Trade surplus: when a country exports more than it imports Trade deficit: when a country imports more than it exports International trade It doesn’t make sense to make everything on your own if you can trade with other countries that have a comparative advantage. Unsafe and unfair working conditions and environmental degradation. NAFTA, 1994 Dropped trade barriers between Canada, the US and Mexico. Increased US trade deficits. Decreased manufacturing jobs. Millions of jobs created. Reduced prices. Net positive impact on 3 countries. Protectionism: Placing high tariffs on imports and limiting the number of foreign goods to protect local businesses. Limited by the WTO or World Trade Organization. Exchange rate - how much your currency is worth when you trade it for another country’s currency. Currency appreciation - An increase in the value of one currency in terms of another. Currency depreciation - A decrease in the level of a currency in a floating exchange rate system due to market forces. Foreign imports get more expensive, which makes them fall. Exports to other countries get cheaper, which makes them rise. Floating exchange rates - they change based on supply and demand. Balance of payments - accounting statement that records all International transactions in a country. It’s made up of two sub-accounts, the current account and the financial account. Current account - records the sale and purchase of goods and services, investment income earned abroad, and other transfers like donations and foreign aid. Financial account - records the purchase and sale of financial assets like stocks and bonds. There is a reason why the flow of goods and the flow of money are symmetric. If consumers, businesses and the government want to buy more stuff than their country is producing symmetrically, they have to import it. So there’s a trade deficit. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:5:1","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s16 Globalization and Trade and Poverty Poverty line: Minimum level of income deemed adequate in a particular country. Extreme poverty: Severe deprivation of basic needs including, food, safe drinking water, sanitation, health, shelter, education and information. UN definition of extreme poverty: People living under \u003c1.25/day (836 mn people as of 2015, down from 1.9 bn in 1950) 1 in 7 people still live without electricity. Mobile phones are single most transformative technology to the developing world - Jeffrey Sachs Leapfrogging: Countries can skip straight to more efficient technology without significant costs Hans Rosling (statistician): 1-2 bn suffer from globalization deficiency. Multiplier effect means more Paul Krugman, “The Bangladeshi apparel industry is going to consist of what we would consider sweatshops, or it wont exist at all.” Outsourcing of jobs, exploitation and oppression, is a form of economic colonialism. Companies don’t follow same rules as developing countries. To tackle this public awareness is growing. e.g. US produces annual publication on list of goods produced by child or forced labor. Many experts believe globalization isn’t sustainable for the planet because of it’s impact on climate change, deforestation and pollution. Microcredit, Muhammad Yunus, small loans ($100) enabled people (especially women) to participate in the economy. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:5:2","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s17 Income and Wealth Inequality Two types of economic inequality - Wealth (accumulated assets such as savings, pensions, real estate, and stocks minus the liabilities) inequality - The unequal distribution of accumulated assets, minus liabilities - North America and Europe host 20% of the population yet have 67% of the world’s wealth - China hosts 20% of the population wet have 8% of the world’s wealth etcetera - Income inequality - The extent to which income is distributed in an uneven matter - Economic Big Bang - “At first countries’ incomes were all bunched together, but with the Industrial Revolution the differences exploded,… [It] pushed some countries forward onto the path to higher incomes while others stayed where they had been for millennia.” - Branko Milanovic - Industrial Revolution accelerated the gap between the richest and poorest - Globalisation and international trade are further accelerating this disparity - “The triumph of globalisation and market capitalism has improved living standards for billions while concentrating billions among the few.” - Richard Freeman - Skill-biased technological change - Skilled workers that have the education/skills best suited for technological work whereas it has served as a replacement for unskilled workers - Further gap between the poor and the rich and the poor and they rocking class - As economies develop and as manufacturing jobs move overseas, low skill low pay and high skill high pay work are the only jobs left - Other factors in exacerbating widening gap - Reduced influence of unions, tax policies that favour the wealthy, allowance for greater CEO salaries, gender, race - Lorenz curve graph allows us to measure the depth of income inequality - Can be used to calculate the GINI Index - the most commonly used measure of income equality (the size of the gap between the equal distribution of income and actual distribution) with 0 as complete quality and 100 as complete inequality - Critiques of income inequality - Claim all income brackets are making more money but the rich’s share is growing faster - However in the last 20 years, their average income has been falling while the rich have continually gotten richer - “Yes, some level of inequality is built in to capitalism… It is inherent to the system. The question is, what level of inequality is acceptable? And when does inequality start doing more harm than good?” - Bill Gates - Some economists argue greater income inequality is associated with increased violence, drug abuse, incarceration, diluted political equality (rich have disproportionate say in what policies advance and in this have an incentive to promote their own self-interest) - Solutions - Education, increased minimum wage, affordable, high quality childcare, provide a social safety net, adjustment of tax code to redistribute income - Increase income taxes and capital taxes on the rich - Progressive tax - a tax in which the tax rate increases as the taxable amount increases - “One idea is to fix loopholes that the rich use to avoid paying taxes. Other economists argue that taxing the rich won’t be as effective as reducing regulation and bureaucratic red tape.” - No society can surely be flourishing and happy of which the far greater part of the members are poor and miserable - Adam Smith ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:5:3","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"CH6 Basics of Microeconomics ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:6:0","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s18 Marginal Analysis, Roller Coasters, Elasticity, and Van Gogh Microeconomics: Study of the economic behavior of individual units of an economy (such as a person, household, firm, or industry). Marginal analysis: An analysis of how individuals, businesses and governments make decisions. Marginal = Additional Utility: Satisfaction or happiness people get from consuming a good or service. Law of Diminishing Marginal Utility = Law of Decreasing Additional Satisfaction Demand curve = Marginal benefit curve Supply curve = Marginal cost curve Utils: A unit used to quantify satisfaction; they are completely subjective. Law of supply: An increase in price gives producers an incentive to produce more. Diamond-water paradox: Although water is on the whole more useful, in terms of survival, than diamonds, diamonds command a higher price in the market. Substitution effect: As prices rise consumers will replace expensive items with less costly alternatives. Elasticity of demand: a measure of the relationship between a change in the quantity demanded of a particular good and a change in its price. Elasticity of supply: a measure of the responsiveness of quantity supplied to a change in price. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:6:1","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s19 Markets, Efficiency, and Price Signals Central planning is not efficient Two types of efficiency: Productive Efficiency- The idea that products are made at lowest costs. Allocative Efficiency- State of economy in which production represents consumer’s preference. Central planners are less likely to be allocatively efficient because they have a harder time getting feedback about what people want. Price Signals tells you what consumers are willing to by at higher prices. Price gouging: When sellers sells essential items (e.g water, food etc) at much higher price than reasonable. Below-Cost Pricing: This is also called Predatory Pricing. It’s when a business drive out competitors by charging lower prices even at a short-term loss. Competitors that can’t sustain such low prices will be forced out of the market giving the surviving businesses more market share and ability to raise prices. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:6:2","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s20 Prices Controls, Subsidies, and the Risks of Good Intentions President Richard Nixon, 1970s, 90-day price and wage freeze to fight inflation. Milton Friedman, ‘One of those plausible schemes with very pleasing commencement that have shameful conclusions’. Price ceiling - Maximum price for a specific good or service Price floor - Minimum price in a specific market. Keeping price artificially high and not allowing price to fall to equilibrium. In terms of helping consumers and producers, price controls are counter-productive. However minimum wage is a better solution. Economists generally agree that rent control results in reduced quantity and quality of housing that is available. A subsidy is a government payment given to individuals, organizations or businesses to offset costs to advance a specific public goal. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:6:3","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s21 Market Failures, Taxes, and Subsidies Free riders: people who benefit without paying Responding to incentives: why pay more if I can get it for less? Market failure: when markets fail to provide enough public goods Public good - anything having the characteristics of non-exclusion (cannot exclude/disfavour people that do not pay f.ex. those who evade taxes are not exempt from national defence) and non-rivalry (one person’s consumption of the good does not ruin it for others f.ex. public parks) Tragedy of the Commons - the idea that common goods that everyones has access to are often misused and exploited (fuelled by personal incentives that disregard the commonwealth) Environmental problems: deforestation/overfishing Externalities: situations when there are external costs or benefits that accrue to other people or society as a whole Regulatory policies (rules established by govt decree) + market-based policies (policies designed to manipulate markets, prices, and incentives to correct market failures) can be used to fix externalities ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:6:4","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"CH7 Environmental and Education Economics ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:7:0","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s22 Evironmentl Econ what can the government do? enforce specific rules outside the market (just limit how much firms can pollute) influence the market through price incentives add tax on products that cause pollution. (gasoline) subsidize products that reduce pollution (electric cars, renewable energy) how can technology help? since our current technology doesn’t provide cheaper renewable energy, we can maximize the use of non renewable energy (energy-efficient cars) rebound effect: efforts to increase energy efficiency creates more available energy that only gets spent into something MORE and MORE. what actions are the world taking? International treatys in which countries commit on reducing greenhouse gases emission. (UN negotiations) funding “green” research into renewable energy. changes can be brought by individual consumers, along with changes by the government and producers. (turn the lights off when not in use! and other small things). ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:7:1","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s23 Economics of Education omit. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:7:2","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"CH8 Types of Competition ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:8:0","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s24 Revenue, Profits, and Price omit. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:8:1","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s25 Monopolies and Anti-Competitive Markets omit. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:8:2","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s26 Game Theory and Oligopoly 4 types of markets Perfect competition: Low barriers, identical products, no control over price (eg strawberries) Monopoly: There is one large company that produces a product with few substitutes. And because high barriers prevent competition, a monopoly has a lot of control over price. Monopolistic competition: a market with many producers and relatively low barriers; their products are very similar but not identical. (eg furniture stores or fast food) Oligopolies: markets that have high barriers to entry and are controlled by a few large companies. Similar to monopolistic competition, their products are similar but not identical. That gives them some control over their prices. How? Non-price competition: Companies focus on things like style, quality, location, or service. The goal is to distinguish their product from their competitors. The most recognizable form of non-price competition is advertising. Game theory: the study of strategic decision making. Collusion: If businesses don’t compete at all and they agree to charge the same high price, conspiring to form what economists call a cartel. They split the customers 50⁄50, but now they make even more profit – benefiting at the expense of consumers. It’s illegal in the US. Price leadership is when one company changes its prices, and its competitors have to decide if they’re going to follow suit. Pay off matrix: In game theory, a payoff matrix is a table in which strategies of one player are listed in rows and those of the other player in columns and the cells show payoffs to each player such that the payoff of the row player is listed first. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:8:3","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"CH9 How People Affect the Economics ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:9:0","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s27 Behavioral Economics Bounded rationality Lack of information Manipulate non interinsic attributes of price Framing effect Psychological pricing Nudge theory Risk(neutral \u0026 averse) ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:9:1","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s28 Labor Markets and Minimum Wage omit. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:9:2","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s29 The Economics of Healthcare omit. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:9:3","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"s30 The Economics of Death omit. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:9:4","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"CH10 Taxes and Informal Economies omit. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:10:0","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Economics"],"content":"CH11 Economics in International Relations 2 omit. ","date":"2020-08-20","objectID":"/blog/crash-course-economics-notes/:11:0","tags":["Notes","Economics"],"title":"Crash Course Economics Notes","uri":"/blog/crash-course-economics-notes/"},{"categories":["Math"],"content":"基本概念 线性变换/映射：不变：直线、直线比例、原点。 仿射变换/映射：线性变换+平移。高维线性变换的投影。 行列式：为了解线性方程组定义。-\u003e Cramer 法则。 线性变换的伸缩因子（小于零时翻面）。 矩阵：运动（旋转、投影、拉伸）。指定基下的线性变换。列为新基。 特征值：运动速度。相似不变量：和为迹，积为行列式。 特征向量：运动方向。 特征值分解：$A = PBP^{-1}$ 以特征向量为基，B为对角矩阵。P旋转，B拉伸。 正交矩阵：（向量长度和夹角不变，即两点欧式距离不变）旋转与镜射。行列式为1时为旋转矩阵，-1时为瑕旋转矩阵（旋转+镜射） 对称矩阵：二次型。（行列空间相同） 对角矩阵：拉伸。 相似矩阵：同一线性变换在不同基下的表示。换个视角看。 ","date":"2019-08-19","objectID":"/blog/some-intuitions-in-linear-algebra/:1:0","tags":["Linear Algebra","Matrix"],"title":"Some Intuitions in Linear Algebra","uri":"/blog/some-intuitions-in-linear-algebra/"},{"categories":["Math"],"content":"Jacobian矩阵 $$ c = \\pm\\sqrt{a^2 + b^2} $$ $$J \\equiv \\left[\\begin{array}{ccc} \\frac{\\partial f }{\\partial x_{1}} \u0026 \\cdots \u0026 \\frac{\\partial f }{\\partial x_{n}} \\end{array}\\right]=\\left[\\begin{array}{ccc} \\frac{\\partial f_{1}}{\\partial x_{1}} \u0026 \\cdots \u0026 \\frac{\\partial f_{1}}{\\partial x_{n}} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\frac{\\partial f_{m}}{\\partial x_{1}} \u0026 \\cdots \u0026 \\frac{\\partial f_{m}}{\\partial x_{n}} \\end{array}\\right]$$ 以一阶偏导数排列。 体现了可微方程在给出点的最优线性逼近，本质是导数，即微分映射的坐标表示。 导数\\微分是切空间上的线性变换，以一组基底（欧式空间中，基底选择自然映射构建即可）给出切空间上点的坐标，线性变换就具体化为一个矩阵。欧式空间中，这个矩阵就是雅可比矩阵。 Jacobian行列式几何意义：矩阵对应的线性变换前后的面积比。故而积分中变换坐标时会乘以Jacobian行列式。 ","date":"2019-08-19","objectID":"/blog/some-intuitions-in-linear-algebra/:2:0","tags":["Linear Algebra","Matrix"],"title":"Some Intuitions in Linear Algebra","uri":"/blog/some-intuitions-in-linear-algebra/"},{"categories":["Math"],"content":"Hessian矩阵 $$H =\\left[\\begin{array}{cccc} \\frac{\\partial^{2} f}{\\partial x_{1}^{2}} \u0026 \\frac{\\partial^{2} f}{\\partial x_{1} \\partial x_{2}} \u0026 \\cdots \u0026 \\frac{\\partial^{2} f}{\\partial x_{1} \\partial x_{n}} \\\\ \\frac{\\partial^{2} f}{\\partial x_{2} \\partial x_{1}} \u0026 \\frac{\\partial^{2} f}{\\partial x_{2}^{2}} \u0026 \\cdots \u0026 \\frac{\\partial^{2} f}{\\partial x_{2} \\partial x_{n}} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\frac{\\partial^{2} f}{\\partial x_{n} \\partial x_{1}} \u0026 \\frac{\\partial^{2} f}{\\partial x_{n} \\partial x_{2}} \u0026 \\cdots \u0026 \\frac{\\partial^{2} f}{\\partial x_{n}^{2}} \\end{array}\\right]$$ 多元函数的二阶偏导数构成的方阵，描述了函数的局部曲率。 可判断多元函数的极值：正定极小、负定极大、不定鞍点。 其特征值反应该点特征向量方向的凹凸性，特征值越大，凸性越强。最大特征值和对应特征向量反应其邻域二维曲线最大曲率强度和方向。 梯度的Jacobian即为Hessian。 向量对向量求导： 一阶导：Jacobian 二阶导：Hessian ","date":"2019-08-19","objectID":"/blog/some-intuitions-in-linear-algebra/:3:0","tags":["Linear Algebra","Matrix"],"title":"Some Intuitions in Linear Algebra","uri":"/blog/some-intuitions-in-linear-algebra/"},{"categories":["Toy"],"content":"Hugo是用Go编写的静态站点生成器,生成速度（不到1s）比其他生成器快了许多,且配置比Hexo等简单，目前主要存在的缺点是主题不够丰富和成熟、插件较少。 话不多说，开始搭建~ ","date":"2019-07-24","objectID":"/blog/helloword/:0:0","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"},{"categories":["Toy"],"content":"创建GitHub Pages 在GitHub新建一个Repository命名为[你的GitHub账户名].github.io，其他可以不用填写，直接创建即可。 进入刚创建的Repository，点击右边的Settings，下滑找到GitHub Pages，点击Choose a theme，随意选择一个theme（因为不会用到），点击commit changes就创建完成了。 ","date":"2019-07-24","objectID":"/blog/helloword/:1:0","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"},{"categories":["Toy"],"content":"安装Hugo和Git ","date":"2019-07-24","objectID":"/blog/helloword/:2:0","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"},{"categories":["Toy"],"content":"安装Hugo macOS使用Homebrew brew install hugo Windows使用Chocolatey choco install hugo -confirm 详见 https://gohugo.io/getting-started/installing ","date":"2019-07-24","objectID":"/blog/helloword/:2:1","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"},{"categories":["Toy"],"content":"安装Git 在Git官网下载安装。 命令行输入 git version 显示 Git 的版本号，说明安装成功。 注意需要将 Hugo 和 Git 安装目录都加入系统Path环境变量。 ","date":"2019-07-24","objectID":"/blog/helloword/:2:2","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"},{"categories":["Toy"],"content":"创建Site 生成网站 ","date":"2019-07-24","objectID":"/blog/helloword/:3:0","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"},{"categories":["Toy"],"content":"创建site 在用来存放博客的路径下，命令行执行： hugo new site mysite 该命令创建一个名为 mysite 的文件夹来存放你的博客。 mysite 的目录结构如下： ├── archetypes // .md 博客原型 模版 ├── content // .md 存放你写的 Markdown 文件 ├── data // YAML, JSON, or TOML等配置文件 ├── layouts // .html 网站模版 ├── static // images, CSS, JavaScript 等，决定网站的外观。 ├── themes // 存放网站主题 └── config.toml // 网站的配置文件` ","date":"2019-07-24","objectID":"/blog/helloword/:3:1","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"},{"categories":["Toy"],"content":"安装主题 你可以在主题选择自己喜欢的主题， 本站使用的是 LeaveIt。 执行： cd mysite/themes 进入该文件夹用来存放主题的文件夹, 执行： git clone https://github.com/liuzc/LeaveIt.git ","date":"2019-07-24","objectID":"/blog/helloword/:3:2","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"},{"categories":["Toy"],"content":"设置模版 回到mysite: cd .. 打开 mysite/archetypes 目录下的 模版 default.md ，更改为： +++ title = \"{{ replace .Name \"-\" \" \" | title }}\" # 文章标题 date = {{ .Date }} # 自动添加日期 draft = true # 是否为草稿 categories = [\"\"] # 目录（数组） tags = [\"\"] # 标签（数组） description = \"\" # 描述 comments = true # 是否开启评论 share = true # 是否开启分享 +++ 执行： hugo new about.md hugo new posts/firstBlog.md 在 content 文件夹创建 about.md 和 firstBlog.md ，打开他们随意输入些内容。 ","date":"2019-07-24","objectID":"/blog/helloword/:3:3","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"},{"categories":["Toy"],"content":"配置 config.toml 按照以下 config.toml 模板更改, 不知道的可以先空着。 在 mysite/static 文件夹中新建 images 文件夹，将你的头像文件放入其中。 baseURL = \"https://[你的GirHub用户名].github.io\" title = \"My Site\" # 网站标题 languageCode = \"zh-cn\" # 语言 hasCJKLanguage = true # 字数统计时统计汉字 theme = \"LeaveIt\" # 主题 paginate = # 每页博客数 enableEmoji = true # 支持 Emoji enableRobotsTXT = true # 支持 robots.txt googleAnalytics = \"\" # Google 统计 id preserveTaxonomyNames = true [blackfriday] # Markdown 渲染引擎 hrefTargetBlank = true # Open external links in a new window/tab. nofollowLinks = true noreferrerLinks = true [Permalinks] posts = \"/:year/:month/:title/\" [menu] [[menu.main]] name = \"Blog\" url = \"/posts/\" weight = 1 [[menu.main]] name = \"Categories\" url = \"/categories/\" weight = 2 [[menu.main]] name = \"Tags\" url = \"/tags/\" weight = 3 [[menu.main]] name = \"About\" url = \"/about/\" weight = 4 [params] since = 2019 author = \"你的名字\" avatar = \"/images/avatar.png\" # 头像文件路径` `subtitle = \"Hugo is Absurdly Fast!\" home_mode = \"\" # 填post则在首页post博客 enableGitalk = true # gitalk 评论系统 google_verification = \"\" description = \"\" # 描述 keywords = \"\" # site keywords beian = \"\" baiduAnalytics = \"\" license= '本文采用\u003ca rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\" target=\"_blank\"\u003e知识共享署名-非商业性使用 4.0 国际许可协议\u003c/a\u003e进行许可' [params.social] GitHub = \"\" Twitter = \"\" Email = \"\" Instagram = \"\" Wechat = \"/images/me/wechat.png\" # Wechat QRcode image Facebook = \"\" Telegram = \"\" Dribbble = \"\" Medium = \"\"` ","date":"2019-07-24","objectID":"/blog/helloword/:3:4","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"},{"categories":["Toy"],"content":"生成草稿网站 配置完 config.toml 后在 mysite 文件夹执行： hugo server -D 打开 http://localhost:1313/ 即可查看生成的静态网站啦。 ","date":"2019-07-24","objectID":"/blog/helloword/:3:5","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"},{"categories":["Toy"],"content":"发布到GitHub Pages 生成网站命令： hugo 生成网站存储在 public 文件夹中。 执行： cd public 进入 public 文件夹，然后： git init git remote add origin https://github.com/[Github 用户名]/[Github 用户名].github.io.git git add . git commit -m \"init commit\" git push -u origin master -f 这里 -u origin master 指定了默认主机，后面可以不加参数直接push了。 以后再发布时的命令如下： git add . git commit -m \"the commit message\" git push` 接下来稍等片刻你便可以在 https://[你的GitHub用户名].github.io/ 访问你的网站啦！ ","date":"2019-07-24","objectID":"/blog/helloword/:4:0","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"},{"categories":["Toy"],"content":"主题优化 下面主要是对 LeaveIt 主题的一些优化。 ","date":"2019-07-24","objectID":"/blog/helloword/:5:0","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"},{"categories":["Toy"],"content":"解决黑色主题闪屏问题 切换为黑色主题后，每次打开新页面都会网页都会闪一下，亮瞎狗眼有没有！ 解决方法只需要更改 mysite/themes/LeaveIt/layouts/_default/baseof.html 的一行代码： \u003cbody class=\"dark-theme \"\u003e 将默认加载的主题设为黑色即可。 ","date":"2019-07-24","objectID":"/blog/helloword/:5:1","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"},{"categories":["Toy"],"content":"站点流量统计 利用不蒜子，两行代码实现访问量统计，将以下代码 \u003cscript async src=\"//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js\"\u003e\u003c/script\u003e 可以加在 mysite/themes/LeaveIt/layouts/partials/js.html 中。 再将以下代码加到需要的位置即可： 网站总访问量Page View:(可以放在partials/footer.html) \u003cspan id=\"busuanzi_container_site_pv\"\u003e本站总访问量\u003cspan id=\"busuanzi_value_site_pv\"\u003e\u003c/span\u003e次\u003c/span\u003e 网站总访客数Unique Visitor: \u003cspan id=\"busuanzi_container_site_uv\"\u003e您是第\u003cspan id=\"busuanzi_value_site_uv\"\u003e\u003c/span\u003e位访客\u003c/span\u003e 博客总阅读量:（放在_default/simple.html） \u003cspan id=\"busuanzi_container_site_pv\"\u003e本文总阅读量\u003cspan id=\"busuanzi_value_page_pv\"\u003e\u003c/span\u003e次\u003c/span\u003e ","date":"2019-07-24","objectID":"/blog/helloword/:5:2","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"},{"categories":["Toy"],"content":"博客目录 ","date":"2019-07-24","objectID":"/blog/helloword/:5:3","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"},{"categories":["Toy"],"content":"站内搜索 留点坑，有空再来填。 最后，致谢GitHub, Hugo, LeaveIt, 和博主Mogeko！ 添加个人域名、CDN、评论区等更多优化可以参考Mogeko的博客。 ","date":"2019-07-24","objectID":"/blog/helloword/:5:4","tags":["Hugo","HTML"],"title":"零基础用 GitHub Pages + Hugo 搭建个人网站","uri":"/blog/helloword/"}]