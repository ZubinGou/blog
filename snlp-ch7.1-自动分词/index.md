# 《统计自然语言处理》第7.1章 - 自动分词


## 7.1 汉语自动分词中的基本问题
- 词是最小的能够独立运用的语言单位，很多孤立语和黏着语文本不像西方屈折语文本，词与词之间没有空格显示指示词的边界，首先需要自动分词。
- 汉语自动分词：让计算机系统在汉语文本中的词与词之间自动加上空格或其他边界标记
- 主要困难：分词规范、歧义切分、未登录词的识别

### 7.1.1 汉语分词的规范问题
- 主要困难：
	1. 单字词与词素之间的划界
	2. 词与短语（词组）的划界

### 7.1.2 歧义切分问题
- 基本切分歧义类型：
	1. 交集型切分歧义：汉字串AJB称作交集型切分歧义，如果满足AJ、JB同时为词（A、J、B分别为汉字串）。此时汉字串J称作交集串。
		- 交集串链：交集型切分歧义拥有的交集串的集合
		- 链长：交集串链中交集串的个数
		- eg. “中国产品质量”字段的链长为4，“部分居民生活水平”字段的链长为6
		- [孙茂松等，2001]认为，定义7-3中给出的名称“多义组合型切分歧义”是不太科学的（实际上，某些交集型切分歧义也是多义组合的），容易引起混淆，与“交集型”这个纯形式的名称相呼应，称作“包孕型”或者“覆盖型”可能更恰当
		- [董振东，1997]称之为“偶发性歧义”
	2. 多义组合型切分歧义：汉字串AB称作多义组合型切分歧义，如果满足A、B、AB同时为词
		- eg. “将来”、“现在”、“才能”、“学生会”
		- [孙茂松等, 2001]补充定义：文本中至少存在一个上下文语境C，在C的约束下，A、B在语法和语义上都成立
		- [董振东，1997]称之为“固有歧义”
- [侯敏等，1995]认为还有“混合型”，集交集型与组合型的特点，交集型字段内包含组合型字段：
  	> - 这篇文章写得太平淡了。
	> - 这墙抹得太平了。
	> - 即使太平时期也不应该放松警惕。

### 7.1.3 未登录词问题
- 未登录词又称生词（unknown word），两种解释
	1. 词表没有收录的词
	2. 训练语料中未曾出现的词，又称集外词（out of vocabulary, OOV）
	- 因词表在大规模语料中容易获取，通常将OOV与未登录词看作一回事
- 未登录词类型：
	1. 新出现的普通词汇：eg. 奥力给、不讲武德
	2. 专有名词（proper names）：人名、地名、组织机构名
		- 命名实体（named entity）：专有名词 + 时间和数字表达（日期、数量值、百分比、序数、货币数量等）
	3. 专业名词和研究领域名称：eg. 三聚氰胺、苏丹红、禽流感、堰塞湖
	4. 其他专用名词，如新出现的产品、电影、书籍名
	- 黄昌宁等人（2003）统计，未登录词约九成为专有名词，其余为新词
- 实际应用中未登录词的影响远大于歧义切分：
	![d7be7fa7cd2deee23cab488bdae47afe.png](/blog/_resources/0c7aa4165cb94cefb13ba7dbe8447d28.png)
- 需要说明的是，在汉语分词中对命名实体词汇的识别处理是指将命名实体中可独立成词的切分单位正确地识别出来，而不是指识别整个实体的左右边界。


## 7.2 汉语分词方法
- 分词方法：
	- 基于词表的方法
		- 正向最大匹配法（forward maximum matching method, FMM）
		- 逆向最大匹配法（backward maximum matching method, BMM）
		- 双向扫描法
		- 逐词扫描法
	- 基于统计模型的方法（结合n元语法）
		- HMM
		- CRF
		- SVM
		- 深度学习
	- 规则方法与统计方法相结合

### 7.2.1 N-最短路径方法
- 分词两阶段：1. 粗分 2. 歧义排除和未登录词识别
- 基于N-最短路径方法的汉语词语粗分模型[张华平等, 2002]
	- 旨在提高召回率并兼顾准确率
- 基本思想：
	1. 根据词典，找出字符串中所有可能的词，构造词语切分有向无环图
	2. 每个词对应一条有向边，边长为权值
	3. 求出N条最短路，作为粗分结果集（算并列长度，最后集合大小>=N）
- 建立词边：
	![a668f30139f25d047243fceccbc71e50.png](/blog/_resources/01adf2b46ad540cc8bae34b14cd1adfc.png)
	
- 考虑边长影响，分为两种模型
	1. 非统计粗分模型：所有词权重对等，即词边长均为1
		- NSP：结点V0到Vn的前N个最短路径的集合
		- N-最短路方法：将词语粗分问题转化为求解有向无环图G的NSP
		- 求解方法：贪心，即Dijkstra简单扩展
			1. 记录每个结点N个最短路和前驱
			2. 回溯求解NSP
			![c45eda261562b8a677da731e209970a8.png](/blog/_resources/0e913d8661614ef2bfe0f03d217b8fc8.png)
		- 复杂度：字符串长度n，最短路径数N，某字作为词尾的平均次数k（总词数/末端词数，即切分图中结点入度平均值），算法复杂度为$O(n\times N\times k)$
	2. 统计粗分模型
		- 词权重设置为词频负对数：$-\ln P(w_i)$
			- $P(W \mid C)=\frac{P(W) P(C \mid W)}{P(C)} \sim P(W)$
			- $P(W)=\prod\_{i=1}^{m} P\left(w\_{i}\right)$
			- 越高频，越是捷径
		- 求min：$p^*(W)=-\ln P(W)= \sum\_{i=1}^{m}\left[-\ln P\left(w\_{i}\right)\right]$
		- 同理用最短路算法求解
- 实验结果[张华平等, 2002]：在N＝10的情况下，非统计粗分模型和统计粗分模型切分句子的召回率分别为99.73％和99.94％，均高于最大匹配方法和最短路径方法获得的召回率。


### 7.2.2 基于词的n元语法模型的分词方法
- 典型的生成式模型
- 基本思想：
	1. 用词典对句子简单匹配，找出所有可能的词典词
	2. 将词典词和所有单个字作为结点，构造n元切分词图
	3. 边上概率表示代价，利用搜索算法（如Viterbi算法）找出代价最小路径
	![e773bac8834931727cc0fb67e73deb47.png](/blog/_resources/41c55cb890f0441c8af23aa6e3325d55.png)
- 未登录词与歧义切分一体化处理：改进的信源信道模型的分词方法[J.Gao等, 2003]
	- 受到启发：[Richard Sproat等, 1996]的基于加权的有限状态转换机（weighted finite-state transducer）模型与未登录词识别一体化切分的实现方法

#### 改进的信源信道模型分词
- 将汉语词定义为4类
	1. 词表中有的词
	2. 文本中任意一个经词法派生出来的词或短语为一个词，如重叠形式（高高兴兴，说说话、天天）、前缀派生（非党员、副部长）、后缀派生（全面性、朋友们）、中缀派生（看得出、看不出）、动词加时态助词（克服了、蚕食着）、动词加趋向动词（走出、走出来）、动词的分离形式（长度不超过3个字，如：洗了澡、洗过澡），等等
	3. 文本中被明确定义的任意一个实体名词（如：日期、时间、货币、百分数、温度、长度、面积、体积、重量、地址、电话号码、传真号码、电子邮件地址等）是一个词。
	4. 文本中任意一个专有名词（人名、地名、机构名）是一个词。
- 假设随机变量S为一个汉字序列，W是S上所有可能切分出来的词序列，分词过程应该是求解使条件概率$P(W\mid S)$最大的切分出来的词序列
	- $W^{*}=\underset{W}{\operatorname{argmax}} P(W \mid S)$
	- 贝叶斯：$W^*=\underset{W}{\operatorname{argmax}} \frac{P(W) P(S \mid W)}{P(S)}$
	- 分母为归一化因子：$W^{*}=\underset{W}{\operatorname{argmax}} P(W) P(S \mid W)$
- 按下表可以把一个可能的词序列W转换成一个可能的词类序列$C＝c_1c_2…c_N$
	![fe3791a431566a9784181f3446b4db51.png](/blog/_resources/c0f6e4231aef408cab2d9b5e3d4def25.png)
	- $W^\*$改写为：$C^{*}=\underset{C}{\operatorname{argmax}} P(C) P(S \mid C)$
	- P(C)即语言模型
	- P(S|C)称生成模型
- 对于语言模型，如采用三元语法：
	- $P(C)=P\left(c\_{1}\right) P\left(c\_{2} \mid c\_{1}\right) \prod\_{i=3}^{N} P\left(c\_{i} \mid c\_{i-2} c\_{i-1}\right)$
- 生成模型：
	- 独立性假设：词类$c_i$生成汉字串$s_i$概率只与$c_i$自身有关，而与其上下文无关
	- 则：$P(S \mid C) \approx \prod\_{i=1}^{N} P\left(s\_{i} \mid c\_{i}\right)$
- [黄昌宁等，2003]实验：
	- 词表、派生词表
	- 语料：新闻文本
	- 模型训练：
		1. 语料类别标记：FMM切分语料、专有名词、实体名词标注
		2. 最大似然估计：估计统计语言模型概率参数
		3. 用语言模型重新切分标注，得到刷新的训练语料，重复2、3直到收敛
	- 交集型歧义字段（OAS）处理：最大匹配算法检测OAS，用特定类 $<GAP>$ 取代全体OAS，训练语言模型P(C)；类 $<GAP>$ 的生成模型的参数通过消歧规则或机器学习方法估计
	- 组合型歧义字段（CAS）处理：对高频、切分分布均匀的70条CAS训练二值分类器，用分类器进行消歧
- 实验结果[黄昌宁等，2003]：自动分词的正确率和召回率分别达到了96.3％和97.4％

### 7.2.3 由字构词的汉语分词方法
- 由字构词（character-based tagging）的汉语分词方法[Xue and Converse,2002]
- 思想：将分词过程看作字的分类问题
- 规定每个字只有4个词位：词首（B）、词中（M）、词尾（E）和单独成词（S）
	![d9fda95b1b9f2bb08497e744856296d9.png](/blog/_resources/8bc2cb8ecf1144d7a1c139f922635283.png)
- 原理：将分词结果表示成字标注形式，分词问题转化为序列标注问题
	- 对于汉语句子$C^{n}={c}\_{1} {c}\_{2} \ldots {c}\_{ {n}}$
	- $P\left(t\_{1}^{n} \mid c\_{1}^{n}\right)=\prod\_{k=1}^{n} P\left(t\_{k} \mid t\_{1}^{k-1}, c\_{1}^{n}\right) \approx \prod\_{k=1}^{n} P\left(t\_{k} \mid t\_{k-1}, c\_{k-2}^{k-2}\right)$
		- $t_k\in \{B, M, E, S\}$
- 特征窗口
	- 一般取w=5个字，前后各两个字
	- 窗口中抽取特征，常用特征模板：
		(a) $c\_{k} (k=-2,-1,0,1,2)$
		(b) $c\_{k} c\_{k+1} (k=-2,-1,0,1)$
		(c) $c\_{-1} c\_{1}$
		(d) $T\left(c\_{-2}\right) T\left(c\_{-1}\right) T\left(c\_{0}\right) T\left(c\_{1}\right) T\left(c\_{2}\right)$
	- 前面三类特征模板（a）～（c）是窗口内的字及其组合特征
	- 模板（d）与定义的字符类别信息有关，主要是为了处理数字、标点符号和英文字符等有明显特征的词
	- 有了特征，利用常用判别式模型（最大熵、CRF、SVM、感知机）进行参数训练，然后利用解码算法找到最优切分结果
- 由字构词优势：平衡看待词表词与未登录词识别问题，分词过程为字重组的简单过程；学习架构上，既可以不必专门强调词表词信息，也不用专门设计特定的未登录词识别模块，因此，大大简化了分词系统的设计。


### 7.2.4 基于词感知机算法的汉语分词方法
- 平均感知机（averaged perceptron）：使用**词**相关的特征

假设x∈X是输入句子，y∈Y是切分结果，其中X是训练语料集合，Y是X中句子标注结果集合。我们用GEN（x）表示输入句子x的切分候选集，用$\phi(x, y)\in R^d$表示训练实例（x, y）对应的特征向量，α表示参数向量，其中$R^d$是模型的特征空间。那么，给定一个输入句子x，其最优切分结果满足如下条件：
$$F(x)=\arg \max \_{y \in \operatorname{GEN}(x)}\{\Phi(x, y) \cdot \alpha\}$$

- 训练算法如下：
	![f8cecfff110697fa575a11ac89bb9591.png](/blog/_resources/f9f31357d60d4a08a757c287fecc8973.png)
- 词感知机思路：
	1. 解码器每次读入一个字，生成所有候选词，候选词两种：
		a. 作为上一个候选词末尾
		b. 作文下一个候选字开始
	2. 解码器维持源列表、目标列表（滚动数组），每读入一个词，与源列表每个候选组合为两个新候选（合并为新词或者作为下一个词的开始），新候选放入目标列表
	3. 处理完成后，copy 目标列表 to 源列表，clear 目标列表，读入下一个词，重复（2）
	![c3807219be7b261cffe418940923398b.png](/blog/_resources/5cc7e868835f4ad78978e361456b140e.png)
- 该解码算法类似于全切分方法，理论上会生成所有$w^{l-1}$个切分结果（l为句长），为提升切分速度，限制目标列表tgt保留B个最高得分的候选（eg. B=16）。对tgt列表中切分候选打分和排序采用平均感知机分类器算法[Zhang and Clark, 2007]，使用如下特征：
![6b74977fe04823b6d064993e1d7f361e.png](/blog/_resources/3262af5dbe6a448494101ae2bc36890c.png)


### 7.2.5 基于字的生成式模型和区分式模型相结合的汉语分词方法
- 汉语分词两大主流方法
	1. 基于词的n元语法模型（生成式模型）
		- 集内词效果好，集外词效果差
	2. 基于字的序列标注模型（区分式模型）
		- 集外词效果好，集内词效果差
- ［Wang et al., 2012］两个处于词边界的字的依赖关系和两个处于词内部的字的依赖关系不同（容易理解，词内部字依赖关系更强）
	![9c06d04caa5d8c39170d8440d626b8cd.png](/blog/_resources/908d280cb2704e8e8e8dec02e9e10ef5.png)
- 两大方法优缺点：
	1. 基于词的生成式模型实际上隐含地考虑了这种处于不同位置字之间
的依赖关系，而在基于字的判别式模型中却无法考虑这种依赖关系。
	2. 但是，区分式模型能够充分利用上下文特征信息等，有较大的灵活性。因
此，基于字的区分式模型具有较强的鲁棒性。
- 基于字的n元语法模型[Wang et al., 2009, 2010a, 2012]
	- 结合基于字的生成式、判别式模型
	- 将字替换为<字-标记>对，即
		$P\left(w\_{1}^{m} \mid c\_{1}^{n}\right) \equiv P\left([c, t]\_{1}^{n} \mid c\_{1}^{n}\right)=\frac{P\left(c\_{1}^{n} \mid[c, t]\_{1}^{n}\right) \times P\left([c, t]\_{1}^{n}\right)}{P\left(c\_{1}^{n}\right)}$
	- 3-gram & Bayes:
		$P\left(w\_{1}^{m}\right)=\prod\_{i=1}^{m} P\left(w\_{i} \mid w\_{1}^{j-1}\right) \approx \prod\_{i=1}^{m} P\left(w\_{i} \mid w\_{i-2}^{i-1}\right)$
	- 简化：
		$P\left([c, t]\_{1}^{n}\right) \approx \prod\_{i=1}^{n} P\left([c, t]\_{i} \mid[c, t]\_{i-k}^{i-1}\right)$
- 基于字的n-gram特点：
	- 以字为基本单位，但考虑了字与字的依赖关系，对词典词处理能力优于基于字的判别式模型
	- 但没有考虑未来信息（下文），对未登录词处理能力仍弱于基于字的判别式模型
- 改进：集成式分词模型[Wang et al., 2010a, 2012]
	- 结合基于字的判别式、基于字的生成式
	- 线性插值法整合两个模型
	- Score $\left(t\_{k}\right)=\alpha \times \log \left(P\left([c, t]\_{k} \mid[c, t]\_{k-2}^{k-1}\right)\right)+(1-\alpha) \times \log \left(P\left(t\_{k} \mid t\_{k-1}, c\_{k-2}^{k-2}\right)\right)$


### 7.2.6 其他分词方法
- [Wu, 2003a]句法分析与自动分词相结合
	- 利用句法结构消除组合型歧义
	- 效果差
		- 组合型歧义少
		- 句法分词本身的歧义，倒打一耙
		- 句法分析器的语法规则使用范围有限
- [Gao et al., 2005]汉语分词的语用方法（pragmatic approach）
	1. 词：根据它们在实际使用和处理中的需要从语用上定义的切分单位
	2. 语用数学框架：切分已知词和检测不同类型的生词能够以一体化的方式同步进行
	3. 切分标准：假设不存在独立于应用的通用切分标准，不同任务需要多重切分标准和不同的词汇粒度
- 由字构词的方法改进
	- ［Zhang et al., 2006a, 2006b］为提升词典词召回率（recall），张瑞强等人提出了基于“子词”（sub-word）的判别式模型
	- ［Zhao et al., 2006a, 2010］赵海等人还比较了不同词位数量对该模型的影响，他们的实验表明，基于6个词位的效果最好
- 将汉语分词与词性标注两项任务同时进行，以达到同时提升两项任务性能的目的，一直是这一领域研究的一个重要方向，这种方法往往需要耗费更多的时间代价

### 7.2.7 分词方法比较
- 测评语料：
	![dbf3ab7f574cf8586da7a33ffe2e0820.png](/blog/_resources/7fe44984dbb8458184b876a2ce85d056.png)
- 测评指标：
	![ff33397139b09acde0bd5523624f0db8.png](/blog/_resources/8479a0877f1f4b308329fa233127b17d.png)
- 测评结果：
	![234ffbf2c9c45d2483125f22a119216c.png](/blog/_resources/550269bc9d5842b683e3e6469c850890.png)
	![a55b55044e3efe14bc4d8a3fd28c5aa6.png](/blog/_resources/b48c36939bd94b7cbf8e98455964d99f.png)
- 存在难题：
	- 跨领域分词性能
	- 非规范文本：微博、短信，存在大量新词、流行语

### 补充：语言结构类型
1. 孤立语，如：汉语
	- 缺乏词性变化
	- 词序严格
	- 虚词十分重要
	- 复合词多，派生词少
2. 屈折语，如：印欧语系诸语言，英语、德语、法语
	- 屈折：内部屈折，词内部的语音形式的变化
	- 词性变化丰富，用以表示词间关系
	- 一种词性变化的语素可以表示几种不同的语法意义
	- 词尾和词干/词根结合紧密，脱离词尾，词根不能独立存在
3. 黏着语，如：日语、土耳其语、维吾尔语、芬兰语
	- 词前面和中间不变，只是词尾变化表示语法意义
	- 变词语素的一种变化只表示一种语法意义
	- 词根与变词语素不紧密，两者有恒大独立性
4. 复综语/编插语/多式综合语，如：美洲印第安语、爱斯基摩语、巴斯克语
	- 分不出词和句子，一个词的构成也是另一个词的组成
	- 没有能独立使用的词，只能许多成分相互编插组合在一起，连缀为句子使用

## References
- 叶蜚声《语言学纲要》
